{"cells":[{"cell_type":"code","source":["!pip install -q condacolab\n","import condacolab\n","condacolab.install()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Lv7iCv1jHix","executionInfo":{"status":"ok","timestamp":1687905578092,"user_tz":-120,"elapsed":21250,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"db37be9d-387d-47e2-b948-fc54d216032d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["â¬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n","ðŸ“¦ Installing...\n","ðŸ“Œ Adjusting configuration...\n","ðŸ©¹ Patching environment...\n","â² Done in 0:00:14\n","ðŸ” Restarting kernel...\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lj2qPoukjWfa","executionInfo":{"status":"ok","timestamp":1687905597170,"user_tz":-120,"elapsed":19083,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"47366bac-a412-41f6-8949-acc3e0af6007"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_30U0v0HiIl9"},"outputs":[],"source":["#!conda  list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hH-6l6I2iImE","outputId":"52f93dd9-8c7c-4795-feaf-c4262e34ca0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/mnt/projects-students/LearningDeformations\r\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FQU_HvIviImF","executionInfo":{"status":"ok","timestamp":1687906462713,"user_tz":-120,"elapsed":859340,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"c7b31807-6d99-4b4c-ae78-db706581566b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","\n","\n","==> WARNING: A newer version of conda exists. <==\n","  current version: 23.1.0\n","  latest version: 23.5.0\n","\n","Please update conda by running\n","\n","    $ conda update -n base -c conda-forge conda\n","\n","Or to minimize the number of packages updated during conda update use\n","\n","     conda install conda=23.5.0\n","\n","\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - trimesh\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    ca-certificates-2023.5.7   |       hbcca054_0         145 KB  conda-forge\n","    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n","    cudatoolkit-11.8.0         |      h37601d7_11       635.9 MB  conda-forge\n","    libblas-3.9.0              |17_linux64_openblas          14 KB  conda-forge\n","    libcblas-3.9.0             |17_linux64_openblas          14 KB  conda-forge\n","    libgfortran-ng-13.1.0      |       h69a702a_0          23 KB  conda-forge\n","    libgfortran5-13.1.0        |       h15d22d2_0         1.4 MB  conda-forge\n","    liblapack-3.9.0            |17_linux64_openblas          14 KB  conda-forge\n","    libopenblas-0.3.23         |pthreads_h80387f5_0         5.2 MB  conda-forge\n","    numpy-1.25.0               |  py310ha4c1d20_0         6.5 MB  conda-forge\n","    openssl-3.1.1              |       hd590300_1         2.5 MB  conda-forge\n","    trimesh-3.22.1             |     pyhd8ed1ab_0         540 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:       652.3 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.8.0-h37601d7_11 \n","  libblas            conda-forge/linux-64::libblas-3.9.0-17_linux64_openblas \n","  libcblas           conda-forge/linux-64::libcblas-3.9.0-17_linux64_openblas \n","  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.1.0-h69a702a_0 \n","  libgfortran5       conda-forge/linux-64::libgfortran5-13.1.0-h15d22d2_0 \n","  liblapack          conda-forge/linux-64::liblapack-3.9.0-17_linux64_openblas \n","  libopenblas        conda-forge/linux-64::libopenblas-0.3.23-pthreads_h80387f5_0 \n","  numpy              conda-forge/linux-64::numpy-1.25.0-py310ha4c1d20_0 \n","  trimesh            conda-forge/noarch::trimesh-3.22.1-pyhd8ed1ab_0 \n","\n","The following packages will be UPDATED:\n","\n","  ca-certificates                      2022.12.7-ha878542_0 --> 2023.5.7-hbcca054_0 \n","  certifi                            2022.12.7-pyhd8ed1ab_0 --> 2023.5.7-pyhd8ed1ab_0 \n","  openssl                                  3.1.0-h0b41bf4_0 --> 3.1.1-hd590300_1 \n","\n","\n","\n","Downloading and Extracting Packages\n","openssl-3.1.1        | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\n","trimesh-3.22.1       | 540 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","libgfortran-ng-13.1. | 23 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","libgfortran5-13.1.0  | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","certifi-2023.5.7     | 149 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libblas-3.9.0        | 14 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","libcblas-3.9.0       | 14 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","liblapack-3.9.0      | 14 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","ca-certificates-2023 | 145 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","trimesh-3.22.1       | 540 KB    | :   3% 0.029615543822292277/1 [00:00<00:06,  6.25s/it]\u001b[A\n","\n","\n","\n","openssl-3.1.1        | 2.5 MB    | :   1% 0.006200398045572774/1 [00:00<00:32, 32.54s/it]\n","\n","libgfortran-ng-13.1. | 23 KB     | :  71% 0.7067552411353637/1 [00:00<00:00,  3.50it/s]\u001b[A\u001b[A\n","\n","\n","libgfortran5-13.1.0  | 1.4 MB    | :   1% 0.011398453305579287/1 [00:00<00:17, 17.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libblas-3.9.0        | 14 KB     | : 100% 1.0/1 [00:00<00:00,  4.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","openssl-3.1.1        | 2.5 MB    | :  68% 0.6758433869674324/1 [00:00<00:00,  2.79it/s]  \n","\n","\n","\n","\n","\n","\n","\n","ca-certificates-2023 | 145 KB    | :  11% 0.1104340792666487/1 [00:00<00:02,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","libcblas-3.9.0       | 14 KB     | : 100% 1.0/1 [00:00<00:00,  3.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   0% 2.4570534161823033e-05/1 [00:00<3:31:05, 12665.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","certifi-2023.5.7     | 149 KB    | : 100% 1.0/1 [00:00<00:00,  3.56it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","certifi-2023.5.7     | 149 KB    | : 100% 1.0/1 [00:00<00:00,  3.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","libgfortran-ng-13.1. | 23 KB     | : 100% 1.0/1 [00:00<00:00,  3.50it/s]               \u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | :   0% 0.003030666258237034/1 [00:00<01:55, 116.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | :   0% 0.0024211937206198987/1 [00:00<02:26, 146.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   0% 0.0036610095901116317/1 [00:00<01:31, 91.35s/it]      \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | :  26% 0.25906772810632916/1 [00:00<00:01,  1.35s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libblas-3.9.0        | 14 KB     | : 100% 1.0/1 [00:00<00:00,  4.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | :  38% 0.37580261602139226/1 [00:00<00:00,  1.02it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   1% 0.005946069267161173/1 [00:00<01:11, 72.27s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | :  63% 0.6334092479715402/1 [00:00<00:00,  1.43it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | :  53% 0.5253990373745181/1 [00:00<00:00,  1.22it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","liblapack-3.9.0      | 14 KB     | : 100% 1.0/1 [00:00<00:00,  3.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   1% 0.008575116422476239/1 [00:00<01:02, 62.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | :  96% 0.9637518701193769/1 [00:00<00:00,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | :  79% 0.7941515403633268/1 [00:00<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   1% 0.011572721590218648/1 [00:00<00:50, 50.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","trimesh-3.22.1       | 540 KB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]                 \u001b[A\n","trimesh-3.22.1       | 540 KB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   2% 0.01516001957784481/1 [00:00<00:40, 41.25s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","libcblas-3.9.0       | 14 KB     | : 100% 1.0/1 [00:00<00:00,  3.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","libgfortran5-13.1.0  | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  1.12it/s]                 \u001b[A\u001b[A\u001b[A\n","\n","\n","libgfortran5-13.1.0  | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  1.12it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   2% 0.01918958718038379/1 [00:00<00:35, 35.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","ca-certificates-2023 | 145 KB    | : 100% 1.0/1 [00:00<00:00,  1.07it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","ca-certificates-2023 | 145 KB    | : 100% 1.0/1 [00:00<00:00,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   3% 0.025184797515868607/1 [00:01<00:26, 27.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   3% 0.030418321292336913/1 [00:01<00:23, 24.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","openssl-3.1.1        | 2.5 MB    | : 100% 1.0/1 [00:01<00:00,  2.79it/s]               \n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   4% 0.040492240298684354/1 [00:01<00:21, 22.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopenblas-0.3.23   | 5.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.95it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   5% 0.0452343533919162/1 [00:01<00:21, 22.45s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   5% 0.05277750737959587/1 [00:01<00:17, 18.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   6% 0.060836642584673827/1 [00:01<00:15, 16.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   7% 0.06830608496986802/1 [00:01<00:14, 15.31s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   8% 0.07690577192650609/1 [00:01<00:12, 14.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   8% 0.08444892591418576/1 [00:02<00:12, 13.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :   9% 0.09275376646088195/1 [00:02<00:12, 13.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  10% 0.10022320884607615/1 [00:02<00:12, 13.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  11% 0.10769265123127035/1 [00:02<00:12, 13.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  12% 0.11759457649848502/1 [00:02<00:10, 12.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  13% 0.12570285277188664/1 [00:02<00:11, 12.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  13% 0.1347202388092757/1 [00:02<00:10, 12.24s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  15% 0.14545756223799236/1 [00:02<00:09, 11.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  16% 0.15523663483439792/1 [00:02<00:09, 10.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","numpy-1.25.0         | 6.5 MB    | : 100% 1.0/1 [00:02<00:00,  1.62it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  18% 0.17661299955518395/1 [00:02<00:06,  7.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  20% 0.2023629193567745/1 [00:03<00:04,  6.01s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  23% 0.22867796144408695/1 [00:03<00:03,  5.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  25% 0.2499560440282257/1 [00:03<00:03,  4.99s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  27% 0.27005474097259696/1 [00:03<00:03,  5.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  29% 0.29214365118407587/1 [00:03<00:03,  4.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  31% 0.3123652007992562/1 [00:03<00:03,  5.07s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  33% 0.3321936218678474/1 [00:03<00:03,  5.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  35% 0.35280830002961694/1 [00:03<00:03,  5.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  37% 0.37425837635288844/1 [00:03<00:03,  5.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  40% 0.3977969480799149/1 [00:03<00:02,  4.83s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  42% 0.4188293253224354/1 [00:04<00:02,  4.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  44% 0.43971427935998497/1 [00:04<00:02,  5.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  46% 0.46408824924851344/1 [00:04<00:02,  4.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  49% 0.4867668522798761/1 [00:04<00:02,  4.64s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  51% 0.5096174490503715/1 [00:04<00:02,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  53% 0.5316326476593649/1 [00:04<00:02,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  55% 0.5534512819950638/1 [00:04<00:02,  5.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  57% 0.5735254084052732/1 [00:04<00:02,  5.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  59% 0.5932309768030553/1 [00:04<00:02,  5.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  61% 0.6126908398592191/1 [00:05<00:02,  6.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  63% 0.6344357625924325/1 [00:05<00:02,  5.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  65% 0.653109368555418/1 [00:05<00:01,  5.62s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  68% 0.678367877673772/1 [00:05<00:01,  5.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  70% 0.698785991562247/1 [00:05<00:01,  5.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  72% 0.7219805758110079/1 [00:05<00:01,  5.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  74% 0.7423004075628357/1 [00:07<00:06, 26.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  76% 0.7568953048549585/1 [00:07<00:05, 22.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  77% 0.7726695877868489/1 [00:07<00:04, 18.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  79% 0.7897952500976395/1 [00:07<00:03, 14.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  81% 0.8110979032159401/1 [00:07<00:02, 11.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  83% 0.8346119044088047/1 [00:07<00:01,  8.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  86% 0.8605583884836898/1 [00:07<00:01,  7.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  88% 0.8835072673908325/1 [00:08<00:00,  6.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  90% 0.9044413624967058/1 [00:08<00:00,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  93% 0.92552288080755/1 [00:08<00:00,  6.69s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  95% 0.9487174650563109/1 [00:08<00:00,  5.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  97% 0.9687178798640348/1 [00:08<00:00,  5.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","cudatoolkit-11.8.0   | 635.9 MB  | :  99% 0.9877354733052859/1 [00:08<00:00,  6.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","Preparing transaction: | \b\bdone\n","Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n","Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n","\n","\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","\n","\n","==> WARNING: A newer version of conda exists. <==\n","  current version: 23.1.0\n","  latest version: 23.5.0\n","\n","Please update conda by running\n","\n","    $ conda update -n base -c conda-forge conda\n","\n","Or to minimize the number of packages updated during conda update use\n","\n","     conda install conda=23.5.0\n","\n","\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - h5py\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    ca-certificates-2023.01.10 |       h06a4308_0         127 KB  anaconda\n","    certifi-2022.12.7          |  py310h06a4308_0         151 KB  anaconda\n","    conda-23.1.0               |  py310h06a4308_0         969 KB  anaconda\n","    h5py-3.7.0                 |  py310he06866b_0         5.4 MB  anaconda\n","    hdf5-1.10.6                |       h3ffc7dd_1         4.9 MB  anaconda\n","    zlib-1.2.13                |       h166bdaf_4          92 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:        11.6 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  h5py               anaconda/linux-64::h5py-3.7.0-py310he06866b_0 \n","  hdf5               anaconda/linux-64::hdf5-1.10.6-h3ffc7dd_1 \n","  zlib               conda-forge/linux-64::zlib-1.2.13-h166bdaf_4 \n","\n","The following packages will be SUPERSEDED by a higher-priority channel:\n","\n","  ca-certificates    conda-forge::ca-certificates-2023.5.7~ --> anaconda::ca-certificates-2023.01.10-h06a4308_0 \n","  certifi            conda-forge/noarch::certifi-2023.5.7-~ --> anaconda/linux-64::certifi-2022.12.7-py310h06a4308_0 \n","  conda              conda-forge::conda-23.1.0-py310hff520~ --> anaconda::conda-23.1.0-py310h06a4308_0 \n","\n","\n","\n","Downloading and Extracting Packages\n","certifi-2022.12.7    | 151 KB    | :   0% 0/1 [00:00<?, ?it/s]\n","ca-certificates-2023 | 127 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","h5py-3.7.0           | 5.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","zlib-1.2.13          | 92 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","conda-23.1.0         | 969 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","hdf5-1.10.6          | 4.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","certifi-2022.12.7    | 151 KB    | :  11% 0.10562690185156533/1 [00:00<00:01,  2.09s/it]\n","\n","\n","zlib-1.2.13          | 92 KB     | : 100% 1.0/1 [00:00<00:00,  1.03s/it]               \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","hdf5-1.10.6          | 4.9 MB    | :   0% 0.003221572581239806/1 [00:00<01:10, 71.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","ca-certificates-2023 | 127 KB    | :  13% 0.1263573543928924/1 [00:00<00:01,  1.92s/it]\u001b[A\n","\n","h5py-3.7.0           | 5.4 MB    | :   0% 0.0028875018086540107/1 [00:00<01:26, 86.98s/it]\u001b[A\u001b[A\n","\n","\n","\n","conda-23.1.0         | 969 KB    | :   2% 0.016515629567654205/1 [00:00<00:14, 15.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","hdf5-1.10.6          | 4.9 MB    | :  47% 0.46712802427977185/1 [00:00<00:00,  1.74it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","certifi-2022.12.7    | 151 KB    | : 100% 1.0/1 [00:00<00:00,  2.98it/s]\n","ca-certificates-2023 | 127 KB    | : 100% 1.0/1 [00:00<00:00,  2.92it/s]               \u001b[A\n","ca-certificates-2023 | 127 KB    | : 100% 1.0/1 [00:00<00:00,  2.92it/s]\u001b[A\n","\n","h5py-3.7.0           | 5.4 MB    | :  84% 0.8431505281269711/1 [00:00<00:00,  2.52it/s] \u001b[A\u001b[A\n","\n","\n","\n","conda-23.1.0         | 969 KB    | : 100% 1.0/1 [00:01<00:00,  1.15s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","conda-23.1.0         | 969 KB    | : 100% 1.0/1 [00:01<00:00,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","hdf5-1.10.6          | 4.9 MB    | : 100% 1.0/1 [00:02<00:00,  2.82s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","hdf5-1.10.6          | 4.9 MB    | : 100% 1.0/1 [00:02<00:00,  2.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","Preparing transaction: / \b\bdone\n","Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n","Executing transaction: | \b\b/ \b\b- \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - plyfile\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    plyfile-0.8.1              |     pyhd8ed1ab_0          31 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:          31 KB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  plyfile            conda-forge/noarch::plyfile-0.8.1-pyhd8ed1ab_0 \n","\n","The following packages will be UPDATED:\n","\n","  ca-certificates    anaconda::ca-certificates-2023.01.10-~ --> conda-forge::ca-certificates-2023.5.7-hbcca054_0 \n","  certifi            anaconda/linux-64::certifi-2022.12.7-~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0 \n","\n","\n","\n","Downloading and Extracting Packages\n","                                                                        \n","Preparing transaction: \\ \b\bdone\n","Verifying transaction: / \b\bdone\n","Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - scikit-learn\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    joblib-1.1.1               |  py310h06a4308_0         410 KB  anaconda\n","    scikit-learn-1.2.0         |  py310h6a678d5_1         9.0 MB  anaconda\n","    scipy-1.9.3                |  py310hdfbd76f_2        26.2 MB  conda-forge\n","    threadpoolctl-2.2.0        |     pyh0d69192_0          16 KB  anaconda\n","    ------------------------------------------------------------\n","                                           Total:        35.6 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  joblib             anaconda/linux-64::joblib-1.1.1-py310h06a4308_0 \n","  scikit-learn       anaconda/linux-64::scikit-learn-1.2.0-py310h6a678d5_1 \n","  scipy              conda-forge/linux-64::scipy-1.9.3-py310hdfbd76f_2 \n","  threadpoolctl      anaconda/noarch::threadpoolctl-2.2.0-pyh0d69192_0 \n","\n","The following packages will be SUPERSEDED by a higher-priority channel:\n","\n","  ca-certificates    conda-forge::ca-certificates-2023.5.7~ --> anaconda::ca-certificates-2023.01.10-h06a4308_0 \n","  certifi            conda-forge/noarch::certifi-2023.5.7-~ --> anaconda/linux-64::certifi-2022.12.7-py310h06a4308_0 \n","\n","\n","\n","Downloading and Extracting Packages\n","threadpoolctl-2.2.0  | 16 KB     | :   0% 0/1 [00:00<?, ?it/s]\n","joblib-1.1.1         | 410 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","threadpoolctl-2.2.0  | 16 KB     | : 100% 1.0/1 [00:00<00:00,  4.85it/s]\n","\n","scipy-1.9.3          | 26.2 MB   | :   0% 0.0005965729607019578/1 [00:00<06:29, 389.28s/it]\u001b[A\u001b[A\n","\n","\n","scikit-learn-1.2.0   | 9.0 MB    | :   0% 0.0017323670537283322/1 [00:00<02:13, 134.01s/it]\u001b[A\u001b[A\u001b[A\n","joblib-1.1.1         | 410 KB    | :   4% 0.03905909165679303/1 [00:00<00:06,  6.40s/it]\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | :  12% 0.12050773806179547/1 [00:00<00:01,  2.19s/it]   \u001b[A\u001b[A\n","\n","\n","scikit-learn-1.2.0   | 9.0 MB    | :  35% 0.35340287896057976/1 [00:00<00:00,  1.34it/s]   \u001b[A\u001b[A\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | :  28% 0.27502013488360255/1 [00:00<00:00,  1.19s/it]\u001b[A\u001b[A\n","\n","\n","scikit-learn-1.2.0   | 9.0 MB    | :  90% 0.902563234992461/1 [00:00<00:00,  2.80it/s]  \u001b[A\u001b[A\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | :  45% 0.44504342868366054/1 [00:00<00:00,  1.11it/s]\u001b[A\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | :  64% 0.6383330679510949/1 [00:00<00:00,  1.37it/s] \u001b[A\u001b[A\n","joblib-1.1.1         | 410 KB    | : 100% 1.0/1 [00:00<00:00,  1.74it/s]                \u001b[A\n","joblib-1.1.1         | 410 KB    | : 100% 1.0/1 [00:00<00:00,  1.74it/s]\u001b[A\n","\n","\n","scikit-learn-1.2.0   | 9.0 MB    | : 100% 1.0/1 [00:03<00:00,  2.80it/s]              \u001b[A\u001b[A\u001b[A\n","\n","scipy-1.9.3          | 26.2 MB   | : 100% 1.0/1 [00:05<00:00,  8.03s/it]               \u001b[A\u001b[A\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","Preparing transaction: | \b\b/ \b\bdone\n","Verifying transaction: \\ \b\b| \b\b/ \b\bdone\n","Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \n","\n","    Installed package of scikit-learn can be accelerated using scikit-learn-intelex.\n","    More details are available here: https://intel.github.io/scikit-learn-intelex\n","\n","    For example:\n","\n","        $ conda install scikit-learn-intelex\n","        $ python -m sklearnex my_application.py\n","\n","    \n","\n","\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - pytorch\n","    - torchvision\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n","    filelock-3.12.2            |     pyhd8ed1ab_0          15 KB  conda-forge\n","    freetype-2.12.1            |       hca18f0e_1         611 KB  conda-forge\n","    gmp-6.2.1                  |       h58526e2_0         806 KB  conda-forge\n","    gmpy2-2.1.2                |  py310h3ec546c_1         215 KB  conda-forge\n","    jinja2-3.1.2               |     pyhd8ed1ab_1          99 KB  conda-forge\n","    lcms2-2.15                 |       haa2dc70_1         236 KB  conda-forge\n","    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n","    libdeflate-1.18            |       h0b41bf4_0          64 KB  conda-forge\n","    libhwloc-2.9.1             |       hd6dc26d_0         2.5 MB  conda-forge\n","    libjpeg-turbo-2.1.5.1      |       h0b41bf4_0         479 KB  conda-forge\n","    libpng-1.6.39              |       h753d276_0         276 KB  conda-forge\n","    libprotobuf-3.21.12        |       h3eb15da_0         2.1 MB  conda-forge\n","    libtiff-4.5.1              |       h8b53f26_0         408 KB  conda-forge\n","    libwebp-base-1.3.0         |       h0b41bf4_0         348 KB  conda-forge\n","    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n","    llvm-openmp-16.0.6         |       h4dfa4b3_0        39.9 MB  conda-forge\n","    markupsafe-2.1.3           |  py310h2372a71_0          23 KB  conda-forge\n","    mkl-2022.2.1               |   h84fe81f_16997       157.3 MB  conda-forge\n","    mpc-1.3.1                  |       hfe3b2da_0         114 KB  conda-forge\n","    mpfr-4.2.0                 |       hb012696_0         616 KB  conda-forge\n","    mpmath-1.3.0               |     pyhd8ed1ab_0         428 KB  conda-forge\n","    networkx-3.1               |     pyhd8ed1ab_0         1.4 MB  conda-forge\n","    openjpeg-2.5.0             |       hfec8fc6_2         344 KB  conda-forge\n","    pillow-9.5.0               |  py310h582fbeb_1        44.0 MB  conda-forge\n","    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n","    pytorch-2.0.0              |cpu_py310hd11e9c7_0        66.7 MB  conda-forge\n","    sleef-3.5.1                |       h9b69904_2         1.5 MB  conda-forge\n","    sympy-1.12                 | pypyh9d50eac_103         4.1 MB  conda-forge\n","    tbb-2021.9.0               |       hf52228f_0         1.5 MB  conda-forge\n","    torchvision-0.15.2         |cpu_py310hb9e6163_1         9.7 MB  conda-forge\n","    typing_extensions-4.6.3    |     pyha770c72_0          34 KB  conda-forge\n","    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n","    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:       336.1 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  filelock           conda-forge/noarch::filelock-3.12.2-pyhd8ed1ab_0 \n","  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 \n","  gmp                conda-forge/linux-64::gmp-6.2.1-h58526e2_0 \n","  gmpy2              conda-forge/linux-64::gmpy2-2.1.2-py310h3ec546c_1 \n","  jinja2             conda-forge/noarch::jinja2-3.1.2-pyhd8ed1ab_1 \n","  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n","  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n","  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n","  libhwloc           conda-forge/linux-64::libhwloc-2.9.1-hd6dc26d_0 \n","  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-h0b41bf4_0 \n","  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 \n","  libprotobuf        conda-forge/linux-64::libprotobuf-3.21.12-h3eb15da_0 \n","  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_0 \n","  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.0-h0b41bf4_0 \n","  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n","  llvm-openmp        conda-forge/linux-64::llvm-openmp-16.0.6-h4dfa4b3_0 \n","  markupsafe         conda-forge/linux-64::markupsafe-2.1.3-py310h2372a71_0 \n","  mkl                conda-forge/linux-64::mkl-2022.2.1-h84fe81f_16997 \n","  mpc                conda-forge/linux-64::mpc-1.3.1-hfe3b2da_0 \n","  mpfr               conda-forge/linux-64::mpfr-4.2.0-hb012696_0 \n","  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_0 \n","  networkx           conda-forge/noarch::networkx-3.1-pyhd8ed1ab_0 \n","  openjpeg           conda-forge/linux-64::openjpeg-2.5.0-hfec8fc6_2 \n","  pillow             conda-forge/linux-64::pillow-9.5.0-py310h582fbeb_1 \n","  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n","  pytorch            conda-forge/linux-64::pytorch-2.0.0-cpu_py310hd11e9c7_0 \n","  sleef              conda-forge/linux-64::sleef-3.5.1-h9b69904_2 \n","  sympy              conda-forge/noarch::sympy-1.12-pypyh9d50eac_103 \n","  tbb                conda-forge/linux-64::tbb-2021.9.0-hf52228f_0 \n","  torchvision        conda-forge/linux-64::torchvision-0.15.2-cpu_py310hb9e6163_1 \n","  typing_extensions  conda-forge/noarch::typing_extensions-4.6.3-pyha770c72_0 \n","  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n","  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n","\n","The following packages will be UPDATED:\n","\n","  ca-certificates    anaconda::ca-certificates-2023.01.10-~ --> conda-forge::ca-certificates-2023.5.7-hbcca054_0 \n","  certifi            anaconda/linux-64::certifi-2022.12.7-~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0 \n","\n","The following packages will be DOWNGRADED:\n","\n","  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm \n","\n","\n","\n","Downloading and Extracting Packages\n","pthread-stubs-0.4    | 5 KB      | :   0% 0/1 [00:00<?, ?it/s]\n","gmpy2-2.1.2          | 215 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","networkx-3.1         | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","libdeflate-1.18      | 64 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libwebp-base-1.3.0   | 348 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","freetype-2.12.1      | 611 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","mpfr-4.2.0           | 616 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","mpmath-1.3.0         | 428 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","lerc-4.0.0           | 275 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libpng-1.6.39        | 276 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","gmp-6.2.1            | 806 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libhwloc-2.9.1       | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","typing_extensions-4. | 34 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libxcb-1.15          | 375 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libtiff-4.5.1        | 408 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","tbb-2021.9.0         | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","markupsafe-2.1.3     | 23 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","pthread-stubs-0.4    | 5 KB      | : 100% 1.0/1 [00:00<00:00,  7.80it/s]\n","\n","\n","\n","libdeflate-1.18      | 64 KB     | :  25% 0.25137701950074415/1 [00:00<00:00,  1.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libwebp-base-1.3.0   | 348 KB    | :   5% 0.04592546082432614/1 [00:00<00:03,  3.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","libdeflate-1.18      | 64 KB     | : 100% 1.0/1 [00:00<00:00,  1.51it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","networkx-3.1         | 1.4 MB    | :   1% 0.011221963925879147/1 [00:00<00:19, 19.37s/it]\u001b[A\u001b[A\n","gmpy2-2.1.2          | 215 KB    | :   7% 0.07448931807538951/1 [00:00<00:02,  2.99s/it]\u001b[A\n","\n","\n","\n","\n","\n","freetype-2.12.1      | 611 KB    | :   3% 0.0261869560700386/1 [00:00<00:07,  8.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :   0% 0.00023437003019559726/1 [00:00<15:42, 943.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :   0% 0.0003916828124661948/1 [00:00<09:51, 592.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","mpmath-1.3.0         | 428 KB    | :   4% 0.03737746356130757/1 [00:00<00:07,  7.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","mpfr-4.2.0           | 616 KB    | :   3% 0.02596525815495161/1 [00:00<00:10, 11.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","networkx-3.1         | 1.4 MB    | :  75% 0.7518715830339029/1 [00:00<00:00,  2.91it/s]  \u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :   2% 0.02085893268740816/1 [00:00<00:12, 12.62s/it]    \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :   4% 0.04269342655881524/1 [00:00<00:05,  6.16s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","lerc-4.0.0           | 275 KB    | :   6% 0.05814093783490302/1 [00:00<00:05,  5.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","gmp-6.2.1            | 806 KB    | :   2% 0.019840539414665338/1 [00:00<00:18, 18.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libhwloc-2.9.1       | 2.5 MB    | :   1% 0.006377320653426253/1 [00:00<01:00, 60.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libpng-1.6.39        | 276 KB    | :   6% 0.05797614287382475/1 [00:00<00:06,  6.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libwebp-base-1.3.0   | 348 KB    | : 100% 1.0/1 [00:00<00:00,  2.86it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libwebp-base-1.3.0   | 348 KB    | : 100% 1.0/1 [00:00<00:00,  2.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :   5% 0.04617089594853266/1 [00:00<00:06,  7.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :   9% 0.09008704686722481/1 [00:00<00:03,  3.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libxcb-1.15          | 375 KB    | :   4% 0.042640238602116395/1 [00:00<00:09, 10.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","typing_extensions-4. | 34 KB     | :  47% 0.46938833977940125/1 [00:00<00:00,  1.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libtiff-4.5.1        | 408 KB    | :   4% 0.03918164317059464/1 [00:00<00:11, 12.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libhwloc-2.9.1       | 2.5 MB    | :  97% 0.9693527393207905/1 [00:00<00:00,  2.55it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","tbb-2021.9.0         | 1.5 MB    | :   1% 0.010723460515163315/1 [00:00<00:46, 46.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :   7% 0.07195159927004836/1 [00:00<00:05,  5.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","markupsafe-2.1.3     | 23 KB     | :  69% 0.6851789896286383/1 [00:00<00:00,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  13% 0.134738887488371/1 [00:00<00:02,  3.12s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","gmpy2-2.1.2          | 215 KB    | : 100% 1.0/1 [00:00<00:00,  1.85it/s]                \u001b[A\n","gmpy2-2.1.2          | 215 KB    | : 100% 1.0/1 [00:00<00:00,  1.85it/s]\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  10% 0.10077911298410683/1 [00:00<00:04,  4.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  18% 0.1817408249843144/1 [00:00<00:02,  2.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","freetype-2.12.1      | 611 KB    | : 100% 1.0/1 [00:00<00:00,  1.66it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","freetype-2.12.1      | 611 KB    | : 100% 1.0/1 [00:00<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  13% 0.12515359612444893/1 [00:00<00:04,  4.61s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  22% 0.22208415466833248/1 [00:00<00:02,  2.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  15% 0.14976244929498667/1 [00:00<00:03,  4.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  26% 0.26281916716481674/1 [00:00<00:01,  2.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  30% 0.3023791312239024/1 [00:00<00:01,  2.65s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  18% 0.17741811285806713/1 [00:00<00:03,  4.42s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","mpfr-4.2.0           | 616 KB    | : 100% 1.0/1 [00:00<00:00,  1.10it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","mpfr-4.2.0           | 616 KB    | : 100% 1.0/1 [00:00<00:00,  1.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","mpmath-1.3.0         | 428 KB    | : 100% 1.0/1 [00:01<00:00,  1.06it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","mpmath-1.3.0         | 428 KB    | : 100% 1.0/1 [00:01<00:00,  1.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  20% 0.20226133605880045/1 [00:01<00:03,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  35% 0.34585592340765003/1 [00:01<00:01,  2.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  23% 0.22733892928972935/1 [00:01<00:03,  4.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  39% 0.392074495278661/1 [00:01<00:01,  2.69s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  25% 0.25241652252065827/1 [00:01<00:03,  4.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  43% 0.4300677280878819/1 [00:01<00:01,  2.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  28% 0.2760878955704136/1 [00:01<00:03,  4.65s/it] \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  47% 0.4688443265220352/1 [00:01<00:01,  2.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  30% 0.29788430837860413/1 [00:01<00:03,  4.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  51% 0.5056625108938575/1 [00:01<00:01,  2.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","lerc-4.0.0           | 275 KB    | : 100% 1.0/1 [00:01<00:00,  1.54s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","lerc-4.0.0           | 275 KB    | : 100% 1.0/1 [00:01<00:00,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  54% 0.5397389155784165/1 [00:01<00:01,  3.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  32% 0.31874324106601226/1 [00:01<00:03,  5.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  57% 0.5706818577632459/1 [00:01<00:01,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  34% 0.3374928434816601/1 [00:01<00:03,  5.72s/it] \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  60% 0.6027998483854738/1 [00:01<00:01,  3.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  36% 0.35577370583691664/1 [00:01<00:03,  5.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  38% 0.37546078837334684/1 [00:02<00:03,  5.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  64% 0.6443182265068905/1 [00:02<00:01,  3.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  69% 0.6889700671280367/1 [00:02<00:00,  2.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  41% 0.40663200238936126/1 [00:02<00:02,  4.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  73% 0.72774666556219/1 [00:02<00:00,  2.80s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  43% 0.43311581580146374/1 [00:02<00:02,  4.59s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  77% 0.7653482155589447/1 [00:02<00:00,  2.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  46% 0.45561533870024107/1 [00:02<00:02,  4.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  80% 0.8049081796180304/1 [00:02<00:00,  2.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  48% 0.478349231629214/1 [00:02<00:02,  4.62s/it]  \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  84% 0.8421180468023188/1 [00:02<00:00,  2.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  50% 0.5001456444374046/1 [00:02<00:02,  4.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  52% 0.5235826474569643/1 [00:02<00:02,  4.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  88% 0.878544548361675/1 [00:02<00:00,  2.89s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  55% 0.545847800325546/1 [00:02<00:02,  4.57s/it] \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  92% 0.9212379749204902/1 [00:02<00:00,  2.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  57% 0.573269093858431/1 [00:02<00:01,  4.41s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | :  97% 0.970981692103697/1 [00:02<00:00,  2.57s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  60% 0.5990497971799467/1 [00:02<00:01,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","gmp-6.2.1            | 806 KB    | : 100% 1.0/1 [00:03<00:00,  2.97s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","gmp-6.2.1            | 806 KB    | : 100% 1.0/1 [00:03<00:00,  2.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  63% 0.6290491610449831/1 [00:03<00:01,  3.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libpng-1.6.39        | 276 KB    | : 100% 1.0/1 [00:03<00:00,  3.10s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libpng-1.6.39        | 276 KB    | : 100% 1.0/1 [00:03<00:00,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  66% 0.6585797848496283/1 [00:03<00:01,  3.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","typing_extensions-4. | 34 KB     | : 100% 1.0/1 [00:03<00:00,  3.63s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","typing_extensions-4. | 34 KB     | : 100% 1.0/1 [00:03<00:00,  3.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  69% 0.6883447786844692/1 [00:03<00:01,  3.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","networkx-3.1         | 1.4 MB    | : 100% 1.0/1 [00:03<00:00,  2.91it/s]               \u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  72% 0.7155317021871584/1 [00:03<00:01,  3.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  75% 0.7504528366863025/1 [00:03<00:00,  3.56s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libtiff-4.5.1        | 408 KB    | : 100% 1.0/1 [00:03<00:00,  3.41s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libtiff-4.5.1        | 408 KB    | : 100% 1.0/1 [00:03<00:00,  3.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  79% 0.78678019136662/1 [00:03<00:00,  3.30s/it]  \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libxcb-1.15          | 375 KB    | : 100% 1.0/1 [00:03<00:00,  3.52s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libxcb-1.15          | 375 KB    | : 100% 1.0/1 [00:03<00:00,  3.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","markupsafe-2.1.3     | 23 KB     | : 100% 1.0/1 [00:03<00:00,  4.43s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","markupsafe-2.1.3     | 23 KB     | : 100% 1.0/1 [00:03<00:00,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  83% 0.8280293166810452/1 [00:03<00:00,  3.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  87% 0.8692784419954702/1 [00:03<00:00,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  91% 0.9074807569173526/1 [00:03<00:00,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","tbb-2021.9.0         | 1.5 MB    | : 100% 1.0/1 [00:03<00:00,  3.82s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","tbb-2021.9.0         | 1.5 MB    | : 100% 1.0/1 [00:03<00:00,  3.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  95% 0.9480267721411909/1 [00:04<00:00,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | :  99% 0.9866978271234645/1 [00:04<00:00,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libhwloc-2.9.1       | 2.5 MB    | : 100% 1.0/1 [00:04<00:00,  2.55it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","llvm-openmp-16.0.6   | 39.9 MB   | : 100% 1.0/1 [00:08<00:00,  2.57s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pytorch-2.0.0        | 66.7 MB   | : 100% 1.0/1 [00:24<00:00,  2.70s/it]               \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - glob2\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    glob2-0.7                  |             py_0          11 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:          11 KB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  glob2              conda-forge/noarch::glob2-0.7-py_0 \n","\n","\n","\n","Downloading and Extracting Packages\n","                                                                        \n","Preparing transaction: | \b\bdone\n","Verifying transaction: - \b\bdone\n","Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - torch-scatter\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    torch-scatter-2.1.1        |  py310h2103909_1         284 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:         284 KB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  torch-scatter      conda-forge/linux-64::torch-scatter-2.1.1-py310h2103909_1 \n","\n","\n","\n","Downloading and Extracting Packages\n","                                                                        \n","Preparing transaction: / \b\bdone\n","Verifying transaction: \\ \b\bdone\n","Executing transaction: / \b\bdone\n","Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with initial frozen solve. Retrying with flexible solve.\n","Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n","Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - opencv\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    alsa-lib-1.2.8             |       h166bdaf_0         578 KB  conda-forge\n","    aom-3.5.0                  |       h27087fc_0         2.7 MB  conda-forge\n","    attr-2.5.1                 |       h166bdaf_1          69 KB  conda-forge\n","    boltons-23.0.0             |     pyhd8ed1ab_0         296 KB  conda-forge\n","    cached-property-1.5.2      |       hd8ed1ab_1           4 KB  conda-forge\n","    cached_property-1.5.2      |     pyha770c72_1          11 KB  conda-forge\n","    cairo-1.16.0               |    h35add3b_1015         1.1 MB  conda-forge\n","    conda-23.3.1               |  py310hff52083_0         941 KB  conda-forge\n","    dbus-1.13.6                |       h5008d03_3         604 KB  conda-forge\n","    expat-2.5.0                |       hcb278e6_1         134 KB  conda-forge\n","    ffmpeg-5.1.2               | gpl_h8dda1f0_106         9.2 MB  conda-forge\n","    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n","    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n","    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n","    font-ttf-ubuntu-0.83       |       hab24e00_0         1.9 MB  conda-forge\n","    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n","    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n","    fonts-conda-forge-1        |                0           4 KB  conda-forge\n","    freeglut-3.2.2             |       h9c3ff4c_1         162 KB  conda-forge\n","    gettext-0.21.1             |       h27087fc_0         4.1 MB  conda-forge\n","    glib-2.76.3                |       hfc55251_0         470 KB  conda-forge\n","    glib-tools-2.76.3          |       hfc55251_0         109 KB  conda-forge\n","    gnutls-3.7.8               |       hf3e180e_0         2.2 MB  conda-forge\n","    graphite2-1.3.13           |    h58526e2_1001         102 KB  conda-forge\n","    gst-plugins-base-1.22.0    |       h4243ec0_2         2.6 MB  conda-forge\n","    gstreamer-1.22.0           |       h25f0c4b_2         1.9 MB  conda-forge\n","    h5py-3.9.0                 |nompi_py310h367e799_100         1.1 MB  conda-forge\n","    harfbuzz-6.0.0             |       h3ff4399_1         1.2 MB  conda-forge\n","    hdf5-1.14.0                |nompi_hb72d44e_103         3.4 MB  conda-forge\n","    jasper-4.0.0               |       h32699f2_1         630 KB  conda-forge\n","    jsonpatch-1.32             |     pyhd8ed1ab_0          14 KB  conda-forge\n","    jsonpointer-2.0            |             py_0           9 KB  conda-forge\n","    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n","    libaec-1.0.6               |       hcb278e6_1          34 KB  conda-forge\n","    libcap-2.67                |       he9d0100_0          97 KB  conda-forge\n","    libclang-16.0.1            |default_h62803fd_0          20 KB  conda-forge\n","    libclang13-16.0.1          |default_h9b593c0_0         9.8 MB  conda-forge\n","    libcups-2.3.3              |       h36d4200_3         4.3 MB  conda-forge\n","    libdrm-2.4.114             |       h166bdaf_0         298 KB  conda-forge\n","    libevent-2.1.10            |       h28343ad_4         1.1 MB  conda-forge\n","    libexpat-2.5.0             |       hcb278e6_1          76 KB  conda-forge\n","    libflac-1.4.3              |       h59595ed_0         385 KB  conda-forge\n","    libgcrypt-1.10.1           |       h166bdaf_0         703 KB  conda-forge\n","    libglib-2.76.3             |       hebfc3b9_0         2.6 MB  conda-forge\n","    libglu-9.0.0               |    he1b5a44_1001         413 KB  conda-forge\n","    libgpg-error-1.47          |       h71f35ed_0         255 KB  conda-forge\n","    libidn2-2.3.4              |       h166bdaf_0         157 KB  conda-forge\n","    liblapacke-3.9.0           |17_linux64_openblas          14 KB  conda-forge\n","    libllvm16-16.0.1           |       hadd5161_0        33.4 MB  conda-forge\n","    libogg-1.3.4               |       h7f98852_1         206 KB  conda-forge\n","    libopencv-4.7.0            |  py310h4120a83_2        29.1 MB  conda-forge\n","    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n","    libpciaccess-0.17          |       h166bdaf_0          39 KB  conda-forge\n","    libpq-15.3                 |       hbcd7760_1         2.4 MB  conda-forge\n","    libsndfile-1.2.0           |       hb75c966_0         342 KB  conda-forge\n","    libsystemd0-253            |       h8c4010b_1         372 KB  conda-forge\n","    libtasn1-4.19.0            |       h166bdaf_0         114 KB  conda-forge\n","    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n","    libva-2.18.0               |       h0b41bf4_0         182 KB  conda-forge\n","    libvorbis-1.3.7            |       h9c3ff4c_0         280 KB  conda-forge\n","    libvpx-1.11.0              |       h9c3ff4c_3         1.1 MB  conda-forge\n","    libxcb-1.13                |    h7f98852_1004         391 KB  conda-forge\n","    libxkbcommon-1.5.0         |       h79f4944_0         549 KB  conda-forge\n","    mpg123-1.31.3              |       hcb278e6_0         474 KB  conda-forge\n","    mysql-common-8.0.33        |       hf1915f5_0         763 KB  conda-forge\n","    mysql-libs-8.0.33          |       hca2cd23_0         1.5 MB  conda-forge\n","    nettle-3.8.1               |       hc379101_1         1.1 MB  conda-forge\n","    nspr-4.35                  |       h27087fc_0         222 KB  conda-forge\n","    nss-3.89                   |       he45b914_0         1.9 MB  conda-forge\n","    opencv-4.7.0               |  py310hff52083_2          24 KB  conda-forge\n","    openh264-2.3.1             |       hcb278e6_2         702 KB  conda-forge\n","    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n","    packaging-23.1             |     pyhd8ed1ab_0          45 KB  conda-forge\n","    pcre2-10.40                |       hc3806b6_0         2.3 MB  conda-forge\n","    pillow-9.5.0               |  py310h065c6d2_0        44.3 MB  conda-forge\n","    pixman-0.40.0              |       h36c2ea0_0         627 KB  conda-forge\n","    pulseaudio-client-16.1     |       hb77b528_4         733 KB  conda-forge\n","    py-opencv-4.7.0            |  py310hfdc917e_2         1.1 MB  conda-forge\n","    qt-main-5.15.8             |       h5c52f38_9        50.2 MB  conda-forge\n","    svt-av1-1.4.1              |       hcb278e6_0         2.4 MB  conda-forge\n","    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n","    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n","    xcb-util-0.4.0             |       h166bdaf_0          20 KB  conda-forge\n","    xcb-util-image-0.4.0       |       h166bdaf_0          24 KB  conda-forge\n","    xcb-util-keysyms-0.4.0     |       h166bdaf_0          12 KB  conda-forge\n","    xcb-util-renderutil-0.3.9  |       h166bdaf_0          15 KB  conda-forge\n","    xcb-util-wm-0.4.1          |       h166bdaf_0          55 KB  conda-forge\n","    xorg-fixesproto-5.0        |    h7f98852_1002           9 KB  conda-forge\n","    xorg-inputproto-2.3.2      |    h7f98852_1002          19 KB  conda-forge\n","    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n","    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n","    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n","    xorg-libx11-1.8.4          |       h0b41bf4_0         810 KB  conda-forge\n","    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n","    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n","    xorg-libxi-1.7.10          |       h7f98852_0          46 KB  conda-forge\n","    xorg-libxrender-0.9.10     |    h7f98852_1003          32 KB  conda-forge\n","    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n","    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n","    xorg-xf86vidmodeproto-2.3.1|    h7f98852_1002          23 KB  conda-forge\n","    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:       245.7 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.8-h166bdaf_0 \n","  aom                conda-forge/linux-64::aom-3.5.0-h27087fc_0 \n","  attr               conda-forge/linux-64::attr-2.5.1-h166bdaf_1 \n","  boltons            conda-forge/noarch::boltons-23.0.0-pyhd8ed1ab_0 \n","  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 \n","  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 \n","  cairo              conda-forge/linux-64::cairo-1.16.0-h35add3b_1015 \n","  dbus               conda-forge/linux-64::dbus-1.13.6-h5008d03_3 \n","  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n","  ffmpeg             conda-forge/linux-64::ffmpeg-5.1.2-gpl_h8dda1f0_106 \n","  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n","  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n","  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n","  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n","  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n","  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n","  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n","  freeglut           conda-forge/linux-64::freeglut-3.2.2-h9c3ff4c_1 \n","  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n","  glib               conda-forge/linux-64::glib-2.76.3-hfc55251_0 \n","  glib-tools         conda-forge/linux-64::glib-tools-2.76.3-hfc55251_0 \n","  gnutls             conda-forge/linux-64::gnutls-3.7.8-hf3e180e_0 \n","  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n","  gst-plugins-base   conda-forge/linux-64::gst-plugins-base-1.22.0-h4243ec0_2 \n","  gstreamer          conda-forge/linux-64::gstreamer-1.22.0-h25f0c4b_2 \n","  harfbuzz           conda-forge/linux-64::harfbuzz-6.0.0-h3ff4399_1 \n","  jasper             conda-forge/linux-64::jasper-4.0.0-h32699f2_1 \n","  jsonpatch          conda-forge/noarch::jsonpatch-1.32-pyhd8ed1ab_0 \n","  jsonpointer        conda-forge/noarch::jsonpointer-2.0-py_0 \n","  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n","  libaec             conda-forge/linux-64::libaec-1.0.6-hcb278e6_1 \n","  libcap             conda-forge/linux-64::libcap-2.67-he9d0100_0 \n","  libclang           conda-forge/linux-64::libclang-16.0.1-default_h62803fd_0 \n","  libclang13         conda-forge/linux-64::libclang13-16.0.1-default_h9b593c0_0 \n","  libcups            conda-forge/linux-64::libcups-2.3.3-h36d4200_3 \n","  libdrm             conda-forge/linux-64::libdrm-2.4.114-h166bdaf_0 \n","  libevent           conda-forge/linux-64::libevent-2.1.10-h28343ad_4 \n","  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n","  libflac            conda-forge/linux-64::libflac-1.4.3-h59595ed_0 \n","  libgcrypt          conda-forge/linux-64::libgcrypt-1.10.1-h166bdaf_0 \n","  libglib            conda-forge/linux-64::libglib-2.76.3-hebfc3b9_0 \n","  libglu             conda-forge/linux-64::libglu-9.0.0-he1b5a44_1001 \n","  libgpg-error       conda-forge/linux-64::libgpg-error-1.47-h71f35ed_0 \n","  libidn2            conda-forge/linux-64::libidn2-2.3.4-h166bdaf_0 \n","  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-17_linux64_openblas \n","  libllvm16          conda-forge/linux-64::libllvm16-16.0.1-hadd5161_0 \n","  libogg             conda-forge/linux-64::libogg-1.3.4-h7f98852_1 \n","  libopencv          conda-forge/linux-64::libopencv-4.7.0-py310h4120a83_2 \n","  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n","  libpciaccess       conda-forge/linux-64::libpciaccess-0.17-h166bdaf_0 \n","  libpq              conda-forge/linux-64::libpq-15.3-hbcd7760_1 \n","  libsndfile         conda-forge/linux-64::libsndfile-1.2.0-hb75c966_0 \n","  libsystemd0        conda-forge/linux-64::libsystemd0-253-h8c4010b_1 \n","  libtasn1           conda-forge/linux-64::libtasn1-4.19.0-h166bdaf_0 \n","  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n","  libva              conda-forge/linux-64::libva-2.18.0-h0b41bf4_0 \n","  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h9c3ff4c_0 \n","  libvpx             conda-forge/linux-64::libvpx-1.11.0-h9c3ff4c_3 \n","  libxkbcommon       conda-forge/linux-64::libxkbcommon-1.5.0-h79f4944_0 \n","  mpg123             conda-forge/linux-64::mpg123-1.31.3-hcb278e6_0 \n","  mysql-common       conda-forge/linux-64::mysql-common-8.0.33-hf1915f5_0 \n","  mysql-libs         conda-forge/linux-64::mysql-libs-8.0.33-hca2cd23_0 \n","  nettle             conda-forge/linux-64::nettle-3.8.1-hc379101_1 \n","  nspr               conda-forge/linux-64::nspr-4.35-h27087fc_0 \n","  nss                conda-forge/linux-64::nss-3.89-he45b914_0 \n","  opencv             conda-forge/linux-64::opencv-4.7.0-py310hff52083_2 \n","  openh264           conda-forge/linux-64::openh264-2.3.1-hcb278e6_2 \n","  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n","  packaging          conda-forge/noarch::packaging-23.1-pyhd8ed1ab_0 \n","  pcre2              conda-forge/linux-64::pcre2-10.40-hc3806b6_0 \n","  pixman             conda-forge/linux-64::pixman-0.40.0-h36c2ea0_0 \n","  pulseaudio-client  conda-forge/linux-64::pulseaudio-client-16.1-hb77b528_4 \n","  py-opencv          conda-forge/linux-64::py-opencv-4.7.0-py310hfdc917e_2 \n","  qt-main            conda-forge/linux-64::qt-main-5.15.8-h5c52f38_9 \n","  svt-av1            conda-forge/linux-64::svt-av1-1.4.1-hcb278e6_0 \n","  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n","  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n","  xcb-util           conda-forge/linux-64::xcb-util-0.4.0-h166bdaf_0 \n","  xcb-util-image     conda-forge/linux-64::xcb-util-image-0.4.0-h166bdaf_0 \n","  xcb-util-keysyms   conda-forge/linux-64::xcb-util-keysyms-0.4.0-h166bdaf_0 \n","  xcb-util-renderut~ conda-forge/linux-64::xcb-util-renderutil-0.3.9-h166bdaf_0 \n","  xcb-util-wm        conda-forge/linux-64::xcb-util-wm-0.4.1-h166bdaf_0 \n","  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n","  xorg-inputproto    conda-forge/linux-64::xorg-inputproto-2.3.2-h7f98852_1002 \n","  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n","  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n","  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n","  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.4-h0b41bf4_0 \n","  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n","  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n","  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.7.10-h7f98852_0 \n","  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h7f98852_1003 \n","  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n","  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n","  xorg-xf86vidmodep~ conda-forge/linux-64::xorg-xf86vidmodeproto-2.3.1-h7f98852_1002 \n","  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n","\n","The following packages will be UPDATED:\n","\n","  conda              anaconda::conda-23.1.0-py310h06a4308_0 --> conda-forge::conda-23.3.1-py310hff52083_0 \n","  h5py                 anaconda::h5py-3.7.0-py310he06866b_0 --> conda-forge::h5py-3.9.0-nompi_py310h367e799_100 \n","  hdf5                     anaconda::hdf5-1.10.6-h3ffc7dd_1 --> conda-forge::hdf5-1.14.0-nompi_hb72d44e_103 \n","\n","The following packages will be DOWNGRADED:\n","\n","  libxcb                                    1.15-h0b41bf4_0 --> 1.13-h7f98852_1004 \n","  pillow                              9.5.0-py310h582fbeb_1 --> 9.5.0-py310h065c6d2_0 \n","\n","\n","\n","Downloading and Extracting Packages\n","libopencv-4.7.0      | 29.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n","expat-2.5.0          | 134 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","libgcrypt-1.10.1     | 703 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","freeglut-3.2.2       | 162 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","xcb-util-renderutil- | 15 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libevent-2.1.10      | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","mysql-libs-8.0.33    | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","font-ttf-ubuntu-0.83 | 1.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","packaging-23.1       | 45 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","hdf5-1.14.0          | 3.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","openh264-2.3.1       | 702 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libvpx-1.11.0        | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","fonts-conda-forge-1  | 4 KB      | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","conda-23.3.1         | 941 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libsndfile-1.2.0     | 342 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","aom-3.5.0            | 2.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libogg-1.3.4         | 206 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xcb-util-keysyms-0.4 | 12 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xorg-libxext-1.3.4   | 49 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","xcb-util-renderutil- | 15 KB     | : 100% 1.0/1 [00:00<00:00,  4.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :   0% 0.0005370662057023227/1 [00:00<07:13, 433.46s/it]\n","expat-2.5.0          | 134 KB    | :  12% 0.11978534559651406/1 [00:00<00:01,  1.95s/it]\u001b[A\n","\n","libgcrypt-1.10.1     | 703 KB    | :   2% 0.022769438588250337/1 [00:00<00:10, 10.29s/it]\u001b[A\u001b[A\n","\n","\n","\n","xcb-util-renderutil- | 15 KB     | : 100% 1.0/1 [00:00<00:00,  4.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libevent-2.1.10      | 1.1 MB    | :   1% 0.014014103059768645/1 [00:00<00:18, 19.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","mysql-libs-8.0.33    | 1.5 MB    | :   1% 0.010702341137123745/1 [00:00<00:26, 27.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :   5% 0.05478075298163692/1 [00:00<00:04,  4.85s/it]   \n","\n","\n","\n","\n","\n","\n","\n","packaging-23.1       | 45 KB     | :  36% 0.3554167208989544/1 [00:00<00:00,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","hdf5-1.14.0          | 3.4 MB    | :   0% 0.004561661219810038/1 [00:00<01:22, 82.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","openh264-2.3.1       | 702 KB    | :   2% 0.0227943375882578/1 [00:00<00:16, 17.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libvpx-1.11.0        | 1.1 MB    | :   1% 0.014286486609034739/1 [00:00<00:28, 28.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","font-ttf-ubuntu-0.83 | 1.9 MB    | :  87% 0.8687881734317249/1 [00:00<00:00,  2.68it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","expat-2.5.0          | 134 KB    | : 100% 1.0/1 [00:00<00:00,  2.67it/s]                \u001b[A\n","libopencv-4.7.0      | 29.1 MB   | :  10% 0.09935724805492971/1 [00:00<00:03,  3.58s/it]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","conda-23.3.1         | 941 KB    | :   2% 0.01699687325585979/1 [00:00<00:27, 27.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","fonts-conda-forge-1  | 4 KB      | : 100% 1.0/1 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","hdf5-1.14.0          | 3.4 MB    | :  58% 0.5793309749158749/1 [00:00<00:00,  1.57it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libsndfile-1.2.0     | 342 KB    | :   5% 0.04679097423126462/1 [00:00<00:10, 11.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :  15% 0.15413800103656664/1 [00:00<00:02,  2.76s/it]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libogg-1.3.4         | 206 KB    | :   8% 0.0778152457848492/1 [00:00<00:06,  7.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xcb-util-keysyms-0.4 | 12 KB     | : 100% 1.0/1 [00:00<00:00,  1.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","freeglut-3.2.2       | 162 KB    | : 100% 1.0/1 [00:00<00:00,  1.82it/s]               \u001b[A\u001b[A\u001b[A\n","\n","\n","freeglut-3.2.2       | 162 KB    | : 100% 1.0/1 [00:00<00:00,  1.82it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xorg-libxext-1.3.4   | 49 KB     | :  33% 0.326745507847556/1 [00:00<00:01,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :  27% 0.2653107056169474/1 [00:00<00:01,  2.19s/it] \n","\n","\n","\n","\n","\n","\n","\n","packaging-23.1       | 45 KB     | : 100% 1.0/1 [00:00<00:00,  1.35it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :  56% 0.5612341849589273/1 [00:01<00:00,  1.93s/it]\n","\n","libgcrypt-1.10.1     | 703 KB    | : 100% 1.0/1 [00:01<00:00,  1.31s/it]                 \u001b[A\u001b[A\n","\n","libopencv-4.7.0      | 29.1 MB   | :  85% 0.8458792739811583/1 [00:01<00:00,  1.69s/it]\n","\n","\n","\n","\n","\n","mysql-libs-8.0.33    | 1.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.67s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | :  94% 0.9404029261847671/1 [00:01<00:00,  1.45s/it]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","openh264-2.3.1       | 702 KB    | : 100% 1.0/1 [00:01<00:00,  1.83s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","openh264-2.3.1       | 702 KB    | : 100% 1.0/1 [00:01<00:00,  1.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libevent-2.1.10      | 1.1 MB    | : 100% 1.0/1 [00:02<00:00,  2.68s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","libevent-2.1.10      | 1.1 MB    | : 100% 1.0/1 [00:02<00:00,  2.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","font-ttf-ubuntu-0.83 | 1.9 MB    | : 100% 1.0/1 [00:03<00:00,  2.68it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","fonts-conda-forge-1  | 4 KB      | : 100% 1.0/1 [00:03<00:00,  2.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libvpx-1.11.0        | 1.1 MB    | : 100% 1.0/1 [00:03<00:00,  3.26s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libvpx-1.11.0        | 1.1 MB    | : 100% 1.0/1 [00:03<00:00,  3.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libsndfile-1.2.0     | 342 KB    | : 100% 1.0/1 [00:03<00:00,  3.41s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libsndfile-1.2.0     | 342 KB    | : 100% 1.0/1 [00:03<00:00,  3.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","conda-23.3.1         | 941 KB    | : 100% 1.0/1 [00:04<00:00,  4.27s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","conda-23.3.1         | 941 KB    | : 100% 1.0/1 [00:04<00:00,  4.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libogg-1.3.4         | 206 KB    | : 100% 1.0/1 [00:04<00:00,  4.73s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libogg-1.3.4         | 206 KB    | : 100% 1.0/1 [00:04<00:00,  4.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xcb-util-keysyms-0.4 | 12 KB     | : 100% 1.0/1 [00:04<00:00,  1.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xorg-libxext-1.3.4   | 49 KB     | : 100% 1.0/1 [00:04<00:00,  5.26s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","xorg-libxext-1.3.4   | 49 KB     | : 100% 1.0/1 [00:04<00:00,  5.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","hdf5-1.14.0          | 3.4 MB    | : 100% 1.0/1 [00:05<00:00,  5.99s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","hdf5-1.14.0          | 3.4 MB    | : 100% 1.0/1 [00:05<00:00,  5.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","libopencv-4.7.0      | 29.1 MB   | : 100% 1.0/1 [00:09<00:00,  1.45s/it]               \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n","Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Collecting open3d\n","  Downloading open3d-0.17.0-cp310-cp310-manylinux_2_27_x86_64.whl (420.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m420.5/420.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting werkzeug>=2.2.3\n","  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dash>=2.6.0\n","  Downloading dash-2.11.0-py3-none-any.whl (10.4 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyyaml>=5.4.1\n","  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas>=1.0\n","  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from open3d) (4.65.0)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/site-packages (from open3d) (9.5.0)\n","Collecting configargparse\n","  Downloading ConfigArgParse-1.5.5-py3-none-any.whl (25 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from open3d) (1.25.0)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/site-packages (from open3d) (1.2.0)\n","Collecting matplotlib>=3\n","  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Collecting pyquaternion\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Collecting ipywidgets>=8.0.4\n","  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nbformat==5.7.0\n","  Downloading nbformat-5.7.0-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastjsonschema\n","  Downloading fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\n","Collecting traitlets>=5.1\n","  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter-core\n","  Downloading jupyter_core-5.3.1-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonschema>=2.6\n","  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting retrying\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Collecting Flask<2.3.0,>=1.0.4\n","  Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nest-asyncio\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.28.2)\n","Collecting werkzeug>=2.2.3\n","  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dash-core-components==2.0.0\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Collecting dash-table==5.0.0\n","  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (4.6.3)\n","Collecting ansi2html\n","  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n","Collecting plotly>=5.0.0\n","  Downloading plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dash-html-components==2.0.0\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Collecting widgetsnbextension~=4.0.7\n","  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyterlab-widgets~=3.0.7\n","  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m198.2/198.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipython>=6.1.0\n","  Downloading ipython-8.14.0-py3-none-any.whl (798 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m798.7/798.7 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipykernel>=4.5.1\n","  Downloading ipykernel-6.23.3-py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contourpy>=1.0.1\n","  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dateutil>=2.7\n","  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fonttools>=4.22.0\n","  Downloading fonttools-4.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kiwisolver>=1.0.1\n","  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->open3d) (23.1)\n","Collecting pyparsing>=2.3.1\n","  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler>=0.10\n","  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Collecting pytz>=2020.1\n","  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tzdata>=2022.1\n","  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (2.2.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.9.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.1.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=2.2.3->open3d) (2.1.3)\n","Collecting click>=8.0\n","  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itsdangerous>=2.0\n","  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/site-packages (from Flask<2.3.0,>=1.0.4->dash>=2.6.0->open3d) (3.1.2)\n","Collecting comm>=0.1.1\n","  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n","Collecting pyzmq>=20\n","  Downloading pyzmq-25.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting psutil\n","  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tornado>=6.1\n","  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter-client>=6.1.12\n","  Downloading jupyter_client-8.3.0-py3-none-any.whl (103 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib-inline>=0.1\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Collecting debugpy>=1.6.5\n","  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting backcall\n","  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pexpect>4.3\n","  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decorator\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting pygments>=2.4.0\n","  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30\n","  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting stack-data\n","  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n","Collecting pickleshare\n","  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n","Collecting attrs>=17.4.0\n","  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n","  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting platformdirs>=2.5\n","  Downloading platformdirs-3.8.0-py3-none-any.whl (16 kB)\n","Collecting tenacity>=6.2.0\n","  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n","Collecting six>=1.5\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (1.26.15)\n","Collecting parso<0.9.0,>=0.8.0\n","  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ptyprocess>=0.5\n","  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n","Collecting wcwidth\n","  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n","Collecting executing>=1.2.0\n","  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n","Collecting pure-eval\n","  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n","Collecting asttokens>=2.1.0\n","  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n","Installing collected packages: wcwidth, pytz, pure-eval, ptyprocess, pickleshare, fastjsonschema, executing, dash-table, dash-html-components, dash-core-components, backcall, addict, widgetsnbextension, werkzeug, tzdata, traitlets, tornado, tenacity, six, pyzmq, pyyaml, pyrsistent, pyquaternion, pyparsing, pygments, psutil, prompt-toolkit, platformdirs, pexpect, parso, nest-asyncio, kiwisolver, jupyterlab-widgets, itsdangerous, fonttools, decorator, debugpy, cycler, contourpy, configargparse, click, attrs, ansi2html, retrying, python-dateutil, plotly, matplotlib-inline, jupyter-core, jsonschema, jedi, Flask, comm, asttokens, stack-data, pandas, nbformat, matplotlib, jupyter-client, dash, ipython, ipykernel, ipywidgets, open3d\n","Successfully installed Flask-2.2.5 addict-2.4.0 ansi2html-1.8.0 asttokens-2.2.1 attrs-23.1.0 backcall-0.2.0 click-8.1.3 comm-0.1.3 configargparse-1.5.5 contourpy-1.1.0 cycler-0.11.0 dash-2.11.0 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 debugpy-1.6.7 decorator-5.1.1 executing-1.2.0 fastjsonschema-2.17.1 fonttools-4.40.0 ipykernel-6.23.3 ipython-8.14.0 ipywidgets-8.0.6 itsdangerous-2.1.2 jedi-0.18.2 jsonschema-4.17.3 jupyter-client-8.3.0 jupyter-core-5.3.1 jupyterlab-widgets-3.0.7 kiwisolver-1.4.4 matplotlib-3.7.1 matplotlib-inline-0.1.6 nbformat-5.7.0 nest-asyncio-1.5.6 open3d-0.17.0 pandas-2.0.2 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.8.0 plotly-5.15.0 prompt-toolkit-3.0.38 psutil-5.9.5 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.15.1 pyparsing-3.1.0 pyquaternion-0.9.9 pyrsistent-0.19.3 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 pyzmq-25.1.0 retrying-1.3.4 six-1.16.0 stack-data-0.6.2 tenacity-8.2.2 tornado-6.3.2 traitlets-5.9.0 tzdata-2023.3 wcwidth-0.2.6 werkzeug-2.2.3 widgetsnbextension-4.0.7\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cycler","dateutil","kiwisolver","matplotlib","matplotlib_inline","mpl_toolkits","pexpect","pickleshare","prompt_toolkit","psutil","six","wcwidth"]}}},"metadata":{}}],"source":["!conda install -c conda-forge trimesh -y\n","!conda install -c anaconda h5py -y\n","!conda install -c conda-forge plyfile -y\n","!conda install -c anaconda scikit-learn -y\n","!conda install pytorch torchvision -c pytorch -y\n","!conda install -c conda-forge glob2 -y\n","!conda install -c conda-forge torch-scatter -y\n","!conda install -c conda-forge opencv -y\n","#!conda install -c open3d-admin open3d -y\n","!pip install open3d"]},{"cell_type":"code","source":["\"\"\"!conda remove pytorch torchvision torchaudio cudatoolkit -y\n","\n","!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\"\"\""],"metadata":{"id":"CvErd6pFpjB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install open3d"],"metadata":{"id":"vDwsKSnWnxQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZhi_joIrxCc","executionInfo":{"status":"ok","timestamp":1687380039435,"user_tz":-120,"elapsed":254,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"a45b950f-03c3-4325-e4bb-c2b497a995b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}]},{"cell_type":"code","source":["!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"],"metadata":{"id":"wfnlG2o7sXby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687906562526,"user_tz":-120,"elapsed":99820,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"b43e93f1-5bf9-4b8e-c56b-cc5f58e912c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n","Collecting torch==1.11.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m624.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.12.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio===0.11.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch==1.11.0+cu113) (4.6.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torchvision==0.12.0+cu113) (1.25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torchvision==0.12.0+cu113) (2.28.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision==0.12.0+cu113) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cu113) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cu113) (2023.5.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cu113) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cu113) (3.4)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0\n","    Uninstalling torch-2.0.0:\n","      Successfully uninstalled torch-2.0.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2a0+072ec57\n","    Uninstalling torchvision-0.15.2a0+072ec57:\n","      Successfully uninstalled torchvision-0.15.2a0+072ec57\n","Successfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#!rm -r /content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_MSE_trainOnTop"],"metadata":{"id":"zQp_QEcRPNsa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVZk_H1PiImG"},"source":["### Run model"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOocEoKXiImI","executionInfo":{"status":"ok","timestamp":1687906712446,"user_tz":-120,"elapsed":229,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"5e433070-457f-4117-dd85-dbafa1f5ad5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dgcnn.pytorch\n"]}],"source":["%cd /content/drive/MyDrive/dgcnn.pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":901,"status":"ok","timestamp":1687522689630,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"},"user_tz":-120},"id":"dpwfKHdPEMb7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fdc35aa-b990-405e-884e-c1a6c983daa7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"code","source":["!rm -r /content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2"],"metadata":{"id":"L1z4eGhSK67b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10177020,"status":"ok","timestamp":1687916892018,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"},"user_tz":-120},"id":"WPwW30d3b1-j","outputId":"c7af61df-f51c-4416-9630-e9e639338b75"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mGÃ¶rÃ¼ntÃ¼lenen Ã§Ä±kÄ±ÅŸ son 5000 satÄ±ra kÄ±saltÄ±ldÄ±.\u001b[0m\n","Epoch:  5001 , current epoch train loss:  0.00194485392421484\n","Epoch:  5002 , current epoch train loss:  0.002227350138127804\n","Epoch:  5003 , current epoch train loss:  0.002054253127425909\n","Epoch:  5004 , current epoch train loss:  0.0020063254050910473\n","Epoch:  5005 , current epoch train loss:  0.0018940767040476203\n","Epoch:  5006 , current epoch train loss:  0.0019968277774751186\n","Epoch:  5007 , current epoch train loss:  0.0020454234909266233\n","Epoch:  5008 , current epoch train loss:  0.0019497566390782595\n","Epoch:  5009 , current epoch train loss:  0.002001882530748844\n","Epoch:  5010 , current epoch train loss:  0.001961223315447569\n","Epoch:  5011 , current epoch train loss:  0.0020609169732779264\n","Epoch:  5012 , current epoch train loss:  0.0021010811906307936\n","Epoch:  5013 , current epoch train loss:  0.0021683541126549244\n","Epoch:  5014 , current epoch train loss:  0.001997290877625346\n","Epoch:  5015 , current epoch train loss:  0.002076838631182909\n","Epoch:  5016 , current epoch train loss:  0.002322276122868061\n","Epoch:  5017 , current epoch train loss:  0.0023270435631275177\n","Epoch:  5018 , current epoch train loss:  0.002145551610738039\n","Epoch:  5019 , current epoch train loss:  0.002061881124973297\n","Epoch:  5020 , current epoch train loss:  0.00206731422804296\n","Epoch:  5021 , current epoch train loss:  0.002089659683406353\n","Epoch:  5022 , current epoch train loss:  0.0020911837927997112\n","Epoch:  5023 , current epoch train loss:  0.0022153290919959545\n","Epoch:  5024 , current epoch train loss:  0.0024596108123660088\n","Epoch:  5025 , current epoch train loss:  0.0022492753341794014\n","Epoch:  5026 , current epoch train loss:  0.0019725856836885214\n","Epoch:  5027 , current epoch train loss:  0.0020244636107236147\n","Epoch:  5028 , current epoch train loss:  0.0020604589954018593\n","Epoch:  5029 , current epoch train loss:  0.0020998921245336533\n","Epoch:  5030 , current epoch train loss:  0.0021205947268754244\n","Epoch:  5031 , current epoch train loss:  0.002048415131866932\n","Epoch:  5032 , current epoch train loss:  0.002032260876148939\n","Epoch:  5033 , current epoch train loss:  0.002016679150983691\n","Epoch:  5034 , current epoch train loss:  0.001916790846735239\n","Epoch:  5035 , current epoch train loss:  0.0019377849530428648\n","Epoch:  5036 , current epoch train loss:  0.001980651170015335\n","Epoch:  5037 , current epoch train loss:  0.0018381287809461355\n","Epoch:  5038 , current epoch train loss:  0.002042230684310198\n","Epoch:  5039 , current epoch train loss:  0.0019950338173657656\n","Epoch:  5040 , current epoch train loss:  0.0021135639399290085\n","Epoch:  5041 , current epoch train loss:  0.0021382325794547796\n","Epoch:  5042 , current epoch train loss:  0.0019233981147408485\n","Epoch:  5043 , current epoch train loss:  0.0020257101859897375\n","Epoch:  5044 , current epoch train loss:  0.0021059680730104446\n","Epoch:  5045 , current epoch train loss:  0.002176456619054079\n","Epoch:  5046 , current epoch train loss:  0.0020170980133116245\n","Epoch:  5047 , current epoch train loss:  0.0021431511268019676\n","Epoch:  5048 , current epoch train loss:  0.0021650390699505806\n","Epoch:  5049 , current epoch train loss:  0.002242638263851404\n","Epoch:  5050 , current epoch train loss:  0.002032097429037094\n","Epoch:  5051 , current epoch train loss:  0.0023081586696207523\n","Epoch:  5052 , current epoch train loss:  0.002066246932372451\n","Epoch:  5053 , current epoch train loss:  0.002002087887376547\n","Epoch:  5054 , current epoch train loss:  0.002019628183916211\n","Epoch:  5055 , current epoch train loss:  0.001918829046189785\n","Epoch:  5056 , current epoch train loss:  0.0020972436759620905\n","Epoch:  5057 , current epoch train loss:  0.0020815860480070114\n","Epoch:  5058 , current epoch train loss:  0.0019643628038465977\n","Epoch:  5059 , current epoch train loss:  0.0026634109672158957\n","Epoch:  5060 , current epoch train loss:  0.0021851244382560253\n","Epoch:  5061 , current epoch train loss:  0.0026316489093005657\n","Epoch:  5062 , current epoch train loss:  0.0020838945638388395\n","Epoch:  5063 , current epoch train loss:  0.002142564859241247\n","Epoch:  5064 , current epoch train loss:  0.0027109382208436728\n","Epoch:  5065 , current epoch train loss:  0.0022501572966575623\n","Epoch:  5066 , current epoch train loss:  0.001963905291631818\n","Epoch:  5067 , current epoch train loss:  0.0023125221487134695\n","Epoch:  5068 , current epoch train loss:  0.00222750473767519\n","Epoch:  5069 , current epoch train loss:  0.0020560133270919323\n","Epoch:  5070 , current epoch train loss:  0.002513499464839697\n","Epoch:  5071 , current epoch train loss:  0.0023565171286463737\n","Epoch:  5072 , current epoch train loss:  0.002030543750151992\n","Epoch:  5073 , current epoch train loss:  0.0030135950073599815\n","Epoch:  5074 , current epoch train loss:  0.002313347067683935\n","Epoch:  5075 , current epoch train loss:  0.002087321598082781\n","Epoch:  5076 , current epoch train loss:  0.0022529957350343466\n","Epoch:  5077 , current epoch train loss:  0.0023161727003753185\n","Epoch:  5078 , current epoch train loss:  0.0020034555345773697\n","Epoch:  5079 , current epoch train loss:  0.0021715709008276463\n","Epoch:  5080 , current epoch train loss:  0.002435381757095456\n","Epoch:  5081 , current epoch train loss:  0.0020717333536595106\n","Epoch:  5082 , current epoch train loss:  0.0021621990017592907\n","Epoch:  5083 , current epoch train loss:  0.0022980226203799248\n","Epoch:  5084 , current epoch train loss:  0.0020522584673017263\n","Epoch:  5085 , current epoch train loss:  0.002213130472227931\n","Epoch:  5086 , current epoch train loss:  0.0025312912184745073\n","Epoch:  5087 , current epoch train loss:  0.0023907178547233343\n","Epoch:  5088 , current epoch train loss:  0.0021575973369181156\n","Epoch:  5089 , current epoch train loss:  0.0024040089920163155\n","Epoch:  5090 , current epoch train loss:  0.002310488373041153\n","Epoch:  5091 , current epoch train loss:  0.0023305858485400677\n","Epoch:  5092 , current epoch train loss:  0.002355987671762705\n","Epoch:  5093 , current epoch train loss:  0.002275590319186449\n","Epoch:  5094 , current epoch train loss:  0.0023002997040748596\n","Epoch:  5095 , current epoch train loss:  0.002210746519267559\n","Epoch:  5096 , current epoch train loss:  0.0023630736395716667\n","Epoch:  5097 , current epoch train loss:  0.002237606095150113\n","Epoch:  5098 , current epoch train loss:  0.00215243361890316\n","Epoch:  5099 , current epoch train loss:  0.0020820791833102703\n","Epoch:  5100 , current epoch train loss:  0.002444628393277526\n","Epoch:  5101 , current epoch train loss:  0.0022269950713962317\n","Epoch:  5102 , current epoch train loss:  0.0021708193235099316\n","Epoch:  5103 , current epoch train loss:  0.0021842033602297306\n","Epoch:  5104 , current epoch train loss:  0.002187508624047041\n","Epoch:  5105 , current epoch train loss:  0.002077236771583557\n","Epoch:  5106 , current epoch train loss:  0.002200853079557419\n","Epoch:  5107 , current epoch train loss:  0.002104184590280056\n","Epoch:  5108 , current epoch train loss:  0.0023723128251731396\n","Epoch:  5109 , current epoch train loss:  0.0021751129534095526\n","Epoch:  5110 , current epoch train loss:  0.0023050042800605297\n","Epoch:  5111 , current epoch train loss:  0.0022520283237099648\n","Epoch:  5112 , current epoch train loss:  0.002170673105865717\n","Epoch:  5113 , current epoch train loss:  0.0024979910813272\n","Epoch:  5114 , current epoch train loss:  0.002328768139705062\n","Epoch:  5115 , current epoch train loss:  0.0022208495065569878\n","Epoch:  5116 , current epoch train loss:  0.0022624556440860033\n","Epoch:  5117 , current epoch train loss:  0.002698009368032217\n","Epoch:  5118 , current epoch train loss:  0.0021116910502314568\n","Epoch:  5119 , current epoch train loss:  0.002053961157798767\n","Epoch:  5120 , current epoch train loss:  0.0020682362373918295\n","Epoch:  5121 , current epoch train loss:  0.002546881325542927\n","Epoch:  5122 , current epoch train loss:  0.002083666156977415\n","Epoch:  5123 , current epoch train loss:  0.0022189521696418524\n","Epoch:  5124 , current epoch train loss:  0.0020212444942444563\n","Epoch:  5125 , current epoch train loss:  0.0019569657742977142\n","Epoch:  5126 , current epoch train loss:  0.002071882365271449\n","Epoch:  5127 , current epoch train loss:  0.0020326694939285517\n","Epoch:  5128 , current epoch train loss:  0.001913801534101367\n","Epoch:  5129 , current epoch train loss:  0.0021146032959222794\n","Epoch:  5130 , current epoch train loss:  0.00238247774541378\n","Epoch:  5131 , current epoch train loss:  0.001858019968494773\n","Epoch:  5132 , current epoch train loss:  0.0024582489859312773\n","Epoch:  5133 , current epoch train loss:  0.002113417722284794\n","Epoch:  5134 , current epoch train loss:  0.0020101298578083515\n","Epoch:  5135 , current epoch train loss:  0.0021627440582960844\n","Epoch:  5136 , current epoch train loss:  0.002082148101180792\n","Epoch:  5137 , current epoch train loss:  0.002124234801158309\n","Epoch:  5138 , current epoch train loss:  0.002200046554207802\n","Epoch:  5139 , current epoch train loss:  0.00195796275511384\n","Epoch:  5140 , current epoch train loss:  0.002037383383139968\n","Epoch:  5141 , current epoch train loss:  0.0021202433854341507\n","Epoch:  5142 , current epoch train loss:  0.0021250194404274225\n","Epoch:  5143 , current epoch train loss:  0.0019739696290344\n","Epoch:  5144 , current epoch train loss:  0.0020323097705841064\n","Epoch:  5145 , current epoch train loss:  0.002298330655321479\n","Epoch:  5146 , current epoch train loss:  0.0019380305893719196\n","Epoch:  5147 , current epoch train loss:  0.0018654137384146452\n","Epoch:  5148 , current epoch train loss:  0.0019956170581281185\n","Epoch:  5149 , current epoch train loss:  0.002012908225879073\n","Epoch:  5150 , current epoch train loss:  0.002014873083680868\n","Epoch:  5151 , current epoch train loss:  0.002031811513006687\n","Epoch:  5152 , current epoch train loss:  0.0018162766937166452\n","Epoch:  5153 , current epoch train loss:  0.0021655892487615347\n","Epoch:  5154 , current epoch train loss:  0.002159982454031706\n","Epoch:  5155 , current epoch train loss:  0.0021164254285395145\n","Epoch:  5156 , current epoch train loss:  0.0021657319739460945\n","Epoch:  5157 , current epoch train loss:  0.002013330115005374\n","Epoch:  5158 , current epoch train loss:  0.0020004012621939182\n","Epoch:  5159 , current epoch train loss:  0.0020725794602185488\n","Epoch:  5160 , current epoch train loss:  0.002189382677897811\n","Epoch:  5161 , current epoch train loss:  0.002074835356324911\n","Epoch:  5162 , current epoch train loss:  0.0020075589418411255\n","Epoch:  5163 , current epoch train loss:  0.002120956312865019\n","Epoch:  5164 , current epoch train loss:  0.0021015333477407694\n","Epoch:  5165 , current epoch train loss:  0.0020869197323918343\n","Epoch:  5166 , current epoch train loss:  0.002231486141681671\n","Epoch:  5167 , current epoch train loss:  0.0020778323523700237\n","Epoch:  5168 , current epoch train loss:  0.0019761137664318085\n","Epoch:  5169 , current epoch train loss:  0.0020322962664067745\n","Epoch:  5170 , current epoch train loss:  0.002093203831464052\n","Epoch:  5171 , current epoch train loss:  0.0019561524968594313\n","Epoch:  5172 , current epoch train loss:  0.0023506467696279287\n","Epoch:  5173 , current epoch train loss:  0.0020050012972205877\n","Epoch:  5174 , current epoch train loss:  0.001926455064676702\n","Epoch:  5175 , current epoch train loss:  0.0020165033638477325\n","Epoch:  5176 , current epoch train loss:  0.0019292662618681788\n","Epoch:  5177 , current epoch train loss:  0.0021034381352365017\n","Epoch:  5178 , current epoch train loss:  0.0020215092226862907\n","Epoch:  5179 , current epoch train loss:  0.002067016903311014\n","Epoch:  5180 , current epoch train loss:  0.0019688736647367477\n","Epoch:  5181 , current epoch train loss:  0.0019007893279194832\n","Epoch:  5182 , current epoch train loss:  0.0019955732859671116\n","Epoch:  5183 , current epoch train loss:  0.00202514068223536\n","Epoch:  5184 , current epoch train loss:  0.0020163487643003464\n","Epoch:  5185 , current epoch train loss:  0.00208241306245327\n","Epoch:  5186 , current epoch train loss:  0.0020164218731224537\n","Epoch:  5187 , current epoch train loss:  0.00191430130507797\n","Epoch:  5188 , current epoch train loss:  0.0019420853350311518\n","Epoch:  5189 , current epoch train loss:  0.0019439681200310588\n","Epoch:  5190 , current epoch train loss:  0.0019500905182212591\n","Epoch:  5191 , current epoch train loss:  0.0019744287710636854\n","Epoch:  5192 , current epoch train loss:  0.0019548973068594933\n","Epoch:  5193 , current epoch train loss:  0.0017988327890634537\n","Epoch:  5194 , current epoch train loss:  0.0019737279508262873\n","Epoch:  5195 , current epoch train loss:  0.0018871387001127005\n","Epoch:  5196 , current epoch train loss:  0.0021347077563405037\n","Epoch:  5197 , current epoch train loss:  0.0019008803647011518\n","Epoch:  5198 , current epoch train loss:  0.0019389940425753593\n","Epoch:  5199 , current epoch train loss:  0.00198926473967731\n","Epoch:  5200 , current epoch train loss:  0.0019828248769044876\n","Epoch:  5201 , current epoch train loss:  0.0019393509719520807\n","Epoch:  5202 , current epoch train loss:  0.0018684163223952055\n","Epoch:  5203 , current epoch train loss:  0.001987928058952093\n","Epoch:  5204 , current epoch train loss:  0.0019226602744311094\n","Epoch:  5205 , current epoch train loss:  0.0018607083475217223\n","Epoch:  5206 , current epoch train loss:  0.001823011552914977\n","Epoch:  5207 , current epoch train loss:  0.001989582320675254\n","Epoch:  5208 , current epoch train loss:  0.001977485604584217\n","Epoch:  5209 , current epoch train loss:  0.0018753123003989458\n","Epoch:  5210 , current epoch train loss:  0.001976544037461281\n","Epoch:  5211 , current epoch train loss:  0.001840647659264505\n","Epoch:  5212 , current epoch train loss:  0.001838045078329742\n","Epoch:  5213 , current epoch train loss:  0.0017731342231854796\n","Epoch:  5214 , current epoch train loss:  0.0018956739222630858\n","Epoch:  5215 , current epoch train loss:  0.0019319219281896949\n","Epoch:  5216 , current epoch train loss:  0.0017737317830324173\n","Epoch:  5217 , current epoch train loss:  0.0020064059644937515\n","Epoch:  5218 , current epoch train loss:  0.002000615932047367\n","Epoch:  5219 , current epoch train loss:  0.001971255987882614\n","Epoch:  5220 , current epoch train loss:  0.0018261962104588747\n","Epoch:  5221 , current epoch train loss:  0.0021012029610574245\n","Epoch:  5222 , current epoch train loss:  0.002002076245844364\n","Epoch:  5223 , current epoch train loss:  0.0018789954483509064\n","Epoch:  5224 , current epoch train loss:  0.0020731440745294094\n","Epoch:  5225 , current epoch train loss:  0.002004974987357855\n","Epoch:  5226 , current epoch train loss:  0.0021130070090293884\n","Epoch:  5227 , current epoch train loss:  0.0018524446059018373\n","Epoch:  5228 , current epoch train loss:  0.001968475989997387\n","Epoch:  5229 , current epoch train loss:  0.002167467726394534\n","Epoch:  5230 , current epoch train loss:  0.002053153235465288\n","Epoch:  5231 , current epoch train loss:  0.0019872139673680067\n","Epoch:  5232 , current epoch train loss:  0.0019408816006034613\n","Epoch:  5233 , current epoch train loss:  0.0020003346726298332\n","Epoch:  5234 , current epoch train loss:  0.002091054804623127\n","Epoch:  5235 , current epoch train loss:  0.0018182167550548911\n","Epoch:  5236 , current epoch train loss:  0.00213880417868495\n","Epoch:  5237 , current epoch train loss:  0.002084691310301423\n","Epoch:  5238 , current epoch train loss:  0.002070784568786621\n","Epoch:  5239 , current epoch train loss:  0.0019008375238627195\n","Epoch:  5240 , current epoch train loss:  0.002063675317913294\n","Epoch:  5241 , current epoch train loss:  0.0020396271720528603\n","Epoch:  5242 , current epoch train loss:  0.002221126575022936\n","Epoch:  5243 , current epoch train loss:  0.0022235438227653503\n","Epoch:  5244 , current epoch train loss:  0.002018633997067809\n","Epoch:  5245 , current epoch train loss:  0.0021825945004820824\n","Epoch:  5246 , current epoch train loss:  0.002094188006594777\n","Epoch:  5247 , current epoch train loss:  0.0020455329213291407\n","Epoch:  5248 , current epoch train loss:  0.002135652583092451\n","Epoch:  5249 , current epoch train loss:  0.0020784521475434303\n","Epoch:  5250 , current epoch train loss:  0.0019897075835615396\n","Epoch:  5251 , current epoch train loss:  0.0021084786858409643\n","Epoch:  5252 , current epoch train loss:  0.0021608187817037106\n","Epoch:  5253 , current epoch train loss:  0.00213414104655385\n","Epoch:  5254 , current epoch train loss:  0.0020571197383105755\n","Epoch:  5255 , current epoch train loss:  0.0020637172274291515\n","Epoch:  5256 , current epoch train loss:  0.0020279334858059883\n","Epoch:  5257 , current epoch train loss:  0.001980667468160391\n","Epoch:  5258 , current epoch train loss:  0.0020835145842283964\n","Epoch:  5259 , current epoch train loss:  0.0017944572027772665\n","Epoch:  5260 , current epoch train loss:  0.0020232340320944786\n","Epoch:  5261 , current epoch train loss:  0.002159703290089965\n","Epoch:  5262 , current epoch train loss:  0.0019955807365477085\n","Epoch:  5263 , current epoch train loss:  0.001973069040104747\n","Epoch:  5264 , current epoch train loss:  0.001952747697941959\n","Epoch:  5265 , current epoch train loss:  0.001946635777130723\n","Epoch:  5266 , current epoch train loss:  0.0018357078079134226\n","Epoch:  5267 , current epoch train loss:  0.0021351161412894726\n","Epoch:  5268 , current epoch train loss:  0.001987895928323269\n","Epoch:  5269 , current epoch train loss:  0.001921777962706983\n","Epoch:  5270 , current epoch train loss:  0.0021189560648053885\n","Epoch:  5271 , current epoch train loss:  0.0019463372882455587\n","Epoch:  5272 , current epoch train loss:  0.0019359522266313434\n","Epoch:  5273 , current epoch train loss:  0.002431771717965603\n","Epoch:  5274 , current epoch train loss:  0.0020530736073851585\n","Epoch:  5275 , current epoch train loss:  0.001986656803637743\n","Epoch:  5276 , current epoch train loss:  0.0020369417034089565\n","Epoch:  5277 , current epoch train loss:  0.0018112215911969543\n","Epoch:  5278 , current epoch train loss:  0.0019234496867284179\n","Epoch:  5279 , current epoch train loss:  0.0020786277018487453\n","Epoch:  5280 , current epoch train loss:  0.0018558832816779613\n","Epoch:  5281 , current epoch train loss:  0.001982258167117834\n","Epoch:  5282 , current epoch train loss:  0.0019558113999664783\n","Epoch:  5283 , current epoch train loss:  0.0019423075718805194\n","Epoch:  5284 , current epoch train loss:  0.001808740897104144\n","Epoch:  5285 , current epoch train loss:  0.0018499812576919794\n","Epoch:  5286 , current epoch train loss:  0.0018505377229303122\n","Epoch:  5287 , current epoch train loss:  0.002014748053625226\n","Epoch:  5288 , current epoch train loss:  0.0019710243213921785\n","Epoch:  5289 , current epoch train loss:  0.0018829288892447948\n","Epoch:  5290 , current epoch train loss:  0.0019624861888587475\n","Epoch:  5291 , current epoch train loss:  0.0020436355844140053\n","Epoch:  5292 , current epoch train loss:  0.002063106745481491\n","Epoch:  5293 , current epoch train loss:  0.002033369382843375\n","Epoch:  5294 , current epoch train loss:  0.0021501078736037016\n","Epoch:  5295 , current epoch train loss:  0.0021226145327091217\n","Epoch:  5296 , current epoch train loss:  0.002064038533717394\n","Epoch:  5297 , current epoch train loss:  0.002390045439824462\n","Epoch:  5298 , current epoch train loss:  0.001975605497136712\n","Epoch:  5299 , current epoch train loss:  0.001941874623298645\n","Epoch:  5300 , current epoch train loss:  0.0022983781527727842\n","Epoch:  5301 , current epoch train loss:  0.0020868713036179543\n","Epoch:  5302 , current epoch train loss:  0.0018797686789184809\n","Epoch:  5303 , current epoch train loss:  0.0022800727747380733\n","Epoch:  5304 , current epoch train loss:  0.0020427771378308535\n","Epoch:  5305 , current epoch train loss:  0.0019463652279227972\n","Epoch:  5306 , current epoch train loss:  0.0019997390918433666\n","Epoch:  5307 , current epoch train loss:  0.0019853152334690094\n","Epoch:  5308 , current epoch train loss:  0.001941436785273254\n","Epoch:  5309 , current epoch train loss:  0.0019102559890598059\n","Epoch:  5310 , current epoch train loss:  0.0018932049861177802\n","Epoch:  5311 , current epoch train loss:  0.0018925059121102095\n","Epoch:  5312 , current epoch train loss:  0.0019024033099412918\n","Epoch:  5313 , current epoch train loss:  0.0017957582604140043\n","Epoch:  5314 , current epoch train loss:  0.0018710617441684008\n","Epoch:  5315 , current epoch train loss:  0.0018981453031301498\n","Epoch:  5316 , current epoch train loss:  0.0018263093661516905\n","Epoch:  5317 , current epoch train loss:  0.002139803720638156\n","Epoch:  5318 , current epoch train loss:  0.0020373351871967316\n","Epoch:  5319 , current epoch train loss:  0.002162794815376401\n","Epoch:  5320 , current epoch train loss:  0.00202626152895391\n","Epoch:  5321 , current epoch train loss:  0.0020341388881206512\n","Epoch:  5322 , current epoch train loss:  0.001984178787097335\n","Epoch:  5323 , current epoch train loss:  0.0019721759017556906\n","Epoch:  5324 , current epoch train loss:  0.0022264542058110237\n","Epoch:  5325 , current epoch train loss:  0.0019630545284599066\n","Epoch:  5326 , current epoch train loss:  0.002008820418268442\n","Epoch:  5327 , current epoch train loss:  0.0020695615094155073\n","Epoch:  5328 , current epoch train loss:  0.002209051977843046\n","Epoch:  5329 , current epoch train loss:  0.0021878958214074373\n","Epoch:  5330 , current epoch train loss:  0.002108984161168337\n","Epoch:  5331 , current epoch train loss:  0.002149492036551237\n","Epoch:  5332 , current epoch train loss:  0.0021844757720828056\n","Epoch:  5333 , current epoch train loss:  0.0018895973917096853\n","Epoch:  5334 , current epoch train loss:  0.0021900287829339504\n","Epoch:  5335 , current epoch train loss:  0.0020151229109615088\n","Epoch:  5336 , current epoch train loss:  0.0018880523275583982\n","Epoch:  5337 , current epoch train loss:  0.0020777243189513683\n","Epoch:  5338 , current epoch train loss:  0.0021872108336538076\n","Epoch:  5339 , current epoch train loss:  0.0018702561501413584\n","Epoch:  5340 , current epoch train loss:  0.0021570443641394377\n","Epoch:  5341 , current epoch train loss:  0.0021507018245756626\n","Epoch:  5342 , current epoch train loss:  0.002067282097414136\n","Epoch:  5343 , current epoch train loss:  0.0019182984251528978\n","Epoch:  5344 , current epoch train loss:  0.0020881304517388344\n","Epoch:  5345 , current epoch train loss:  0.001967385644093156\n","Epoch:  5346 , current epoch train loss:  0.0018475571414455771\n","Epoch:  5347 , current epoch train loss:  0.0019126062979921699\n","Epoch:  5348 , current epoch train loss:  0.00224787974730134\n","Epoch:  5349 , current epoch train loss:  0.0019676885567605495\n","Epoch:  5350 , current epoch train loss:  0.002094072289764881\n","Epoch:  5351 , current epoch train loss:  0.0022516916505992413\n","Epoch:  5352 , current epoch train loss:  0.00210289447568357\n","Epoch:  5353 , current epoch train loss:  0.002078730147331953\n","Epoch:  5354 , current epoch train loss:  0.0020565614104270935\n","Epoch:  5355 , current epoch train loss:  0.002095520030707121\n","Epoch:  5356 , current epoch train loss:  0.0019905539229512215\n","Epoch:  5357 , current epoch train loss:  0.002239649649709463\n","Epoch:  5358 , current epoch train loss:  0.002037562895566225\n","Epoch:  5359 , current epoch train loss:  0.001953920116648078\n","Epoch:  5360 , current epoch train loss:  0.0024016727693378925\n","Epoch:  5361 , current epoch train loss:  0.002128336112946272\n","Epoch:  5362 , current epoch train loss:  0.00192201544996351\n","Epoch:  5363 , current epoch train loss:  0.0020302978809922934\n","Epoch:  5364 , current epoch train loss:  0.002032030140981078\n","Epoch:  5365 , current epoch train loss:  0.0019201983232051134\n","Epoch:  5366 , current epoch train loss:  0.002010034630075097\n","Epoch:  5367 , current epoch train loss:  0.0020937840454280376\n","Epoch:  5368 , current epoch train loss:  0.0019204154377803206\n","Epoch:  5369 , current epoch train loss:  0.0020451180171221495\n","Epoch:  5370 , current epoch train loss:  0.001993611454963684\n","Epoch:  5371 , current epoch train loss:  0.0019707982428371906\n","Epoch:  5372 , current epoch train loss:  0.0019804006442427635\n","Epoch:  5373 , current epoch train loss:  0.002212036168202758\n","Epoch:  5374 , current epoch train loss:  0.0018912691157311201\n","Epoch:  5375 , current epoch train loss:  0.0018916146364063025\n","Epoch:  5376 , current epoch train loss:  0.0019173076143488288\n","Epoch:  5377 , current epoch train loss:  0.0018134850542992353\n","Epoch:  5378 , current epoch train loss:  0.001873378991149366\n","Epoch:  5379 , current epoch train loss:  0.0018997527658939362\n","Epoch:  5380 , current epoch train loss:  0.001952286809682846\n","Epoch:  5381 , current epoch train loss:  0.001964603317901492\n","Epoch:  5382 , current epoch train loss:  0.0019362885504961014\n","Epoch:  5383 , current epoch train loss:  0.001742177875712514\n","Epoch:  5384 , current epoch train loss:  0.001939262612722814\n","Epoch:  5385 , current epoch train loss:  0.0019055705051869154\n","Epoch:  5386 , current epoch train loss:  0.0018126768991351128\n","Epoch:  5387 , current epoch train loss:  0.002104334067553282\n","Epoch:  5388 , current epoch train loss:  0.0020871481392532587\n","Epoch:  5389 , current epoch train loss:  0.0021051152143627405\n","Epoch:  5390 , current epoch train loss:  0.0019454965367913246\n","Epoch:  5391 , current epoch train loss:  0.002002222463488579\n","Epoch:  5392 , current epoch train loss:  0.0021426628809422255\n","Epoch:  5393 , current epoch train loss:  0.002084750682115555\n","Epoch:  5394 , current epoch train loss:  0.002158666495233774\n","Epoch:  5395 , current epoch train loss:  0.0019589834846556187\n","Epoch:  5396 , current epoch train loss:  0.0018935820553451777\n","Epoch:  5397 , current epoch train loss:  0.0019511803984642029\n","Epoch:  5398 , current epoch train loss:  0.002144426805898547\n","Epoch:  5399 , current epoch train loss:  0.002028530463576317\n","Epoch:  5400 , current epoch train loss:  0.0019594128243625164\n","Epoch:  5401 , current epoch train loss:  0.0021980158053338528\n","Epoch:  5402 , current epoch train loss:  0.002039868850260973\n","Epoch:  5403 , current epoch train loss:  0.001847463077865541\n","Epoch:  5404 , current epoch train loss:  0.0020352944266051054\n","Epoch:  5405 , current epoch train loss:  0.002056002151221037\n","Epoch:  5406 , current epoch train loss:  0.002152208471670747\n","Epoch:  5407 , current epoch train loss:  0.001883033663034439\n","Epoch:  5408 , current epoch train loss:  0.002008669776841998\n","Epoch:  5409 , current epoch train loss:  0.0021583791822195053\n","Epoch:  5410 , current epoch train loss:  0.0019345872569829226\n","Epoch:  5411 , current epoch train loss:  0.0020818549674004316\n","Epoch:  5412 , current epoch train loss:  0.002023145090788603\n","Epoch:  5413 , current epoch train loss:  0.0019812420941889286\n","Epoch:  5414 , current epoch train loss:  0.0019051342969760299\n","Epoch:  5415 , current epoch train loss:  0.0019100599456578493\n","Epoch:  5416 , current epoch train loss:  0.0019993018358945847\n","Epoch:  5417 , current epoch train loss:  0.001973892329260707\n","Epoch:  5418 , current epoch train loss:  0.002136769238859415\n","Epoch:  5419 , current epoch train loss:  0.0020784500520676374\n","Epoch:  5420 , current epoch train loss:  0.001836091629229486\n","Epoch:  5421 , current epoch train loss:  0.0020716229919344187\n","Epoch:  5422 , current epoch train loss:  0.0019410103559494019\n","Epoch:  5423 , current epoch train loss:  0.00189223513007164\n","Epoch:  5424 , current epoch train loss:  0.0018291494343429804\n","Epoch:  5425 , current epoch train loss:  0.001917688176035881\n","Epoch:  5426 , current epoch train loss:  0.0022130184806883335\n","Epoch:  5427 , current epoch train loss:  0.00205197068862617\n","Epoch:  5428 , current epoch train loss:  0.0018480992875993252\n","Epoch:  5429 , current epoch train loss:  0.0018576623406261206\n","Epoch:  5430 , current epoch train loss:  0.0018594488501548767\n","Epoch:  5431 , current epoch train loss:  0.0018279728246852756\n","Epoch:  5432 , current epoch train loss:  0.00176618795376271\n","Epoch:  5433 , current epoch train loss:  0.001724392524920404\n","Epoch:  5434 , current epoch train loss:  0.001808097935281694\n","Epoch:  5435 , current epoch train loss:  0.0017531219637021422\n","Epoch:  5436 , current epoch train loss:  0.0017914536874741316\n","Epoch:  5437 , current epoch train loss:  0.0018703019013628364\n","Epoch:  5438 , current epoch train loss:  0.0017257817089557648\n","Epoch:  5439 , current epoch train loss:  0.0018352833576500416\n","Epoch:  5440 , current epoch train loss:  0.0019114082679152489\n","Epoch:  5441 , current epoch train loss:  0.001855451031588018\n","Epoch:  5442 , current epoch train loss:  0.0018149539828300476\n","Epoch:  5443 , current epoch train loss:  0.001944274641573429\n","Epoch:  5444 , current epoch train loss:  0.0019193622283637524\n","Epoch:  5445 , current epoch train loss:  0.0018413104116916656\n","Epoch:  5446 , current epoch train loss:  0.001846221275627613\n","Epoch:  5447 , current epoch train loss:  0.001976728904992342\n","Epoch:  5448 , current epoch train loss:  0.0019526223186403513\n","Epoch:  5449 , current epoch train loss:  0.002026870846748352\n","Epoch:  5450 , current epoch train loss:  0.0018997308798134327\n","Epoch:  5451 , current epoch train loss:  0.0020765734370797873\n","Epoch:  5452 , current epoch train loss:  0.0019290124764665961\n","Epoch:  5453 , current epoch train loss:  0.002110946923494339\n","Epoch:  5454 , current epoch train loss:  0.0020324743818491697\n","Epoch:  5455 , current epoch train loss:  0.0020203811582177877\n","Epoch:  5456 , current epoch train loss:  0.002196172485128045\n","Epoch:  5457 , current epoch train loss:  0.0018846611492335796\n","Epoch:  5458 , current epoch train loss:  0.002150058513507247\n","Epoch:  5459 , current epoch train loss:  0.002424887614324689\n","Epoch:  5460 , current epoch train loss:  0.00196463312022388\n","Epoch:  5461 , current epoch train loss:  0.002167376223951578\n","Epoch:  5462 , current epoch train loss:  0.002013766672462225\n","Epoch:  5463 , current epoch train loss:  0.002159212715923786\n","Epoch:  5464 , current epoch train loss:  0.0020784432999789715\n","Epoch:  5465 , current epoch train loss:  0.001839237054809928\n","Epoch:  5466 , current epoch train loss:  0.0019572086166590452\n","Epoch:  5467 , current epoch train loss:  0.002030908828601241\n","Epoch:  5468 , current epoch train loss:  0.0018342509865760803\n","Epoch:  5469 , current epoch train loss:  0.001886898186057806\n","Epoch:  5470 , current epoch train loss:  0.0019449203973636031\n","Epoch:  5471 , current epoch train loss:  0.001972632482647896\n","Epoch:  5472 , current epoch train loss:  0.0017937150551006198\n","Epoch:  5473 , current epoch train loss:  0.0019318744307383895\n","Epoch:  5474 , current epoch train loss:  0.0018636935856193304\n","Epoch:  5475 , current epoch train loss:  0.0018192225834354758\n","Epoch:  5476 , current epoch train loss:  0.0019002286717295647\n","Epoch:  5477 , current epoch train loss:  0.0018699802458286285\n","Epoch:  5478 , current epoch train loss:  0.0018150319810956717\n","Epoch:  5479 , current epoch train loss:  0.002006600145250559\n","Epoch:  5480 , current epoch train loss:  0.001912114443257451\n","Epoch:  5481 , current epoch train loss:  0.0018645292147994041\n","Epoch:  5482 , current epoch train loss:  0.0018418887630105019\n","Epoch:  5483 , current epoch train loss:  0.0018686939729377627\n","Epoch:  5484 , current epoch train loss:  0.0020838933996856213\n","Epoch:  5485 , current epoch train loss:  0.001986670307815075\n","Epoch:  5486 , current epoch train loss:  0.0023550186306238174\n","Epoch:  5487 , current epoch train loss:  0.0020229413639754057\n","Epoch:  5488 , current epoch train loss:  0.0019445093348622322\n","Epoch:  5489 , current epoch train loss:  0.0019119944190606475\n","Epoch:  5490 , current epoch train loss:  0.0020208777859807014\n","Epoch:  5491 , current epoch train loss:  0.002041519619524479\n","Epoch:  5492 , current epoch train loss:  0.0020422691013664007\n","Epoch:  5493 , current epoch train loss:  0.0018029342172667384\n","Epoch:  5494 , current epoch train loss:  0.001913893735036254\n","Epoch:  5495 , current epoch train loss:  0.002130684908479452\n","Epoch:  5496 , current epoch train loss:  0.002333862241357565\n","Epoch:  5497 , current epoch train loss:  0.0019162127282470465\n","Epoch:  5498 , current epoch train loss:  0.002178588882088661\n","Epoch:  5499 , current epoch train loss:  0.0019674976356327534\n","Epoch:  5500 , current epoch train loss:  0.0018475695978850126\n","Epoch:  5501 , current epoch train loss:  0.0020547350868582726\n","Epoch:  5502 , current epoch train loss:  0.0018573551205918193\n","Epoch:  5503 , current epoch train loss:  0.0018879675772041082\n","Epoch:  5504 , current epoch train loss:  0.002265503630042076\n","Epoch:  5505 , current epoch train loss:  0.0019485497614368796\n","Epoch:  5506 , current epoch train loss:  0.001941451570019126\n","Epoch:  5507 , current epoch train loss:  0.0018056582193821669\n","Epoch:  5508 , current epoch train loss:  0.0019899611361324787\n","Epoch:  5509 , current epoch train loss:  0.0022657583467662334\n","Epoch:  5510 , current epoch train loss:  0.0018056505359709263\n","Epoch:  5511 , current epoch train loss:  0.0022025960497558117\n","Epoch:  5512 , current epoch train loss:  0.0025546287652105093\n","Epoch:  5513 , current epoch train loss:  0.002080712467432022\n","Epoch:  5514 , current epoch train loss:  0.0019723521545529366\n","Epoch:  5515 , current epoch train loss:  0.001977498410269618\n","Epoch:  5516 , current epoch train loss:  0.0020072339102625847\n","Epoch:  5517 , current epoch train loss:  0.0020311602856963873\n","Epoch:  5518 , current epoch train loss:  0.0019348175264894962\n","Epoch:  5519 , current epoch train loss:  0.0020846715196967125\n","Epoch:  5520 , current epoch train loss:  0.002088934648782015\n","Epoch:  5521 , current epoch train loss:  0.001948020188137889\n","Epoch:  5522 , current epoch train loss:  0.0017887534340843558\n","Epoch:  5523 , current epoch train loss:  0.002191965002566576\n","Epoch:  5524 , current epoch train loss:  0.0019973753951489925\n","Epoch:  5525 , current epoch train loss:  0.0020695910789072514\n","Epoch:  5526 , current epoch train loss:  0.001870484440587461\n","Epoch:  5527 , current epoch train loss:  0.002046216046437621\n","Epoch:  5528 , current epoch train loss:  0.001933901570737362\n","Epoch:  5529 , current epoch train loss:  0.0018732892349362373\n","Epoch:  5530 , current epoch train loss:  0.0018704704707488418\n","Epoch:  5531 , current epoch train loss:  0.0019347798079252243\n","Epoch:  5532 , current epoch train loss:  0.002023496199399233\n","Epoch:  5533 , current epoch train loss:  0.002286503091454506\n","Epoch:  5534 , current epoch train loss:  0.0018994504353031516\n","Epoch:  5535 , current epoch train loss:  0.0018027903279289603\n","Epoch:  5536 , current epoch train loss:  0.002023611683398485\n","Epoch:  5537 , current epoch train loss:  0.0019344377797096968\n","Epoch:  5538 , current epoch train loss:  0.0017579005798324943\n","Epoch:  5539 , current epoch train loss:  0.0019064036896452308\n","Epoch:  5540 , current epoch train loss:  0.001965187257155776\n","Epoch:  5541 , current epoch train loss:  0.0018846327438950539\n","Epoch:  5542 , current epoch train loss:  0.0020560026168823242\n","Epoch:  5543 , current epoch train loss:  0.002008283045142889\n","Epoch:  5544 , current epoch train loss:  0.0022006817162036896\n","Epoch:  5545 , current epoch train loss:  0.0019545897375792265\n","Epoch:  5546 , current epoch train loss:  0.0020011337473988533\n","Epoch:  5547 , current epoch train loss:  0.001997878775000572\n","Epoch:  5548 , current epoch train loss:  0.001927248202264309\n","Epoch:  5549 , current epoch train loss:  0.002009338466450572\n","Epoch:  5550 , current epoch train loss:  0.0018838224932551384\n","Epoch:  5551 , current epoch train loss:  0.0018488363130018115\n","Epoch:  5552 , current epoch train loss:  0.0023023204412311316\n","Epoch:  5553 , current epoch train loss:  0.002143008168786764\n","Epoch:  5554 , current epoch train loss:  0.0018686698749661446\n","Epoch:  5555 , current epoch train loss:  0.001910042017698288\n","Epoch:  5556 , current epoch train loss:  0.0020099282264709473\n","Epoch:  5557 , current epoch train loss:  0.0019420189782977104\n","Epoch:  5558 , current epoch train loss:  0.0018380300607532263\n","Epoch:  5559 , current epoch train loss:  0.0018152599222958088\n","Epoch:  5560 , current epoch train loss:  0.0019506968092173338\n","Epoch:  5561 , current epoch train loss:  0.0018413120415061712\n","Epoch:  5562 , current epoch train loss:  0.001890685292892158\n","Epoch:  5563 , current epoch train loss:  0.0019201692193746567\n","Epoch:  5564 , current epoch train loss:  0.0019347848137840629\n","Epoch:  5565 , current epoch train loss:  0.001964591909199953\n","Epoch:  5566 , current epoch train loss:  0.0018748037982732058\n","Epoch:  5567 , current epoch train loss:  0.001847372273914516\n","Epoch:  5568 , current epoch train loss:  0.0020423687528818846\n","Epoch:  5569 , current epoch train loss:  0.0018408342730253935\n","Epoch:  5570 , current epoch train loss:  0.0018992451950907707\n","Epoch:  5571 , current epoch train loss:  0.00196686084382236\n","Epoch:  5572 , current epoch train loss:  0.0019439341267570853\n","Epoch:  5573 , current epoch train loss:  0.0018972717225551605\n","Epoch:  5574 , current epoch train loss:  0.0019655863288789988\n","Epoch:  5575 , current epoch train loss:  0.0017866035923361778\n","Epoch:  5576 , current epoch train loss:  0.0021232948638498783\n","Epoch:  5577 , current epoch train loss:  0.0020007481798529625\n","Epoch:  5578 , current epoch train loss:  0.002053826581686735\n","Epoch:  5579 , current epoch train loss:  0.0018412884091958404\n","Epoch:  5580 , current epoch train loss:  0.002063372638076544\n","Epoch:  5581 , current epoch train loss:  0.001989350188523531\n","Epoch:  5582 , current epoch train loss:  0.0018104747869074345\n","Epoch:  5583 , current epoch train loss:  0.0018650935962796211\n","Epoch:  5584 , current epoch train loss:  0.0019241635454818606\n","Epoch:  5585 , current epoch train loss:  0.0019216460641473532\n","Epoch:  5586 , current epoch train loss:  0.001922639086842537\n","Epoch:  5587 , current epoch train loss:  0.0019368687644600868\n","Epoch:  5588 , current epoch train loss:  0.002055622171610594\n","Epoch:  5589 , current epoch train loss:  0.0019159566145390272\n","Epoch:  5590 , current epoch train loss:  0.002072639297693968\n","Epoch:  5591 , current epoch train loss:  0.0019489614060148597\n","Epoch:  5592 , current epoch train loss:  0.0020235246047377586\n","Epoch:  5593 , current epoch train loss:  0.0019304899033159018\n","Epoch:  5594 , current epoch train loss:  0.0022126329131424427\n","Epoch:  5595 , current epoch train loss:  0.0020728784147650003\n","Epoch:  5596 , current epoch train loss:  0.0017842407105490565\n","Epoch:  5597 , current epoch train loss:  0.001996233593672514\n","Epoch:  5598 , current epoch train loss:  0.0019113648450002074\n","Epoch:  5599 , current epoch train loss:  0.0018900757422670722\n","Epoch:  5600 , current epoch train loss:  0.002077176235616207\n","Epoch:  5601 , current epoch train loss:  0.002067147521302104\n","Epoch:  5602 , current epoch train loss:  0.001873159664683044\n","Epoch:  5603 , current epoch train loss:  0.00200944347307086\n","Epoch:  5604 , current epoch train loss:  0.001968197524547577\n","Epoch:  5605 , current epoch train loss:  0.0020325137302279472\n","Epoch:  5606 , current epoch train loss:  0.001984529197216034\n","Epoch:  5607 , current epoch train loss:  0.0019890335388481617\n","Epoch:  5608 , current epoch train loss:  0.002082572318613529\n","Epoch:  5609 , current epoch train loss:  0.0020663440227508545\n","Epoch:  5610 , current epoch train loss:  0.0019167959690093994\n","Epoch:  5611 , current epoch train loss:  0.0019420379539951682\n","Epoch:  5612 , current epoch train loss:  0.0019426908111199737\n","Epoch:  5613 , current epoch train loss:  0.001976746367290616\n","Epoch:  5614 , current epoch train loss:  0.0019851222168654203\n","Epoch:  5615 , current epoch train loss:  0.0019956931937485933\n","Epoch:  5616 , current epoch train loss:  0.001973132137209177\n","Epoch:  5617 , current epoch train loss:  0.0019429707899689674\n","Epoch:  5618 , current epoch train loss:  0.00193620880600065\n","Epoch:  5619 , current epoch train loss:  0.002282059285789728\n","Epoch:  5620 , current epoch train loss:  0.0018438393017277122\n","Epoch:  5621 , current epoch train loss:  0.001774936681613326\n","Epoch:  5622 , current epoch train loss:  0.0019578328356146812\n","Epoch:  5623 , current epoch train loss:  0.001877130358479917\n","Epoch:  5624 , current epoch train loss:  0.0018246298423036933\n","Epoch:  5625 , current epoch train loss:  0.0018971310928463936\n","Epoch:  5626 , current epoch train loss:  0.002095992909744382\n","Epoch:  5627 , current epoch train loss:  0.001832430949434638\n","Epoch:  5628 , current epoch train loss:  0.0019141220254823565\n","Epoch:  5629 , current epoch train loss:  0.0019543396774679422\n","Epoch:  5630 , current epoch train loss:  0.0019877604208886623\n","Epoch:  5631 , current epoch train loss:  0.001810624497011304\n","Epoch:  5632 , current epoch train loss:  0.0020128805190324783\n","Epoch:  5633 , current epoch train loss:  0.0020572759676724672\n","Epoch:  5634 , current epoch train loss:  0.001864064484834671\n","Epoch:  5635 , current epoch train loss:  0.002140443306416273\n","Epoch:  5636 , current epoch train loss:  0.0018704989925026894\n","Epoch:  5637 , current epoch train loss:  0.001874141744337976\n","Epoch:  5638 , current epoch train loss:  0.001944508170709014\n","Epoch:  5639 , current epoch train loss:  0.0017569151241332293\n","Epoch:  5640 , current epoch train loss:  0.0019100647186860442\n","Epoch:  5641 , current epoch train loss:  0.002015096601098776\n","Epoch:  5642 , current epoch train loss:  0.0020725287031382322\n","Epoch:  5643 , current epoch train loss:  0.0017349831759929657\n","Epoch:  5644 , current epoch train loss:  0.001851701526902616\n","Epoch:  5645 , current epoch train loss:  0.002018942963331938\n","Epoch:  5646 , current epoch train loss:  0.0019631178583949804\n","Epoch:  5647 , current epoch train loss:  0.0018767362926155329\n","Epoch:  5648 , current epoch train loss:  0.002103559672832489\n","Epoch:  5649 , current epoch train loss:  0.0018697114428505301\n","Epoch:  5650 , current epoch train loss:  0.0018148906528949738\n","Epoch:  5651 , current epoch train loss:  0.0017470167949795723\n","Epoch:  5652 , current epoch train loss:  0.0021417797543108463\n","Epoch:  5653 , current epoch train loss:  0.001908483449369669\n","Epoch:  5654 , current epoch train loss:  0.0018638381734490395\n","Epoch:  5655 , current epoch train loss:  0.0018508908106014132\n","Epoch:  5656 , current epoch train loss:  0.001863007782958448\n","Epoch:  5657 , current epoch train loss:  0.001835644245147705\n","Epoch:  5658 , current epoch train loss:  0.0019476537127047777\n","Epoch:  5659 , current epoch train loss:  0.0020223469473421574\n","Epoch:  5660 , current epoch train loss:  0.001977382693439722\n","Epoch:  5661 , current epoch train loss:  0.0018426881870254874\n","Epoch:  5662 , current epoch train loss:  0.0018776594661176205\n","Epoch:  5663 , current epoch train loss:  0.0019090797286480665\n","Epoch:  5664 , current epoch train loss:  0.0020419207867234945\n","Epoch:  5665 , current epoch train loss:  0.0020499895326793194\n","Epoch:  5666 , current epoch train loss:  0.0018466997426003218\n","Epoch:  5667 , current epoch train loss:  0.0020258380100131035\n","Epoch:  5668 , current epoch train loss:  0.0019842577166855335\n","Epoch:  5669 , current epoch train loss:  0.0019565215334296227\n","Epoch:  5670 , current epoch train loss:  0.002050845418125391\n","Epoch:  5671 , current epoch train loss:  0.0020217285491526127\n","Epoch:  5672 , current epoch train loss:  0.0018990717362612486\n","Epoch:  5673 , current epoch train loss:  0.002158448100090027\n","Epoch:  5674 , current epoch train loss:  0.0019258004613220692\n","Epoch:  5675 , current epoch train loss:  0.0019277443643659353\n","Epoch:  5676 , current epoch train loss:  0.002169051906093955\n","Epoch:  5677 , current epoch train loss:  0.0022345641627907753\n","Epoch:  5678 , current epoch train loss:  0.0018733463948592544\n","Epoch:  5679 , current epoch train loss:  0.0019413267727941275\n","Epoch:  5680 , current epoch train loss:  0.002280022483319044\n","Epoch:  5681 , current epoch train loss:  0.002173316664993763\n","Epoch:  5682 , current epoch train loss:  0.0019160783849656582\n","Epoch:  5683 , current epoch train loss:  0.0021808280143886805\n","Epoch:  5684 , current epoch train loss:  0.002284275135025382\n","Epoch:  5685 , current epoch train loss:  0.002007721457630396\n","Epoch:  5686 , current epoch train loss:  0.0020882233511656523\n","Epoch:  5687 , current epoch train loss:  0.002287359908223152\n","Epoch:  5688 , current epoch train loss:  0.0020109694451093674\n","Epoch:  5689 , current epoch train loss:  0.0018496084958314896\n","Epoch:  5690 , current epoch train loss:  0.002010696567595005\n","Epoch:  5691 , current epoch train loss:  0.0023711035028100014\n","Epoch:  5692 , current epoch train loss:  0.001967509277164936\n","Epoch:  5693 , current epoch train loss:  0.0020064134150743484\n","Epoch:  5694 , current epoch train loss:  0.002209977712482214\n","Epoch:  5695 , current epoch train loss:  0.002047597197815776\n","Epoch:  5696 , current epoch train loss:  0.0019997458439320326\n","Epoch:  5697 , current epoch train loss:  0.0018158548045903444\n","Epoch:  5698 , current epoch train loss:  0.001850757747888565\n","Epoch:  5699 , current epoch train loss:  0.001870434731245041\n","Epoch:  5700 , current epoch train loss:  0.0018470449140295386\n","Epoch:  5701 , current epoch train loss:  0.0018980823224410415\n","Epoch:  5702 , current epoch train loss:  0.0018380437977612019\n","Epoch:  5703 , current epoch train loss:  0.0018868930637836456\n","Epoch:  5704 , current epoch train loss:  0.0019773675594478846\n","Epoch:  5705 , current epoch train loss:  0.00194559374358505\n","Epoch:  5706 , current epoch train loss:  0.0018863532459363341\n","Epoch:  5707 , current epoch train loss:  0.001808664295822382\n","Epoch:  5708 , current epoch train loss:  0.0020989568438380957\n","Epoch:  5709 , current epoch train loss:  0.00198225025087595\n","Epoch:  5710 , current epoch train loss:  0.0018294679466634989\n","Epoch:  5711 , current epoch train loss:  0.0018810981418937445\n","Epoch:  5712 , current epoch train loss:  0.0020038029178977013\n","Epoch:  5713 , current epoch train loss:  0.0019333974923938513\n","Epoch:  5714 , current epoch train loss:  0.0019111235160380602\n","Epoch:  5715 , current epoch train loss:  0.002212319290265441\n","Epoch:  5716 , current epoch train loss:  0.0018679052591323853\n","Epoch:  5717 , current epoch train loss:  0.0018645230447873473\n","Epoch:  5718 , current epoch train loss:  0.001840092008933425\n","Epoch:  5719 , current epoch train loss:  0.0019316670950502157\n","Epoch:  5720 , current epoch train loss:  0.0017867902060970664\n","Epoch:  5721 , current epoch train loss:  0.0017785507952794433\n","Epoch:  5722 , current epoch train loss:  0.0017986483871936798\n","Epoch:  5723 , current epoch train loss:  0.0021351343020796776\n","Epoch:  5724 , current epoch train loss:  0.0018246996914967895\n","Epoch:  5725 , current epoch train loss:  0.0018507227068766952\n","Epoch:  5726 , current epoch train loss:  0.0019310455536469817\n","Epoch:  5727 , current epoch train loss:  0.0020207595080137253\n","Epoch:  5728 , current epoch train loss:  0.001811058260500431\n","Epoch:  5729 , current epoch train loss:  0.0022015280555933714\n","Epoch:  5730 , current epoch train loss:  0.0018479464342817664\n","Epoch:  5731 , current epoch train loss:  0.0018191011622548103\n","Epoch:  5732 , current epoch train loss:  0.0017491684993728995\n","Epoch:  5733 , current epoch train loss:  0.0017566829919815063\n","Epoch:  5734 , current epoch train loss:  0.001794273965060711\n","Epoch:  5735 , current epoch train loss:  0.001851795008406043\n","Epoch:  5736 , current epoch train loss:  0.0017871404998004436\n","Epoch:  5737 , current epoch train loss:  0.001749378046952188\n","Epoch:  5738 , current epoch train loss:  0.0018559545278549194\n","Epoch:  5739 , current epoch train loss:  0.0019053174182772636\n","Epoch:  5740 , current epoch train loss:  0.0019099467899650335\n","Epoch:  5741 , current epoch train loss:  0.0017435180488973856\n","Epoch:  5742 , current epoch train loss:  0.00188532005995512\n","Epoch:  5743 , current epoch train loss:  0.0018264356767758727\n","Epoch:  5744 , current epoch train loss:  0.0018686286639422178\n","Epoch:  5745 , current epoch train loss:  0.0018804734572768211\n","Epoch:  5746 , current epoch train loss:  0.0018675019964575768\n","Epoch:  5747 , current epoch train loss:  0.0019805091433227062\n","Epoch:  5748 , current epoch train loss:  0.002184949815273285\n","Epoch:  5749 , current epoch train loss:  0.001890240702778101\n","Epoch:  5750 , current epoch train loss:  0.0017847444396466017\n","Epoch:  5751 , current epoch train loss:  0.001926757162436843\n","Epoch:  5752 , current epoch train loss:  0.0019026435911655426\n","Epoch:  5753 , current epoch train loss:  0.001856625429354608\n","Epoch:  5754 , current epoch train loss:  0.0019339071586728096\n","Epoch:  5755 , current epoch train loss:  0.0018527609063312411\n","Epoch:  5756 , current epoch train loss:  0.00198574922978878\n","Epoch:  5757 , current epoch train loss:  0.0019873203709721565\n","Epoch:  5758 , current epoch train loss:  0.001899953931570053\n","Epoch:  5759 , current epoch train loss:  0.0017257272265851498\n","Epoch:  5760 , current epoch train loss:  0.00187678390648216\n","Epoch:  5761 , current epoch train loss:  0.001888855709694326\n","Epoch:  5762 , current epoch train loss:  0.0018503715982660651\n","Epoch:  5763 , current epoch train loss:  0.001824328675866127\n","Epoch:  5764 , current epoch train loss:  0.0017498095985502005\n","Epoch:  5765 , current epoch train loss:  0.0019344290485605597\n","Epoch:  5766 , current epoch train loss:  0.0018379980465397239\n","Epoch:  5767 , current epoch train loss:  0.0019230402540415525\n","Epoch:  5768 , current epoch train loss:  0.0019157947972416878\n","Epoch:  5769 , current epoch train loss:  0.0017720363102853298\n","Epoch:  5770 , current epoch train loss:  0.0019115840550512075\n","Epoch:  5771 , current epoch train loss:  0.0017608525231480598\n","Epoch:  5772 , current epoch train loss:  0.0017707737861201167\n","Epoch:  5773 , current epoch train loss:  0.0017981405835598707\n","Epoch:  5774 , current epoch train loss:  0.0017722267657518387\n","Epoch:  5775 , current epoch train loss:  0.0018166908994317055\n","Epoch:  5776 , current epoch train loss:  0.0019103491213172674\n","Epoch:  5777 , current epoch train loss:  0.0017726165242493153\n","Epoch:  5778 , current epoch train loss:  0.0017771241255104542\n","Epoch:  5779 , current epoch train loss:  0.001825974090024829\n","Epoch:  5780 , current epoch train loss:  0.0019153159810230136\n","Epoch:  5781 , current epoch train loss:  0.0018382822163403034\n","Epoch:  5782 , current epoch train loss:  0.0017848412971943617\n","Epoch:  5783 , current epoch train loss:  0.0019166404381394386\n","Epoch:  5784 , current epoch train loss:  0.001733328914269805\n","Epoch:  5785 , current epoch train loss:  0.0018087950302287936\n","Epoch:  5786 , current epoch train loss:  0.0018696282058954239\n","Epoch:  5787 , current epoch train loss:  0.001767613342963159\n","Epoch:  5788 , current epoch train loss:  0.0019501135684549809\n","Epoch:  5789 , current epoch train loss:  0.0019392434041947126\n","Epoch:  5790 , current epoch train loss:  0.001783512532711029\n","Epoch:  5791 , current epoch train loss:  0.0018767917063087225\n","Epoch:  5792 , current epoch train loss:  0.0018978979205712676\n","Epoch:  5793 , current epoch train loss:  0.0018406976014375687\n","Epoch:  5794 , current epoch train loss:  0.001891021616756916\n","Epoch:  5795 , current epoch train loss:  0.001992630772292614\n","Epoch:  5796 , current epoch train loss:  0.0018738436046987772\n","Epoch:  5797 , current epoch train loss:  0.0018865198362618685\n","Epoch:  5798 , current epoch train loss:  0.0019572863820940256\n","Epoch:  5799 , current epoch train loss:  0.0019371048547327518\n","Epoch:  5800 , current epoch train loss:  0.0020437873899936676\n","Epoch:  5801 , current epoch train loss:  0.00240219640545547\n","Epoch:  5802 , current epoch train loss:  0.0020230459049344063\n","Epoch:  5803 , current epoch train loss:  0.0018208697438240051\n","Epoch:  5804 , current epoch train loss:  0.002058636862784624\n","Epoch:  5805 , current epoch train loss:  0.0019877543672919273\n","Epoch:  5806 , current epoch train loss:  0.001771061448380351\n","Epoch:  5807 , current epoch train loss:  0.001923445612192154\n","Epoch:  5808 , current epoch train loss:  0.0019383094040676951\n","Epoch:  5809 , current epoch train loss:  0.0018780704122036695\n","Epoch:  5810 , current epoch train loss:  0.0020107943564653397\n","Epoch:  5811 , current epoch train loss:  0.001975628547370434\n","Epoch:  5812 , current epoch train loss:  0.001992597011849284\n","Epoch:  5813 , current epoch train loss:  0.0018665180541574955\n","Epoch:  5814 , current epoch train loss:  0.0018972174730151892\n","Epoch:  5815 , current epoch train loss:  0.0017792036524042487\n","Epoch:  5816 , current epoch train loss:  0.0018670273711904883\n","Epoch:  5817 , current epoch train loss:  0.0019784211181104183\n","Epoch:  5818 , current epoch train loss:  0.0018750799354165792\n","Epoch:  5819 , current epoch train loss:  0.001861067721620202\n","Epoch:  5820 , current epoch train loss:  0.0019180880626663566\n","Epoch:  5821 , current epoch train loss:  0.0017912228358909488\n","Epoch:  5822 , current epoch train loss:  0.0018494334071874619\n","Epoch:  5823 , current epoch train loss:  0.00180591712705791\n","Epoch:  5824 , current epoch train loss:  0.0017737600719556212\n","Epoch:  5825 , current epoch train loss:  0.0018243886297568679\n","Epoch:  5826 , current epoch train loss:  0.0017898104852065444\n","Epoch:  5827 , current epoch train loss:  0.0017839669017121196\n","Epoch:  5828 , current epoch train loss:  0.001911904546432197\n","Epoch:  5829 , current epoch train loss:  0.001771076349541545\n","Epoch:  5830 , current epoch train loss:  0.0018072010716423392\n","Epoch:  5831 , current epoch train loss:  0.001833766931667924\n","Epoch:  5832 , current epoch train loss:  0.001852033194154501\n","Epoch:  5833 , current epoch train loss:  0.001758368918672204\n","Epoch:  5834 , current epoch train loss:  0.0018311869353055954\n","Epoch:  5835 , current epoch train loss:  0.0018687983974814415\n","Epoch:  5836 , current epoch train loss:  0.0018276255577802658\n","Epoch:  5837 , current epoch train loss:  0.0018728089053183794\n","Epoch:  5838 , current epoch train loss:  0.0018163518980145454\n","Epoch:  5839 , current epoch train loss:  0.001833018264733255\n","Epoch:  5840 , current epoch train loss:  0.0018026402685791254\n","Epoch:  5841 , current epoch train loss:  0.001791472896002233\n","Epoch:  5842 , current epoch train loss:  0.0018324678530916572\n","Epoch:  5843 , current epoch train loss:  0.001768103800714016\n","Epoch:  5844 , current epoch train loss:  0.0017913167830556631\n","Epoch:  5845 , current epoch train loss:  0.0019421622855588794\n","Epoch:  5846 , current epoch train loss:  0.001761027961038053\n","Epoch:  5847 , current epoch train loss:  0.0018112962134182453\n","Epoch:  5848 , current epoch train loss:  0.0018642311915755272\n","Epoch:  5849 , current epoch train loss:  0.0018202618230134249\n","Epoch:  5850 , current epoch train loss:  0.0018925913609564304\n","Epoch:  5851 , current epoch train loss:  0.0018014793749898672\n","Epoch:  5852 , current epoch train loss:  0.0019494047155603766\n","Epoch:  5853 , current epoch train loss:  0.0019832563120871782\n","Epoch:  5854 , current epoch train loss:  0.001988806063309312\n","Epoch:  5855 , current epoch train loss:  0.0018697757041081786\n","Epoch:  5856 , current epoch train loss:  0.0021857586689293385\n","Epoch:  5857 , current epoch train loss:  0.001833813963457942\n","Epoch:  5858 , current epoch train loss:  0.002005336806178093\n","Epoch:  5859 , current epoch train loss:  0.0019043240463361144\n","Epoch:  5860 , current epoch train loss:  0.002136914525181055\n","Epoch:  5861 , current epoch train loss:  0.0017749122343957424\n","Epoch:  5862 , current epoch train loss:  0.0019078438635915518\n","Epoch:  5863 , current epoch train loss:  0.001898356480523944\n","Epoch:  5864 , current epoch train loss:  0.002096335170790553\n","Epoch:  5865 , current epoch train loss:  0.001918506226502359\n","Epoch:  5866 , current epoch train loss:  0.0021471274085342884\n","Epoch:  5867 , current epoch train loss:  0.001955091254785657\n","Epoch:  5868 , current epoch train loss:  0.0018361116526648402\n","Epoch:  5869 , current epoch train loss:  0.0021042083390057087\n","Epoch:  5870 , current epoch train loss:  0.0019015027210116386\n","Epoch:  5871 , current epoch train loss:  0.002011542208492756\n","Epoch:  5872 , current epoch train loss:  0.0018191144336014986\n","Epoch:  5873 , current epoch train loss:  0.0019213499035686255\n","Epoch:  5874 , current epoch train loss:  0.0021320024970918894\n","Epoch:  5875 , current epoch train loss:  0.0018546530045568943\n","Epoch:  5876 , current epoch train loss:  0.0019703791476786137\n","Epoch:  5877 , current epoch train loss:  0.0020599213894456625\n","Epoch:  5878 , current epoch train loss:  0.0018746893620118499\n","Epoch:  5879 , current epoch train loss:  0.001847052713856101\n","Epoch:  5880 , current epoch train loss:  0.0020696213468909264\n","Epoch:  5881 , current epoch train loss:  0.0018465283792465925\n","Epoch:  5882 , current epoch train loss:  0.0017872692551463842\n","Epoch:  5883 , current epoch train loss:  0.0019513614242896438\n","Epoch:  5884 , current epoch train loss:  0.002036256715655327\n","Epoch:  5885 , current epoch train loss:  0.0018070148071274161\n","Epoch:  5886 , current epoch train loss:  0.0018257393967360258\n","Epoch:  5887 , current epoch train loss:  0.0020623686723411083\n","Epoch:  5888 , current epoch train loss:  0.0017796773463487625\n","Epoch:  5889 , current epoch train loss:  0.001788988127373159\n","Epoch:  5890 , current epoch train loss:  0.00177760049700737\n","Epoch:  5891 , current epoch train loss:  0.0017879389924928546\n","Epoch:  5892 , current epoch train loss:  0.001819095341488719\n","Epoch:  5893 , current epoch train loss:  0.001916646957397461\n","Epoch:  5894 , current epoch train loss:  0.0019766229670494795\n","Epoch:  5895 , current epoch train loss:  0.0020473706535995007\n","Epoch:  5896 , current epoch train loss:  0.0018460381543263793\n","Epoch:  5897 , current epoch train loss:  0.0018161849584430456\n","Epoch:  5898 , current epoch train loss:  0.0018495757831260562\n","Epoch:  5899 , current epoch train loss:  0.0018548162188380957\n","Epoch:  5900 , current epoch train loss:  0.0018489256035536528\n","Epoch:  5901 , current epoch train loss:  0.0018403725698590279\n","Epoch:  5902 , current epoch train loss:  0.001822008052840829\n","Epoch:  5903 , current epoch train loss:  0.0018051012884825468\n","Epoch:  5904 , current epoch train loss:  0.0017010732553899288\n","Epoch:  5905 , current epoch train loss:  0.0018372021149843931\n","Epoch:  5906 , current epoch train loss:  0.001809505745768547\n","Epoch:  5907 , current epoch train loss:  0.0018185088410973549\n","Epoch:  5908 , current epoch train loss:  0.0017472676699981093\n","Epoch:  5909 , current epoch train loss:  0.0019041297491639853\n","Epoch:  5910 , current epoch train loss:  0.0018056619446724653\n","Epoch:  5911 , current epoch train loss:  0.0017465734854340553\n","Epoch:  5912 , current epoch train loss:  0.001973339356482029\n","Epoch:  5913 , current epoch train loss:  0.0018342763651162386\n","Epoch:  5914 , current epoch train loss:  0.0022825668565928936\n","Epoch:  5915 , current epoch train loss:  0.0018805050058290362\n","Epoch:  5916 , current epoch train loss:  0.0017796929460018873\n","Epoch:  5917 , current epoch train loss:  0.0019195787608623505\n","Epoch:  5918 , current epoch train loss:  0.0018624064978212118\n","Epoch:  5919 , current epoch train loss:  0.0018243507947772741\n","Epoch:  5920 , current epoch train loss:  0.0018191015115007758\n","Epoch:  5921 , current epoch train loss:  0.0018300944939255714\n","Epoch:  5922 , current epoch train loss:  0.0017581665888428688\n","Epoch:  5923 , current epoch train loss:  0.00179969088640064\n","Epoch:  5924 , current epoch train loss:  0.0018682284280657768\n","Epoch:  5925 , current epoch train loss:  0.0018241568468511105\n","Epoch:  5926 , current epoch train loss:  0.0017989713232964277\n","Epoch:  5927 , current epoch train loss:  0.0018770731985569\n","Epoch:  5928 , current epoch train loss:  0.0018006465397775173\n","Epoch:  5929 , current epoch train loss:  0.0018165751826018095\n","Epoch:  5930 , current epoch train loss:  0.0018396427622064948\n","Epoch:  5931 , current epoch train loss:  0.0018673331942409277\n","Epoch:  5932 , current epoch train loss:  0.0017277570441365242\n","Epoch:  5933 , current epoch train loss:  0.001947134267538786\n","Epoch:  5934 , current epoch train loss:  0.0018523919861763716\n","Epoch:  5935 , current epoch train loss:  0.001944559859111905\n","Epoch:  5936 , current epoch train loss:  0.0018976384308189154\n","Epoch:  5937 , current epoch train loss:  0.0017198965651914477\n","Epoch:  5938 , current epoch train loss:  0.0019265682203695178\n","Epoch:  5939 , current epoch train loss:  0.001870839623734355\n","Epoch:  5940 , current epoch train loss:  0.001768898218870163\n","Epoch:  5941 , current epoch train loss:  0.0018788818269968033\n","Epoch:  5942 , current epoch train loss:  0.0017957541858777404\n","Epoch:  5943 , current epoch train loss:  0.0017788674449548125\n","Epoch:  5944 , current epoch train loss:  0.0018270009895786643\n","Epoch:  5945 , current epoch train loss:  0.001756950281560421\n","Epoch:  5946 , current epoch train loss:  0.0017528168391436338\n","Epoch:  5947 , current epoch train loss:  0.0018209402915090322\n","Epoch:  5948 , current epoch train loss:  0.0017360951751470566\n","Epoch:  5949 , current epoch train loss:  0.0018783698324114084\n","Epoch:  5950 , current epoch train loss:  0.0018903732998296618\n","Epoch:  5951 , current epoch train loss:  0.0018241325160488486\n","Epoch:  5952 , current epoch train loss:  0.0019265455193817616\n","Epoch:  5953 , current epoch train loss:  0.001887334743514657\n","Epoch:  5954 , current epoch train loss:  0.0017841930966824293\n","Epoch:  5955 , current epoch train loss:  0.0018801618134602904\n","Epoch:  5956 , current epoch train loss:  0.001928234240040183\n","Epoch:  5957 , current epoch train loss:  0.0017953617498278618\n","Epoch:  5958 , current epoch train loss:  0.0017686327919363976\n","Epoch:  5959 , current epoch train loss:  0.0018239831551909447\n","Epoch:  5960 , current epoch train loss:  0.00169307307805866\n","Epoch:  5961 , current epoch train loss:  0.0016977939521893859\n","Epoch:  5962 , current epoch train loss:  0.0017962995916604996\n","Epoch:  5963 , current epoch train loss:  0.001822484191507101\n","Epoch:  5964 , current epoch train loss:  0.001744446693919599\n","Epoch:  5965 , current epoch train loss:  0.0017812587320804596\n","Epoch:  5966 , current epoch train loss:  0.001978873973712325\n","Epoch:  5967 , current epoch train loss:  0.0017824682872742414\n","Epoch:  5968 , current epoch train loss:  0.0019417281728237867\n","Epoch:  5969 , current epoch train loss:  0.0017535280203446746\n","Epoch:  5970 , current epoch train loss:  0.0017503933049738407\n","Epoch:  5971 , current epoch train loss:  0.0019150007283315063\n","Epoch:  5972 , current epoch train loss:  0.0018927439814433455\n","Epoch:  5973 , current epoch train loss:  0.0018481704173609614\n","Epoch:  5974 , current epoch train loss:  0.0021744342520833015\n","Epoch:  5975 , current epoch train loss:  0.001868085702881217\n","Epoch:  5976 , current epoch train loss:  0.0018161989282816648\n","Epoch:  5977 , current epoch train loss:  0.0018503674073144794\n","Epoch:  5978 , current epoch train loss:  0.0018550450913608074\n","Epoch:  5979 , current epoch train loss:  0.0017341029597446322\n","Epoch:  5980 , current epoch train loss:  0.0019336249679327011\n","Epoch:  5981 , current epoch train loss:  0.0017293894197791815\n","Epoch:  5982 , current epoch train loss:  0.0019313001539558172\n","Epoch:  5983 , current epoch train loss:  0.0017482067923992872\n","Epoch:  5984 , current epoch train loss:  0.001697141444310546\n","Epoch:  5985 , current epoch train loss:  0.001798786106519401\n","Epoch:  5986 , current epoch train loss:  0.0018661643844097853\n","Epoch:  5987 , current epoch train loss:  0.0019122334197163582\n","Epoch:  5988 , current epoch train loss:  0.001839346601627767\n","Epoch:  5989 , current epoch train loss:  0.001818702556192875\n","Epoch:  5990 , current epoch train loss:  0.0018290018197149038\n","Epoch:  5991 , current epoch train loss:  0.001798880286514759\n","Epoch:  5992 , current epoch train loss:  0.0017888210713863373\n","Epoch:  5993 , current epoch train loss:  0.0017342906212434173\n","Epoch:  5994 , current epoch train loss:  0.0017175859538838267\n","Epoch:  5995 , current epoch train loss:  0.001941639930009842\n","Epoch:  5996 , current epoch train loss:  0.0017153489170596004\n","Epoch:  5997 , current epoch train loss:  0.0018194264266639948\n","Epoch:  5998 , current epoch train loss:  0.001809161389246583\n","Epoch:  5999 , current epoch train loss:  0.0018154148710891604\n","Epoch:  6000 , current epoch train loss:  0.0017491616308689117\n","Epoch:  6001 , current epoch train loss:  0.0017427759012207389\n","Epoch:  6002 , current epoch train loss:  0.001794207259081304\n","Epoch:  6003 , current epoch train loss:  0.0018373372731730342\n","Epoch:  6004 , current epoch train loss:  0.001849713153205812\n","Epoch:  6005 , current epoch train loss:  0.0017471823375672102\n","Epoch:  6006 , current epoch train loss:  0.0017943847924470901\n","Epoch:  6007 , current epoch train loss:  0.0017769001424312592\n","Epoch:  6008 , current epoch train loss:  0.0017492531333118677\n","Epoch:  6009 , current epoch train loss:  0.0019036612939089537\n","Epoch:  6010 , current epoch train loss:  0.0017822773661464453\n","Epoch:  6011 , current epoch train loss:  0.0018079634755849838\n","Epoch:  6012 , current epoch train loss:  0.001772186136804521\n","Epoch:  6013 , current epoch train loss:  0.0017944267019629478\n","Epoch:  6014 , current epoch train loss:  0.0017147758044302464\n","Epoch:  6015 , current epoch train loss:  0.0018742650281637907\n","Epoch:  6016 , current epoch train loss:  0.0019484366057440639\n","Epoch:  6017 , current epoch train loss:  0.0018448194023221731\n","Epoch:  6018 , current epoch train loss:  0.0018346573924645782\n","Epoch:  6019 , current epoch train loss:  0.0017166010802611709\n","Epoch:  6020 , current epoch train loss:  0.001819735742174089\n","Epoch:  6021 , current epoch train loss:  0.0017679405864328146\n","Epoch:  6022 , current epoch train loss:  0.0017413187306374311\n","Epoch:  6023 , current epoch train loss:  0.0018664974486455321\n","Epoch:  6024 , current epoch train loss:  0.0018573722336441278\n","Epoch:  6025 , current epoch train loss:  0.0018746297573670745\n","Epoch:  6026 , current epoch train loss:  0.0018354239873588085\n","Epoch:  6027 , current epoch train loss:  0.0017471153987571597\n","Epoch:  6028 , current epoch train loss:  0.0017231421079486609\n","Epoch:  6029 , current epoch train loss:  0.0018268044805154204\n","Epoch:  6030 , current epoch train loss:  0.0018217272590845823\n","Epoch:  6031 , current epoch train loss:  0.0018352956976741552\n","Epoch:  6032 , current epoch train loss:  0.0017772128339856863\n","Epoch:  6033 , current epoch train loss:  0.0018278516363352537\n","Epoch:  6034 , current epoch train loss:  0.0018653040751814842\n","Epoch:  6035 , current epoch train loss:  0.001814552815631032\n","Epoch:  6036 , current epoch train loss:  0.0017916399519890547\n","Epoch:  6037 , current epoch train loss:  0.0018165584187954664\n","Epoch:  6038 , current epoch train loss:  0.0021422053687274456\n","Epoch:  6039 , current epoch train loss:  0.0019474751316010952\n","Epoch:  6040 , current epoch train loss:  0.0017952043563127518\n","Epoch:  6041 , current epoch train loss:  0.0018294667825102806\n","Epoch:  6042 , current epoch train loss:  0.0017598563572391868\n","Epoch:  6043 , current epoch train loss:  0.0018454905366525054\n","Epoch:  6044 , current epoch train loss:  0.0017632486997172236\n","Epoch:  6045 , current epoch train loss:  0.001823951373808086\n","Epoch:  6046 , current epoch train loss:  0.001725929556414485\n","Epoch:  6047 , current epoch train loss:  0.001776411198079586\n","Epoch:  6048 , current epoch train loss:  0.00168616883456707\n","Epoch:  6049 , current epoch train loss:  0.0018521544989198446\n","Epoch:  6050 , current epoch train loss:  0.0017632631352171302\n","Epoch:  6051 , current epoch train loss:  0.0018994588172063231\n","Epoch:  6052 , current epoch train loss:  0.0019720313139259815\n","Epoch:  6053 , current epoch train loss:  0.001827260828576982\n","Epoch:  6054 , current epoch train loss:  0.0018719478975981474\n","Epoch:  6055 , current epoch train loss:  0.0017743207281455398\n","Epoch:  6056 , current epoch train loss:  0.0018028494669124484\n","Epoch:  6057 , current epoch train loss:  0.0018612043932080269\n","Epoch:  6058 , current epoch train loss:  0.0017756301676854491\n","Epoch:  6059 , current epoch train loss:  0.0018043483141809702\n","Epoch:  6060 , current epoch train loss:  0.0018285538535565138\n","Epoch:  6061 , current epoch train loss:  0.0018121324246749282\n","Epoch:  6062 , current epoch train loss:  0.001820296747609973\n","Epoch:  6063 , current epoch train loss:  0.0017972588539123535\n","Epoch:  6064 , current epoch train loss:  0.0017340702470391989\n","Epoch:  6065 , current epoch train loss:  0.0017294459976255894\n","Epoch:  6066 , current epoch train loss:  0.0016944281524047256\n","Epoch:  6067 , current epoch train loss:  0.0017924162093549967\n","Epoch:  6068 , current epoch train loss:  0.001722422195598483\n","Epoch:  6069 , current epoch train loss:  0.001746312715113163\n","Epoch:  6070 , current epoch train loss:  0.0018086162162944674\n","Epoch:  6071 , current epoch train loss:  0.0017464696429669857\n","Epoch:  6072 , current epoch train loss:  0.001812345813959837\n","Epoch:  6073 , current epoch train loss:  0.0018261255463585258\n","Epoch:  6074 , current epoch train loss:  0.0017844876274466515\n","Epoch:  6075 , current epoch train loss:  0.0017043452244251966\n","Epoch:  6076 , current epoch train loss:  0.0017550354823470116\n","Epoch:  6077 , current epoch train loss:  0.0016924282535910606\n","Epoch:  6078 , current epoch train loss:  0.0017201954033225775\n","Epoch:  6079 , current epoch train loss:  0.0018344083800911903\n","Epoch:  6080 , current epoch train loss:  0.0017596748657524586\n","Epoch:  6081 , current epoch train loss:  0.0018509009387344122\n","Epoch:  6082 , current epoch train loss:  0.0018302545649930835\n","Epoch:  6083 , current epoch train loss:  0.0017587707843631506\n","Epoch:  6084 , current epoch train loss:  0.0019360752776265144\n","Epoch:  6085 , current epoch train loss:  0.0017441477393731475\n","Epoch:  6086 , current epoch train loss:  0.0017455518245697021\n","Epoch:  6087 , current epoch train loss:  0.001818345277570188\n","Epoch:  6088 , current epoch train loss:  0.0018640959169715643\n","Epoch:  6089 , current epoch train loss:  0.0018420780543237925\n","Epoch:  6090 , current epoch train loss:  0.0017537486273795366\n","Epoch:  6091 , current epoch train loss:  0.001767961191944778\n","Epoch:  6092 , current epoch train loss:  0.0017717571463435888\n","Epoch:  6093 , current epoch train loss:  0.0016779527068138123\n","Epoch:  6094 , current epoch train loss:  0.0017147175967693329\n","Epoch:  6095 , current epoch train loss:  0.001701944973319769\n","Epoch:  6096 , current epoch train loss:  0.0017559300176799297\n","Epoch:  6097 , current epoch train loss:  0.0017699655145406723\n","Epoch:  6098 , current epoch train loss:  0.0017342893406748772\n","Epoch:  6099 , current epoch train loss:  0.0017492088954895735\n","Epoch:  6100 , current epoch train loss:  0.0018022442236542702\n","Epoch:  6101 , current epoch train loss:  0.0017910468159243464\n","Epoch:  6102 , current epoch train loss:  0.001843043603003025\n","Epoch:  6103 , current epoch train loss:  0.001740147708915174\n","Epoch:  6104 , current epoch train loss:  0.0018109902739524841\n","Epoch:  6105 , current epoch train loss:  0.0018064528703689575\n","Epoch:  6106 , current epoch train loss:  0.0017463888507336378\n","Epoch:  6107 , current epoch train loss:  0.002017226070165634\n","Epoch:  6108 , current epoch train loss:  0.0019319800194352865\n","Epoch:  6109 , current epoch train loss:  0.0018478301353752613\n","Epoch:  6110 , current epoch train loss:  0.0019005369395017624\n","Epoch:  6111 , current epoch train loss:  0.0018303242977708578\n","Epoch:  6112 , current epoch train loss:  0.0019621062092483044\n","Epoch:  6113 , current epoch train loss:  0.0017243623733520508\n","Epoch:  6114 , current epoch train loss:  0.0018541052704676986\n","Epoch:  6115 , current epoch train loss:  0.0019408126827329397\n","Epoch:  6116 , current epoch train loss:  0.0018646891694515944\n","Epoch:  6117 , current epoch train loss:  0.0017932746559381485\n","Epoch:  6118 , current epoch train loss:  0.0017661340534687042\n","Epoch:  6119 , current epoch train loss:  0.0018545454367995262\n","Epoch:  6120 , current epoch train loss:  0.0019028887618333101\n","Epoch:  6121 , current epoch train loss:  0.001830484252423048\n","Epoch:  6122 , current epoch train loss:  0.0018803009297698736\n","Epoch:  6123 , current epoch train loss:  0.0020932117477059364\n","Epoch:  6124 , current epoch train loss:  0.0018673756858333945\n","Epoch:  6125 , current epoch train loss:  0.0018909231293946505\n","Epoch:  6126 , current epoch train loss:  0.001817687414586544\n","Epoch:  6127 , current epoch train loss:  0.0019003254128620028\n","Epoch:  6128 , current epoch train loss:  0.0017969518667086959\n","Epoch:  6129 , current epoch train loss:  0.0017341652419418097\n","Epoch:  6130 , current epoch train loss:  0.002007658826187253\n","Epoch:  6131 , current epoch train loss:  0.0018464550375938416\n","Epoch:  6132 , current epoch train loss:  0.0018563522025942802\n","Epoch:  6133 , current epoch train loss:  0.0017677809810265899\n","Epoch:  6134 , current epoch train loss:  0.0018318586517125368\n","Epoch:  6135 , current epoch train loss:  0.0019185239216312766\n","Epoch:  6136 , current epoch train loss:  0.0018425341695547104\n","Epoch:  6137 , current epoch train loss:  0.0018216887256130576\n","Epoch:  6138 , current epoch train loss:  0.0017440065275877714\n","Epoch:  6139 , current epoch train loss:  0.0017197498818859458\n","Epoch:  6140 , current epoch train loss:  0.0019067031098529696\n","Epoch:  6141 , current epoch train loss:  0.0018087343778461218\n","Epoch:  6142 , current epoch train loss:  0.0017300414619967341\n","Epoch:  6143 , current epoch train loss:  0.0017869342118501663\n","Epoch:  6144 , current epoch train loss:  0.0017933420604094863\n","Epoch:  6145 , current epoch train loss:  0.0017276661237701774\n","Epoch:  6146 , current epoch train loss:  0.0017811439465731382\n","Epoch:  6147 , current epoch train loss:  0.001794325071386993\n","Epoch:  6148 , current epoch train loss:  0.0018086070194840431\n","Epoch:  6149 , current epoch train loss:  0.0017981705022975802\n","Epoch:  6150 , current epoch train loss:  0.001838276511989534\n","Epoch:  6151 , current epoch train loss:  0.0017356876051053405\n","Epoch:  6152 , current epoch train loss:  0.0016740402206778526\n","Epoch:  6153 , current epoch train loss:  0.0018059371504932642\n","Epoch:  6154 , current epoch train loss:  0.0019516758620738983\n","Epoch:  6155 , current epoch train loss:  0.0018002851866185665\n","Epoch:  6156 , current epoch train loss:  0.0016742631560191512\n","Epoch:  6157 , current epoch train loss:  0.0018551148241385818\n","Epoch:  6158 , current epoch train loss:  0.001789208734408021\n","Epoch:  6159 , current epoch train loss:  0.0017557991668581963\n","Epoch:  6160 , current epoch train loss:  0.0016962699592113495\n","Epoch:  6161 , current epoch train loss:  0.0019186809659004211\n","Epoch:  6162 , current epoch train loss:  0.001905531040392816\n","Epoch:  6163 , current epoch train loss:  0.0019798744469881058\n","Epoch:  6164 , current epoch train loss:  0.0018318088259547949\n","Epoch:  6165 , current epoch train loss:  0.0018358103698119521\n","Epoch:  6166 , current epoch train loss:  0.0017867032438516617\n","Epoch:  6167 , current epoch train loss:  0.0017486636061221361\n","Epoch:  6168 , current epoch train loss:  0.0017980759730562568\n","Epoch:  6169 , current epoch train loss:  0.001759533304721117\n","Epoch:  6170 , current epoch train loss:  0.0019130052533000708\n","Epoch:  6171 , current epoch train loss:  0.0018609848339110613\n","Epoch:  6172 , current epoch train loss:  0.0018307592254132032\n","Epoch:  6173 , current epoch train loss:  0.0018317371141165495\n","Epoch:  6174 , current epoch train loss:  0.001865197322331369\n","Epoch:  6175 , current epoch train loss:  0.001967274583876133\n","Epoch:  6176 , current epoch train loss:  0.0019243815913796425\n","Epoch:  6177 , current epoch train loss:  0.0018527121283113956\n","Epoch:  6178 , current epoch train loss:  0.0018296119524165988\n","Epoch:  6179 , current epoch train loss:  0.0019471988780423999\n","Epoch:  6180 , current epoch train loss:  0.0019429351668804884\n","Epoch:  6181 , current epoch train loss:  0.0018501494778320193\n","Epoch:  6182 , current epoch train loss:  0.0017282649641856551\n","Epoch:  6183 , current epoch train loss:  0.0018351683393120766\n","Epoch:  6184 , current epoch train loss:  0.0018067790661007166\n","Epoch:  6185 , current epoch train loss:  0.0017863688990473747\n","Epoch:  6186 , current epoch train loss:  0.001807086868211627\n","Epoch:  6187 , current epoch train loss:  0.0017841480439528823\n","Epoch:  6188 , current epoch train loss:  0.0016988764982670546\n","Epoch:  6189 , current epoch train loss:  0.0017453993204981089\n","Epoch:  6190 , current epoch train loss:  0.0017414717003703117\n","Epoch:  6191 , current epoch train loss:  0.0017204589676111937\n","Epoch:  6192 , current epoch train loss:  0.0017297047888860106\n","Epoch:  6193 , current epoch train loss:  0.001844347221776843\n","Epoch:  6194 , current epoch train loss:  0.001778526697307825\n","Epoch:  6195 , current epoch train loss:  0.0018681598594412208\n","Epoch:  6196 , current epoch train loss:  0.0016663357382640243\n","Epoch:  6197 , current epoch train loss:  0.0017253269907087088\n","Epoch:  6198 , current epoch train loss:  0.0019721928983926773\n","Epoch:  6199 , current epoch train loss:  0.0018201054772362113\n","Epoch:  6200 , current epoch train loss:  0.0018331725150346756\n","Epoch:  6201 , current epoch train loss:  0.001799202524125576\n","Epoch:  6202 , current epoch train loss:  0.001705651986412704\n","Epoch:  6203 , current epoch train loss:  0.0017534791259095073\n","Epoch:  6204 , current epoch train loss:  0.0017598638078197837\n","Epoch:  6205 , current epoch train loss:  0.0017666368512436748\n","Epoch:  6206 , current epoch train loss:  0.0017580867279320955\n","Epoch:  6207 , current epoch train loss:  0.0018825277220457792\n","Epoch:  6208 , current epoch train loss:  0.0017719296738505363\n","Epoch:  6209 , current epoch train loss:  0.0018009282648563385\n","Epoch:  6210 , current epoch train loss:  0.0018358910456299782\n","Epoch:  6211 , current epoch train loss:  0.0018106938805431128\n","Epoch:  6212 , current epoch train loss:  0.001867529354058206\n","Epoch:  6213 , current epoch train loss:  0.001851934241130948\n","Epoch:  6214 , current epoch train loss:  0.0017783859511837363\n","Epoch:  6215 , current epoch train loss:  0.001982104731723666\n","Epoch:  6216 , current epoch train loss:  0.0017806346295401454\n","Epoch:  6217 , current epoch train loss:  0.002006729831919074\n","Epoch:  6218 , current epoch train loss:  0.0018370611360296607\n","Epoch:  6219 , current epoch train loss:  0.0017354048322886229\n","Epoch:  6220 , current epoch train loss:  0.0018005322199314833\n","Epoch:  6221 , current epoch train loss:  0.0017908294685184956\n","Epoch:  6222 , current epoch train loss:  0.0018152486300095916\n","Epoch:  6223 , current epoch train loss:  0.0018181264167651534\n","Epoch:  6224 , current epoch train loss:  0.0019253831123933196\n","Epoch:  6225 , current epoch train loss:  0.001955573447048664\n","Epoch:  6226 , current epoch train loss:  0.0018996646394953132\n","Epoch:  6227 , current epoch train loss:  0.0021066227927803993\n","Epoch:  6228 , current epoch train loss:  0.001919455244205892\n","Epoch:  6229 , current epoch train loss:  0.001927168807014823\n","Epoch:  6230 , current epoch train loss:  0.001861348282545805\n","Epoch:  6231 , current epoch train loss:  0.0020246263593435287\n","Epoch:  6232 , current epoch train loss:  0.0018725886475294828\n","Epoch:  6233 , current epoch train loss:  0.0018202796345576644\n","Epoch:  6234 , current epoch train loss:  0.001947689219377935\n","Epoch:  6235 , current epoch train loss:  0.0018257778137922287\n","Epoch:  6236 , current epoch train loss:  0.001851527951657772\n","Epoch:  6237 , current epoch train loss:  0.0018483720486983657\n","Epoch:  6238 , current epoch train loss:  0.001790702692233026\n","Epoch:  6239 , current epoch train loss:  0.001750247087329626\n","Epoch:  6240 , current epoch train loss:  0.0018740606028586626\n","Epoch:  6241 , current epoch train loss:  0.0019245212897658348\n","Epoch:  6242 , current epoch train loss:  0.001829603686928749\n","Epoch:  6243 , current epoch train loss:  0.001795936725102365\n","Epoch:  6244 , current epoch train loss:  0.0017693673726171255\n","Epoch:  6245 , current epoch train loss:  0.0018718885257840157\n","Epoch:  6246 , current epoch train loss:  0.0018521568272262812\n","Epoch:  6247 , current epoch train loss:  0.0018966367933899164\n","Epoch:  6248 , current epoch train loss:  0.0021283337846398354\n","Epoch:  6249 , current epoch train loss:  0.0018082894384860992\n","Epoch:  6250 , current epoch train loss:  0.001871303073130548\n","Epoch:  6251 , current epoch train loss:  0.0018149439711123705\n","Epoch:  6252 , current epoch train loss:  0.0018232341390103102\n","Epoch:  6253 , current epoch train loss:  0.0018680030480027199\n","Epoch:  6254 , current epoch train loss:  0.0018138106679543853\n","Epoch:  6255 , current epoch train loss:  0.00211237627081573\n","Epoch:  6256 , current epoch train loss:  0.0018175607547163963\n","Epoch:  6257 , current epoch train loss:  0.0017834504833444953\n","Epoch:  6258 , current epoch train loss:  0.0018099596491083503\n","Epoch:  6259 , current epoch train loss:  0.0020157573744654655\n","Epoch:  6260 , current epoch train loss:  0.0018003120785579085\n","Epoch:  6261 , current epoch train loss:  0.001824478036724031\n","Epoch:  6262 , current epoch train loss:  0.0018472167430445552\n","Epoch:  6263 , current epoch train loss:  0.0021311526652425528\n","Epoch:  6264 , current epoch train loss:  0.0018055470427498221\n","Epoch:  6265 , current epoch train loss:  0.001680302433669567\n","Epoch:  6266 , current epoch train loss:  0.001813754322938621\n","Epoch:  6267 , current epoch train loss:  0.0018838352989405394\n","Epoch:  6268 , current epoch train loss:  0.0016830674139782786\n","Epoch:  6269 , current epoch train loss:  0.001753126154653728\n","Epoch:  6270 , current epoch train loss:  0.0017318048048764467\n","Epoch:  6271 , current epoch train loss:  0.0018093250691890717\n","Epoch:  6272 , current epoch train loss:  0.0017170285573229194\n","Epoch:  6273 , current epoch train loss:  0.001907393685542047\n","Epoch:  6274 , current epoch train loss:  0.001795186661183834\n","Epoch:  6275 , current epoch train loss:  0.0017843181267380714\n","Epoch:  6276 , current epoch train loss:  0.0018636675085872412\n","Epoch:  6277 , current epoch train loss:  0.002108241431415081\n","Epoch:  6278 , current epoch train loss:  0.0020591679494827986\n","Epoch:  6279 , current epoch train loss:  0.0019431873224675655\n","Epoch:  6280 , current epoch train loss:  0.0021542618051171303\n","Epoch:  6281 , current epoch train loss:  0.0017693275585770607\n","Epoch:  6282 , current epoch train loss:  0.0017989939078688622\n","Epoch:  6283 , current epoch train loss:  0.002021741820499301\n","Epoch:  6284 , current epoch train loss:  0.002016306621953845\n","Epoch:  6285 , current epoch train loss:  0.0019118161872029305\n","Epoch:  6286 , current epoch train loss:  0.0017312741838395596\n","Epoch:  6287 , current epoch train loss:  0.00186881422996521\n","Epoch:  6288 , current epoch train loss:  0.0020676529966294765\n","Epoch:  6289 , current epoch train loss:  0.0018823187565430999\n","Epoch:  6290 , current epoch train loss:  0.001877903938293457\n","Epoch:  6291 , current epoch train loss:  0.0018176441080868244\n","Epoch:  6292 , current epoch train loss:  0.0019258711254224181\n","Epoch:  6293 , current epoch train loss:  0.0018483882304280996\n","Epoch:  6294 , current epoch train loss:  0.0018733926117420197\n","Epoch:  6295 , current epoch train loss:  0.0019675258081406355\n","Epoch:  6296 , current epoch train loss:  0.001998303225263953\n","Epoch:  6297 , current epoch train loss:  0.0018194937147200108\n","Epoch:  6298 , current epoch train loss:  0.0018214394804090261\n","Epoch:  6299 , current epoch train loss:  0.0018720678053796291\n","Epoch:  6300 , current epoch train loss:  0.0018367294687777758\n","Epoch:  6301 , current epoch train loss:  0.0017666801577433944\n","Epoch:  6302 , current epoch train loss:  0.0018367863958701491\n","Epoch:  6303 , current epoch train loss:  0.0018458182457834482\n","Epoch:  6304 , current epoch train loss:  0.0020160567946732044\n","Epoch:  6305 , current epoch train loss:  0.0018275445327162743\n","Epoch:  6306 , current epoch train loss:  0.0017836911138147116\n","Epoch:  6307 , current epoch train loss:  0.0017651885282248259\n","Epoch:  6308 , current epoch train loss:  0.0018248471897095442\n","Epoch:  6309 , current epoch train loss:  0.0019163073739036918\n","Epoch:  6310 , current epoch train loss:  0.0018395887454971671\n","Epoch:  6311 , current epoch train loss:  0.0017181867733597755\n","Epoch:  6312 , current epoch train loss:  0.0018205306259915233\n","Epoch:  6313 , current epoch train loss:  0.0019491019193083048\n","Epoch:  6314 , current epoch train loss:  0.001925198594108224\n","Epoch:  6315 , current epoch train loss:  0.0019362474558874965\n","Epoch:  6316 , current epoch train loss:  0.0017845164984464645\n","Epoch:  6317 , current epoch train loss:  0.0018673310987651348\n","Epoch:  6318 , current epoch train loss:  0.0018893519882112741\n","Epoch:  6319 , current epoch train loss:  0.0017692077672109008\n","Epoch:  6320 , current epoch train loss:  0.0017236759886145592\n","Epoch:  6321 , current epoch train loss:  0.0019689726177603006\n","Epoch:  6322 , current epoch train loss:  0.002031096024438739\n","Epoch:  6323 , current epoch train loss:  0.0017682507168501616\n","Epoch:  6324 , current epoch train loss:  0.0017320889746770263\n","Epoch:  6325 , current epoch train loss:  0.001858863397501409\n","Epoch:  6326 , current epoch train loss:  0.0018336174543946981\n","Epoch:  6327 , current epoch train loss:  0.0018144480418413877\n","Epoch:  6328 , current epoch train loss:  0.0017519963439553976\n","Epoch:  6329 , current epoch train loss:  0.0018830819753929973\n","Epoch:  6330 , current epoch train loss:  0.0017793963197618723\n","Epoch:  6331 , current epoch train loss:  0.001724829664453864\n","Epoch:  6332 , current epoch train loss:  0.001827573636546731\n","Epoch:  6333 , current epoch train loss:  0.0017955972580239177\n","Epoch:  6334 , current epoch train loss:  0.0018071895465254784\n","Epoch:  6335 , current epoch train loss:  0.001894662855193019\n","Epoch:  6336 , current epoch train loss:  0.0020275269635021687\n","Epoch:  6337 , current epoch train loss:  0.0018612693529576063\n","Epoch:  6338 , current epoch train loss:  0.002008474664762616\n","Epoch:  6339 , current epoch train loss:  0.002003416186198592\n","Epoch:  6340 , current epoch train loss:  0.002016330836340785\n","Epoch:  6341 , current epoch train loss:  0.0020054613705724478\n","Epoch:  6342 , current epoch train loss:  0.0018818717217072845\n","Epoch:  6343 , current epoch train loss:  0.0018369583413004875\n","Epoch:  6344 , current epoch train loss:  0.002115383744239807\n","Epoch:  6345 , current epoch train loss:  0.00196059956215322\n","Epoch:  6346 , current epoch train loss:  0.0019466693047434092\n","Epoch:  6347 , current epoch train loss:  0.0017789732664823532\n","Epoch:  6348 , current epoch train loss:  0.002144125523045659\n","Epoch:  6349 , current epoch train loss:  0.002015160396695137\n","Epoch:  6350 , current epoch train loss:  0.0017496105283498764\n","Epoch:  6351 , current epoch train loss:  0.001885028905235231\n","Epoch:  6352 , current epoch train loss:  0.0021140207536518574\n","Epoch:  6353 , current epoch train loss:  0.0019544255919754505\n","Epoch:  6354 , current epoch train loss:  0.0017833722522482276\n","Epoch:  6355 , current epoch train loss:  0.0017176538240164518\n","Epoch:  6356 , current epoch train loss:  0.0017787923570722342\n","Epoch:  6357 , current epoch train loss:  0.002053078729659319\n","Epoch:  6358 , current epoch train loss:  0.0018481961451470852\n","Epoch:  6359 , current epoch train loss:  0.0019439873285591602\n","Epoch:  6360 , current epoch train loss:  0.0019405436469241977\n","Epoch:  6361 , current epoch train loss:  0.002258671447634697\n","Epoch:  6362 , current epoch train loss:  0.0017861983506008983\n","Epoch:  6363 , current epoch train loss:  0.0019767414778470993\n","Epoch:  6364 , current epoch train loss:  0.0021386127918958664\n","Epoch:  6365 , current epoch train loss:  0.002237532986328006\n","Epoch:  6366 , current epoch train loss:  0.0017886979039758444\n","Epoch:  6367 , current epoch train loss:  0.0019316168036311865\n","Epoch:  6368 , current epoch train loss:  0.002204537857323885\n","Epoch:  6369 , current epoch train loss:  0.0022370275110006332\n","Epoch:  6370 , current epoch train loss:  0.0018488166388124228\n","Epoch:  6371 , current epoch train loss:  0.0019997120834887028\n","Epoch:  6372 , current epoch train loss:  0.0020873609464615583\n","Epoch:  6373 , current epoch train loss:  0.0019906393717974424\n","Epoch:  6374 , current epoch train loss:  0.0019302609143778682\n","Epoch:  6375 , current epoch train loss:  0.0021218720357865095\n","Epoch:  6376 , current epoch train loss:  0.0022380815353244543\n","Epoch:  6377 , current epoch train loss:  0.0021144177298992872\n","Epoch:  6378 , current epoch train loss:  0.001866001170128584\n","Epoch:  6379 , current epoch train loss:  0.002221411559730768\n","Epoch:  6380 , current epoch train loss:  0.002183434320613742\n","Epoch:  6381 , current epoch train loss:  0.0020499955862760544\n","Epoch:  6382 , current epoch train loss:  0.0018400669796392322\n","Epoch:  6383 , current epoch train loss:  0.002242201939225197\n","Epoch:  6384 , current epoch train loss:  0.0021949743386358023\n","Epoch:  6385 , current epoch train loss:  0.0019483634969219565\n","Epoch:  6386 , current epoch train loss:  0.0017484540585428476\n","Epoch:  6387 , current epoch train loss:  0.0019531790167093277\n","Epoch:  6388 , current epoch train loss:  0.002081465907394886\n","Epoch:  6389 , current epoch train loss:  0.0019556551706045866\n","Epoch:  6390 , current epoch train loss:  0.0017266354989260435\n","Epoch:  6391 , current epoch train loss:  0.00200025481171906\n","Epoch:  6392 , current epoch train loss:  0.002086479216814041\n","Epoch:  6393 , current epoch train loss:  0.0019451514817774296\n","Epoch:  6394 , current epoch train loss:  0.001764706103131175\n","Epoch:  6395 , current epoch train loss:  0.00221567926928401\n","Epoch:  6396 , current epoch train loss:  0.0019676238298416138\n","Epoch:  6397 , current epoch train loss:  0.002082094084471464\n","Epoch:  6398 , current epoch train loss:  0.002269927179440856\n","Epoch:  6399 , current epoch train loss:  0.0020078537054359913\n","Epoch:  6400 , current epoch train loss:  0.0019139337819069624\n","Epoch:  6401 , current epoch train loss:  0.0017755187582224607\n","Epoch:  6402 , current epoch train loss:  0.001836619689129293\n","Epoch:  6403 , current epoch train loss:  0.0018318768125027418\n","Epoch:  6404 , current epoch train loss:  0.0019079986959695816\n","Epoch:  6405 , current epoch train loss:  0.0017920131795108318\n","Epoch:  6406 , current epoch train loss:  0.001706921262666583\n","Epoch:  6407 , current epoch train loss:  0.0019228409510105848\n","Epoch:  6408 , current epoch train loss:  0.0017741825431585312\n","Epoch:  6409 , current epoch train loss:  0.0017502836417406797\n","Epoch:  6410 , current epoch train loss:  0.0019765524193644524\n","Epoch:  6411 , current epoch train loss:  0.00192912551574409\n","Epoch:  6412 , current epoch train loss:  0.00187715794891119\n","Epoch:  6413 , current epoch train loss:  0.0017482232069596648\n","Epoch:  6414 , current epoch train loss:  0.0018196370219811797\n","Epoch:  6415 , current epoch train loss:  0.0017941072583198547\n","Epoch:  6416 , current epoch train loss:  0.0018431221833452582\n","Epoch:  6417 , current epoch train loss:  0.0016601989045739174\n","Epoch:  6418 , current epoch train loss:  0.0017345476662740111\n","Epoch:  6419 , current epoch train loss:  0.001904099015519023\n","Epoch:  6420 , current epoch train loss:  0.0018942583119496703\n","Epoch:  6421 , current epoch train loss:  0.0019049020484089851\n","Epoch:  6422 , current epoch train loss:  0.0017143597360700369\n","Epoch:  6423 , current epoch train loss:  0.0016656022053211927\n","Epoch:  6424 , current epoch train loss:  0.0018870520871132612\n","Epoch:  6425 , current epoch train loss:  0.001773824798874557\n","Epoch:  6426 , current epoch train loss:  0.0018834536895155907\n","Epoch:  6427 , current epoch train loss:  0.0017146201571449637\n","Epoch:  6428 , current epoch train loss:  0.0017442721873521805\n","Epoch:  6429 , current epoch train loss:  0.0018300502561032772\n","Epoch:  6430 , current epoch train loss:  0.0019138295901939273\n","Epoch:  6431 , current epoch train loss:  0.0019460225012153387\n","Epoch:  6432 , current epoch train loss:  0.0017456111963838339\n","Epoch:  6433 , current epoch train loss:  0.001730390707962215\n","Epoch:  6434 , current epoch train loss:  0.0017942971317097545\n","Epoch:  6435 , current epoch train loss:  0.0017784272786229849\n","Epoch:  6436 , current epoch train loss:  0.0016579022631049156\n","Epoch:  6437 , current epoch train loss:  0.0016804011538624763\n","Epoch:  6438 , current epoch train loss:  0.0018741519888862967\n","Epoch:  6439 , current epoch train loss:  0.0018082046881318092\n","Epoch:  6440 , current epoch train loss:  0.0018066329648718238\n","Epoch:  6441 , current epoch train loss:  0.0016658667009323835\n","Epoch:  6442 , current epoch train loss:  0.00172913889400661\n","Epoch:  6443 , current epoch train loss:  0.0017124249134212732\n","Epoch:  6444 , current epoch train loss:  0.0017646979540586472\n","Epoch:  6445 , current epoch train loss:  0.0016849921084940434\n","Epoch:  6446 , current epoch train loss:  0.001779069541953504\n","Epoch:  6447 , current epoch train loss:  0.0017003207467496395\n","Epoch:  6448 , current epoch train loss:  0.0017205977346748114\n","Epoch:  6449 , current epoch train loss:  0.0017914165509864688\n","Epoch:  6450 , current epoch train loss:  0.0017193679232150316\n","Epoch:  6451 , current epoch train loss:  0.0017207199707627296\n","Epoch:  6452 , current epoch train loss:  0.0017921649850904942\n","Epoch:  6453 , current epoch train loss:  0.0017201559385284781\n","Epoch:  6454 , current epoch train loss:  0.0017068739980459213\n","Epoch:  6455 , current epoch train loss:  0.0017715217545628548\n","Epoch:  6456 , current epoch train loss:  0.0018628346733748913\n","Epoch:  6457 , current epoch train loss:  0.0017718406161293387\n","Epoch:  6458 , current epoch train loss:  0.00184162228833884\n","Epoch:  6459 , current epoch train loss:  0.0019166184356436133\n","Epoch:  6460 , current epoch train loss:  0.0016957058105617762\n","Epoch:  6461 , current epoch train loss:  0.0017613975796848536\n","Epoch:  6462 , current epoch train loss:  0.002211268525570631\n","Epoch:  6463 , current epoch train loss:  0.0018373888451606035\n","Epoch:  6464 , current epoch train loss:  0.001718848361633718\n","Epoch:  6465 , current epoch train loss:  0.0019376035779714584\n","Epoch:  6466 , current epoch train loss:  0.0018709276337176561\n","Epoch:  6467 , current epoch train loss:  0.0018272147281095386\n","Epoch:  6468 , current epoch train loss:  0.001751782139763236\n","Epoch:  6469 , current epoch train loss:  0.0018476766999810934\n","Epoch:  6470 , current epoch train loss:  0.0017974702641367912\n","Epoch:  6471 , current epoch train loss:  0.0019491862040013075\n","Epoch:  6472 , current epoch train loss:  0.001914230058901012\n","Epoch:  6473 , current epoch train loss:  0.0017414757749065757\n","Epoch:  6474 , current epoch train loss:  0.0017778470646589994\n","Epoch:  6475 , current epoch train loss:  0.00218747416511178\n","Epoch:  6476 , current epoch train loss:  0.0017896473873406649\n","Epoch:  6477 , current epoch train loss:  0.0017001836095005274\n","Epoch:  6478 , current epoch train loss:  0.0019854758866131306\n","Epoch:  6479 , current epoch train loss:  0.0017794219311326742\n","Epoch:  6480 , current epoch train loss:  0.001784633263014257\n","Epoch:  6481 , current epoch train loss:  0.0016972587909549475\n","Epoch:  6482 , current epoch train loss:  0.001768298796378076\n","Epoch:  6483 , current epoch train loss:  0.0017926373984664679\n","Epoch:  6484 , current epoch train loss:  0.0017111122142523527\n","Epoch:  6485 , current epoch train loss:  0.0016956174513325095\n","Epoch:  6486 , current epoch train loss:  0.0017499672248959541\n","Epoch:  6487 , current epoch train loss:  0.0017779290210455656\n","Epoch:  6488 , current epoch train loss:  0.0016549796564504504\n","Epoch:  6489 , current epoch train loss:  0.0018130363896489143\n","Epoch:  6490 , current epoch train loss:  0.0017769894329831004\n","Epoch:  6491 , current epoch train loss:  0.0016782754100859165\n","Epoch:  6492 , current epoch train loss:  0.0017306621884927154\n","Epoch:  6493 , current epoch train loss:  0.0019068194087594748\n","Epoch:  6494 , current epoch train loss:  0.0017501454567536712\n","Epoch:  6495 , current epoch train loss:  0.0016814654227346182\n","Epoch:  6496 , current epoch train loss:  0.0017438408685848117\n","Epoch:  6497 , current epoch train loss:  0.0017868796130642295\n","Epoch:  6498 , current epoch train loss:  0.0017423976678401232\n","Epoch:  6499 , current epoch train loss:  0.0017758769681677222\n","Epoch:  6500 , current epoch train loss:  0.0017896993085741997\n","Epoch:  6501 , current epoch train loss:  0.0018826683517545462\n","Epoch:  6502 , current epoch train loss:  0.001693859463557601\n","Epoch:  6503 , current epoch train loss:  0.0017274983692914248\n","Epoch:  6504 , current epoch train loss:  0.0016974088503047824\n","Epoch:  6505 , current epoch train loss:  0.0017465581186115742\n","Epoch:  6506 , current epoch train loss:  0.0018835568334907293\n","Epoch:  6507 , current epoch train loss:  0.0017573864897713065\n","Epoch:  6508 , current epoch train loss:  0.0018630156992003322\n","Epoch:  6509 , current epoch train loss:  0.0018109036609530449\n","Epoch:  6510 , current epoch train loss:  0.001901288516819477\n","Epoch:  6511 , current epoch train loss:  0.0018656528554856777\n","Epoch:  6512 , current epoch train loss:  0.0018203016370534897\n","Epoch:  6513 , current epoch train loss:  0.0019281705608591437\n","Epoch:  6514 , current epoch train loss:  0.0017107672756537795\n","Epoch:  6515 , current epoch train loss:  0.0017051277682185173\n","Epoch:  6516 , current epoch train loss:  0.001982589950785041\n","Epoch:  6517 , current epoch train loss:  0.0018206353997811675\n","Epoch:  6518 , current epoch train loss:  0.0016746483743190765\n","Epoch:  6519 , current epoch train loss:  0.001829732907935977\n","Epoch:  6520 , current epoch train loss:  0.0018279290525242686\n","Epoch:  6521 , current epoch train loss:  0.00169932353310287\n","Epoch:  6522 , current epoch train loss:  0.0017551505006849766\n","Epoch:  6523 , current epoch train loss:  0.0017101629637181759\n","Epoch:  6524 , current epoch train loss:  0.0019149603322148323\n","Epoch:  6525 , current epoch train loss:  0.0019820802845060825\n","Epoch:  6526 , current epoch train loss:  0.0018422682769596577\n","Epoch:  6527 , current epoch train loss:  0.001741229323670268\n","Epoch:  6528 , current epoch train loss:  0.001995060360059142\n","Epoch:  6529 , current epoch train loss:  0.0020276103168725967\n","Epoch:  6530 , current epoch train loss:  0.0017862042877823114\n","Epoch:  6531 , current epoch train loss:  0.0018193157156929374\n","Epoch:  6532 , current epoch train loss:  0.0018340683309361339\n","Epoch:  6533 , current epoch train loss:  0.0023126909509301186\n","Epoch:  6534 , current epoch train loss:  0.0019187245052307844\n","Epoch:  6535 , current epoch train loss:  0.001859743962995708\n","Epoch:  6536 , current epoch train loss:  0.0019106081454083323\n","Epoch:  6537 , current epoch train loss:  0.0021555207204073668\n","Epoch:  6538 , current epoch train loss:  0.002193848602473736\n","Epoch:  6539 , current epoch train loss:  0.001964304596185684\n","Epoch:  6540 , current epoch train loss:  0.001778353238478303\n","Epoch:  6541 , current epoch train loss:  0.0019906065426766872\n","Epoch:  6542 , current epoch train loss:  0.001973584294319153\n","Epoch:  6543 , current epoch train loss:  0.0017838309286162257\n","Epoch:  6544 , current epoch train loss:  0.001786051201634109\n","Epoch:  6545 , current epoch train loss:  0.0018557555740699172\n","Epoch:  6546 , current epoch train loss:  0.0020672676619142294\n","Epoch:  6547 , current epoch train loss:  0.0019024912035092711\n","Epoch:  6548 , current epoch train loss:  0.0016745880711823702\n","Epoch:  6549 , current epoch train loss:  0.0018133155535906553\n","Epoch:  6550 , current epoch train loss:  0.0020036878995597363\n","Epoch:  6551 , current epoch train loss:  0.0018654472660273314\n","Epoch:  6552 , current epoch train loss:  0.00171503028832376\n","Epoch:  6553 , current epoch train loss:  0.001723501249216497\n","Epoch:  6554 , current epoch train loss:  0.0017852525925263762\n","Epoch:  6555 , current epoch train loss:  0.0018481432925909758\n","Epoch:  6556 , current epoch train loss:  0.0017237169668078423\n","Epoch:  6557 , current epoch train loss:  0.0017451446037739515\n","Epoch:  6558 , current epoch train loss:  0.0018749868031591177\n","Epoch:  6559 , current epoch train loss:  0.0017928264569491148\n","Epoch:  6560 , current epoch train loss:  0.0018265126273036003\n","Epoch:  6561 , current epoch train loss:  0.0016962094232439995\n","Epoch:  6562 , current epoch train loss:  0.0018314584158360958\n","Epoch:  6563 , current epoch train loss:  0.0017851343145594\n","Epoch:  6564 , current epoch train loss:  0.0019692040514200926\n","Epoch:  6565 , current epoch train loss:  0.001740943524055183\n","Epoch:  6566 , current epoch train loss:  0.001733648357912898\n","Epoch:  6567 , current epoch train loss:  0.0017300210893154144\n","Epoch:  6568 , current epoch train loss:  0.0020385379903018475\n","Epoch:  6569 , current epoch train loss:  0.002136379014700651\n","Epoch:  6570 , current epoch train loss:  0.0018358312081545591\n","Epoch:  6571 , current epoch train loss:  0.0017564658774062991\n","Epoch:  6572 , current epoch train loss:  0.0017940731486305594\n","Epoch:  6573 , current epoch train loss:  0.0018156920559704304\n","Epoch:  6574 , current epoch train loss:  0.0017349733971059322\n","Epoch:  6575 , current epoch train loss:  0.0018068344797939062\n","Epoch:  6576 , current epoch train loss:  0.0017389360582455993\n","Epoch:  6577 , current epoch train loss:  0.0019203019328415394\n","Epoch:  6578 , current epoch train loss:  0.0017033533658832312\n","Epoch:  6579 , current epoch train loss:  0.0016800443409010768\n","Epoch:  6580 , current epoch train loss:  0.0017947738524526358\n","Epoch:  6581 , current epoch train loss:  0.001754938974045217\n","Epoch:  6582 , current epoch train loss:  0.001864653779193759\n","Epoch:  6583 , current epoch train loss:  0.0017654295079410076\n","Epoch:  6584 , current epoch train loss:  0.0016577289206907153\n","Epoch:  6585 , current epoch train loss:  0.001981722190976143\n","Epoch:  6586 , current epoch train loss:  0.0019012823468074203\n","Epoch:  6587 , current epoch train loss:  0.001661378308199346\n","Epoch:  6588 , current epoch train loss:  0.001793024828657508\n","Epoch:  6589 , current epoch train loss:  0.0017742732306942344\n","Epoch:  6590 , current epoch train loss:  0.0018785863649100065\n","Epoch:  6591 , current epoch train loss:  0.0017321421764791012\n","Epoch:  6592 , current epoch train loss:  0.0018341377144679427\n","Epoch:  6593 , current epoch train loss:  0.0018000773852691054\n","Epoch:  6594 , current epoch train loss:  0.0017980741104111075\n","Epoch:  6595 , current epoch train loss:  0.001753051532432437\n","Epoch:  6596 , current epoch train loss:  0.0016640659887343645\n","Epoch:  6597 , current epoch train loss:  0.0017489742022007704\n","Epoch:  6598 , current epoch train loss:  0.0018124738708138466\n","Epoch:  6599 , current epoch train loss:  0.0017244117334485054\n","Epoch:  6600 , current epoch train loss:  0.0017476066714152694\n","Epoch:  6601 , current epoch train loss:  0.0017414276953786612\n","Epoch:  6602 , current epoch train loss:  0.0017052515177056193\n","Epoch:  6603 , current epoch train loss:  0.001726609538309276\n","Epoch:  6604 , current epoch train loss:  0.0017595285316929221\n","Epoch:  6605 , current epoch train loss:  0.0017099492251873016\n","Epoch:  6606 , current epoch train loss:  0.0017522176494821906\n","Epoch:  6607 , current epoch train loss:  0.0017711385153234005\n","Epoch:  6608 , current epoch train loss:  0.0016743913292884827\n","Epoch:  6609 , current epoch train loss:  0.0017469576559960842\n","Epoch:  6610 , current epoch train loss:  0.002008781535550952\n","Epoch:  6611 , current epoch train loss:  0.0016791883390396833\n","Epoch:  6612 , current epoch train loss:  0.001752224750816822\n","Epoch:  6613 , current epoch train loss:  0.0017137490212917328\n","Epoch:  6614 , current epoch train loss:  0.0016979806823655963\n","Epoch:  6615 , current epoch train loss:  0.0017823142698034644\n","Epoch:  6616 , current epoch train loss:  0.0016936254687607288\n","Epoch:  6617 , current epoch train loss:  0.0017610992072150111\n","Epoch:  6618 , current epoch train loss:  0.0017873018514364958\n","Epoch:  6619 , current epoch train loss:  0.001780349644832313\n","Epoch:  6620 , current epoch train loss:  0.0018455645767971873\n","Epoch:  6621 , current epoch train loss:  0.0020919214002788067\n","Epoch:  6622 , current epoch train loss:  0.001666249125264585\n","Epoch:  6623 , current epoch train loss:  0.0017893698532134295\n","Epoch:  6624 , current epoch train loss:  0.0020626948680728674\n","Epoch:  6625 , current epoch train loss:  0.0017779418267309666\n","Epoch:  6626 , current epoch train loss:  0.0016276815440505743\n","Epoch:  6627 , current epoch train loss:  0.0020649260841310024\n","Epoch:  6628 , current epoch train loss:  0.0017035220516845584\n","Epoch:  6629 , current epoch train loss:  0.0017104133730754256\n","Epoch:  6630 , current epoch train loss:  0.0017946766456589103\n","Epoch:  6631 , current epoch train loss:  0.0019244251307100058\n","Epoch:  6632 , current epoch train loss:  0.0017376719042658806\n","Epoch:  6633 , current epoch train loss:  0.0017660954035818577\n","Epoch:  6634 , current epoch train loss:  0.001658873981796205\n","Epoch:  6635 , current epoch train loss:  0.0017476261127740145\n","Epoch:  6636 , current epoch train loss:  0.001802832237444818\n","Epoch:  6637 , current epoch train loss:  0.001744565088301897\n","Epoch:  6638 , current epoch train loss:  0.0017139380797743797\n","Epoch:  6639 , current epoch train loss:  0.0017327660461887717\n","Epoch:  6640 , current epoch train loss:  0.0018943235045298934\n","Epoch:  6641 , current epoch train loss:  0.0017679155571386218\n","Epoch:  6642 , current epoch train loss:  0.0016696107340976596\n","Epoch:  6643 , current epoch train loss:  0.0017328420653939247\n","Epoch:  6644 , current epoch train loss:  0.001709301257506013\n","Epoch:  6645 , current epoch train loss:  0.0018345376010984182\n","Epoch:  6646 , current epoch train loss:  0.002108729910105467\n","Epoch:  6647 , current epoch train loss:  0.0017232752870768309\n","Epoch:  6648 , current epoch train loss:  0.0017912192270159721\n","Epoch:  6649 , current epoch train loss:  0.0016998895443975925\n","Epoch:  6650 , current epoch train loss:  0.0017628315836191177\n","Epoch:  6651 , current epoch train loss:  0.0017935771029442549\n","Epoch:  6652 , current epoch train loss:  0.0017302646301686764\n","Epoch:  6653 , current epoch train loss:  0.0017040780512616038\n","Epoch:  6654 , current epoch train loss:  0.0017746174708008766\n","Epoch:  6655 , current epoch train loss:  0.0018411165801808238\n","Epoch:  6656 , current epoch train loss:  0.0017750372644513845\n","Epoch:  6657 , current epoch train loss:  0.0017869402654469013\n","Epoch:  6658 , current epoch train loss:  0.0017079634126275778\n","Epoch:  6659 , current epoch train loss:  0.001724504865705967\n","Epoch:  6660 , current epoch train loss:  0.0017342278733849525\n","Epoch:  6661 , current epoch train loss:  0.0017170741921290755\n","Epoch:  6662 , current epoch train loss:  0.0017605375032871962\n","Epoch:  6663 , current epoch train loss:  0.0019176739733666182\n","Epoch:  6664 , current epoch train loss:  0.0016949940472841263\n","Epoch:  6665 , current epoch train loss:  0.0016906302189454436\n","Epoch:  6666 , current epoch train loss:  0.0017850964795798063\n","Epoch:  6667 , current epoch train loss:  0.0016562584787607193\n","Epoch:  6668 , current epoch train loss:  0.001720034284517169\n","Epoch:  6669 , current epoch train loss:  0.0018636466702446342\n","Epoch:  6670 , current epoch train loss:  0.0018297586357221007\n","Epoch:  6671 , current epoch train loss:  0.0016877816524356604\n","Epoch:  6672 , current epoch train loss:  0.0017620850121602416\n","Epoch:  6673 , current epoch train loss:  0.0018349707825109363\n","Epoch:  6674 , current epoch train loss:  0.0017983851721510291\n","Epoch:  6675 , current epoch train loss:  0.0018494429532438517\n","Epoch:  6676 , current epoch train loss:  0.0018285386031493545\n","Epoch:  6677 , current epoch train loss:  0.0017771401908248663\n","Epoch:  6678 , current epoch train loss:  0.0016688354080542922\n","Epoch:  6679 , current epoch train loss:  0.0016941915964707732\n","Epoch:  6680 , current epoch train loss:  0.0019121103687211871\n","Epoch:  6681 , current epoch train loss:  0.0019490522099658847\n","Epoch:  6682 , current epoch train loss:  0.001714119571261108\n","Epoch:  6683 , current epoch train loss:  0.0018054551910609007\n","Epoch:  6684 , current epoch train loss:  0.0017931213369593024\n","Epoch:  6685 , current epoch train loss:  0.0017434805631637573\n","Epoch:  6686 , current epoch train loss:  0.0016770006623119116\n","Epoch:  6687 , current epoch train loss:  0.0017468119040131569\n","Epoch:  6688 , current epoch train loss:  0.0017918625380843878\n","Epoch:  6689 , current epoch train loss:  0.0017336062155663967\n","Epoch:  6690 , current epoch train loss:  0.0018738983199000359\n","Epoch:  6691 , current epoch train loss:  0.0018290443113073707\n","Epoch:  6692 , current epoch train loss:  0.0017332034185528755\n","Epoch:  6693 , current epoch train loss:  0.001970915589481592\n","Epoch:  6694 , current epoch train loss:  0.0018477337434887886\n","Epoch:  6695 , current epoch train loss:  0.002005385234951973\n","Epoch:  6696 , current epoch train loss:  0.0018365628784522414\n","Epoch:  6697 , current epoch train loss:  0.0017155115492641926\n","Epoch:  6698 , current epoch train loss:  0.0017775375163182616\n","Epoch:  6699 , current epoch train loss:  0.0019429773092269897\n","Epoch:  6700 , current epoch train loss:  0.0019246491137892008\n","Epoch:  6701 , current epoch train loss:  0.0017107161693274975\n","Epoch:  6702 , current epoch train loss:  0.0017729063984006643\n","Epoch:  6703 , current epoch train loss:  0.0020596047397702932\n","Epoch:  6704 , current epoch train loss:  0.002006117720156908\n","Epoch:  6705 , current epoch train loss:  0.0017416581977158785\n","Epoch:  6706 , current epoch train loss:  0.0017565166344866157\n","Epoch:  6707 , current epoch train loss:  0.0017641078447923064\n","Epoch:  6708 , current epoch train loss:  0.0017708390951156616\n","Epoch:  6709 , current epoch train loss:  0.001817272393964231\n","Epoch:  6710 , current epoch train loss:  0.001744449487887323\n","Epoch:  6711 , current epoch train loss:  0.0017567783361300826\n","Epoch:  6712 , current epoch train loss:  0.0017572941724210978\n","Epoch:  6713 , current epoch train loss:  0.0018441160209476948\n","Epoch:  6714 , current epoch train loss:  0.0017666876083239913\n","Epoch:  6715 , current epoch train loss:  0.0017195743275806308\n","Epoch:  6716 , current epoch train loss:  0.0017814597813412547\n","Epoch:  6717 , current epoch train loss:  0.0017740842886269093\n","Epoch:  6718 , current epoch train loss:  0.0018170718103647232\n","Epoch:  6719 , current epoch train loss:  0.0018322698306292295\n","Epoch:  6720 , current epoch train loss:  0.001699534012004733\n","Epoch:  6721 , current epoch train loss:  0.001817217911593616\n","Epoch:  6722 , current epoch train loss:  0.0017245353665202856\n","Epoch:  6723 , current epoch train loss:  0.001770978793501854\n","Epoch:  6724 , current epoch train loss:  0.0017235006671398878\n","Epoch:  6725 , current epoch train loss:  0.0018156984588131309\n","Epoch:  6726 , current epoch train loss:  0.0016829449450597167\n","Epoch:  6727 , current epoch train loss:  0.0017123937141150236\n","Epoch:  6728 , current epoch train loss:  0.0017263437621295452\n","Epoch:  6729 , current epoch train loss:  0.001645238371565938\n","Epoch:  6730 , current epoch train loss:  0.0017551573691889644\n","Epoch:  6731 , current epoch train loss:  0.0018257362535223365\n","Epoch:  6732 , current epoch train loss:  0.001701215049251914\n","Epoch:  6733 , current epoch train loss:  0.0016982525121420622\n","Epoch:  6734 , current epoch train loss:  0.001719396561384201\n","Epoch:  6735 , current epoch train loss:  0.0017042465042322874\n","Epoch:  6736 , current epoch train loss:  0.0017901037354022264\n","Epoch:  6737 , current epoch train loss:  0.0018209705594927073\n","Epoch:  6738 , current epoch train loss:  0.0017265559872612357\n","Epoch:  6739 , current epoch train loss:  0.0017397988121956587\n","Epoch:  6740 , current epoch train loss:  0.001708118012174964\n","Epoch:  6741 , current epoch train loss:  0.0017149716150015593\n","Epoch:  6742 , current epoch train loss:  0.0016962136141955853\n","Epoch:  6743 , current epoch train loss:  0.0017793414881452918\n","Epoch:  6744 , current epoch train loss:  0.0017696706345304847\n","Epoch:  6745 , current epoch train loss:  0.001888456754386425\n","Epoch:  6746 , current epoch train loss:  0.0017527341842651367\n","Epoch:  6747 , current epoch train loss:  0.00176159106194973\n","Epoch:  6748 , current epoch train loss:  0.0017710141837596893\n","Epoch:  6749 , current epoch train loss:  0.0016907758545130491\n","Epoch:  6750 , current epoch train loss:  0.0017532140482217073\n","Epoch:  6751 , current epoch train loss:  0.0017619282007217407\n","Epoch:  6752 , current epoch train loss:  0.001752660027705133\n","Epoch:  6753 , current epoch train loss:  0.0017058899393305182\n","Epoch:  6754 , current epoch train loss:  0.0018259554635733366\n","Epoch:  6755 , current epoch train loss:  0.0018627513200044632\n","Epoch:  6756 , current epoch train loss:  0.0017552149947732687\n","Epoch:  6757 , current epoch train loss:  0.00175028620287776\n","Epoch:  6758 , current epoch train loss:  0.0016788158100098372\n","Epoch:  6759 , current epoch train loss:  0.001668749377131462\n","Epoch:  6760 , current epoch train loss:  0.0016817974392324686\n","Epoch:  6761 , current epoch train loss:  0.0017750270199030638\n","Epoch:  6762 , current epoch train loss:  0.0016869649989530444\n","Epoch:  6763 , current epoch train loss:  0.001737566664814949\n","Epoch:  6764 , current epoch train loss:  0.001718288054689765\n","Epoch:  6765 , current epoch train loss:  0.0017867780989035964\n","Epoch:  6766 , current epoch train loss:  0.001728113740682602\n","Epoch:  6767 , current epoch train loss:  0.0018818778917193413\n","Epoch:  6768 , current epoch train loss:  0.0016460299957543612\n","Epoch:  6769 , current epoch train loss:  0.0018432338256388903\n","Epoch:  6770 , current epoch train loss:  0.0017996702808886766\n","Epoch:  6771 , current epoch train loss:  0.0019093252485617995\n","Epoch:  6772 , current epoch train loss:  0.0016831166576594114\n","Epoch:  6773 , current epoch train loss:  0.0018158855382353067\n","Epoch:  6774 , current epoch train loss:  0.0016590231098234653\n","Epoch:  6775 , current epoch train loss:  0.001691822661086917\n","Epoch:  6776 , current epoch train loss:  0.0016701215645298362\n","Epoch:  6777 , current epoch train loss:  0.0017759324982762337\n","Epoch:  6778 , current epoch train loss:  0.0016927904216572642\n","Epoch:  6779 , current epoch train loss:  0.001810590736567974\n","Epoch:  6780 , current epoch train loss:  0.0017129171174019575\n","Epoch:  6781 , current epoch train loss:  0.0017270901007577777\n","Epoch:  6782 , current epoch train loss:  0.001675974577665329\n","Epoch:  6783 , current epoch train loss:  0.0016924104420468211\n","Epoch:  6784 , current epoch train loss:  0.0017280883621424437\n","Epoch:  6785 , current epoch train loss:  0.0016634967178106308\n","Epoch:  6786 , current epoch train loss:  0.0017445420380681753\n","Epoch:  6787 , current epoch train loss:  0.0017236046260222793\n","Epoch:  6788 , current epoch train loss:  0.0017132200300693512\n","Epoch:  6789 , current epoch train loss:  0.0018302416428923607\n","Epoch:  6790 , current epoch train loss:  0.001610517967492342\n","Epoch:  6791 , current epoch train loss:  0.0017222464084625244\n","Epoch:  6792 , current epoch train loss:  0.0017246399074792862\n","Epoch:  6793 , current epoch train loss:  0.0016497544711455703\n","Epoch:  6794 , current epoch train loss:  0.0017670940142124891\n","Epoch:  6795 , current epoch train loss:  0.0017270337557420135\n","Epoch:  6796 , current epoch train loss:  0.0016516202595084906\n","Epoch:  6797 , current epoch train loss:  0.0018002204596996307\n","Epoch:  6798 , current epoch train loss:  0.001772858900949359\n","Epoch:  6799 , current epoch train loss:  0.0017184074968099594\n","Epoch:  6800 , current epoch train loss:  0.0017043156549334526\n","Epoch:  6801 , current epoch train loss:  0.001749213202856481\n","Epoch:  6802 , current epoch train loss:  0.0017710209358483553\n","Epoch:  6803 , current epoch train loss:  0.001740297069773078\n","Epoch:  6804 , current epoch train loss:  0.001683020032942295\n","Epoch:  6805 , current epoch train loss:  0.0016626585274934769\n","Epoch:  6806 , current epoch train loss:  0.0016957141924649477\n","Epoch:  6807 , current epoch train loss:  0.0016679385444149375\n","Epoch:  6808 , current epoch train loss:  0.0017259863670915365\n","Epoch:  6809 , current epoch train loss:  0.001652149367146194\n","Epoch:  6810 , current epoch train loss:  0.0016547593986615539\n","Epoch:  6811 , current epoch train loss:  0.0016251795459538698\n","Epoch:  6812 , current epoch train loss:  0.0017670784145593643\n","Epoch:  6813 , current epoch train loss:  0.001737123355269432\n","Epoch:  6814 , current epoch train loss:  0.001667772768996656\n","Epoch:  6815 , current epoch train loss:  0.001697430619969964\n","Epoch:  6816 , current epoch train loss:  0.0018549661617726088\n","Epoch:  6817 , current epoch train loss:  0.0016255530063062906\n","Epoch:  6818 , current epoch train loss:  0.00166189088486135\n","Epoch:  6819 , current epoch train loss:  0.0018372846534475684\n","Epoch:  6820 , current epoch train loss:  0.0016482765786349773\n","Epoch:  6821 , current epoch train loss:  0.0016799940494820476\n","Epoch:  6822 , current epoch train loss:  0.0016726674512028694\n","Epoch:  6823 , current epoch train loss:  0.0016852733679115772\n","Epoch:  6824 , current epoch train loss:  0.0016301353462040424\n","Epoch:  6825 , current epoch train loss:  0.0016448611859232187\n","Epoch:  6826 , current epoch train loss:  0.001754805794917047\n","Epoch:  6827 , current epoch train loss:  0.0017726359656080604\n","Epoch:  6828 , current epoch train loss:  0.0016475811135023832\n","Epoch:  6829 , current epoch train loss:  0.001681182417087257\n","Epoch:  6830 , current epoch train loss:  0.0016339782159775496\n","Epoch:  6831 , current epoch train loss:  0.0016749053029343486\n","Epoch:  6832 , current epoch train loss:  0.0016728532500565052\n","Epoch:  6833 , current epoch train loss:  0.0016500197816640139\n","Epoch:  6834 , current epoch train loss:  0.0016460645711049438\n","Epoch:  6835 , current epoch train loss:  0.0016635730862617493\n","Epoch:  6836 , current epoch train loss:  0.0017026446294039488\n","Epoch:  6837 , current epoch train loss:  0.0016746795736253262\n","Epoch:  6838 , current epoch train loss:  0.0018958584405481815\n","Epoch:  6839 , current epoch train loss:  0.0016712537035346031\n","Epoch:  6840 , current epoch train loss:  0.0016685226000845432\n","Epoch:  6841 , current epoch train loss:  0.0017595401732251048\n","Epoch:  6842 , current epoch train loss:  0.0017254347912967205\n","Epoch:  6843 , current epoch train loss:  0.0016548652201890945\n","Epoch:  6844 , current epoch train loss:  0.0016723701264709234\n","Epoch:  6845 , current epoch train loss:  0.0017008974682539701\n","Epoch:  6846 , current epoch train loss:  0.0017081648111343384\n","Epoch:  6847 , current epoch train loss:  0.0016897500026971102\n","Epoch:  6848 , current epoch train loss:  0.0017213536193594337\n","Epoch:  6849 , current epoch train loss:  0.0017132123466581106\n","Epoch:  6850 , current epoch train loss:  0.0016334324609488249\n","Epoch:  6851 , current epoch train loss:  0.0017673065885901451\n","Epoch:  6852 , current epoch train loss:  0.001643868163228035\n","Epoch:  6853 , current epoch train loss:  0.0018139863386750221\n","Epoch:  6854 , current epoch train loss:  0.0016975260805338621\n","Epoch:  6855 , current epoch train loss:  0.0016622748225927353\n","Epoch:  6856 , current epoch train loss:  0.001683944370597601\n","Epoch:  6857 , current epoch train loss:  0.0019086734391748905\n","Epoch:  6858 , current epoch train loss:  0.0016860527684912086\n","Epoch:  6859 , current epoch train loss:  0.0017208545468747616\n","Epoch:  6860 , current epoch train loss:  0.0017186440527439117\n","Epoch:  6861 , current epoch train loss:  0.0018076295964419842\n","Epoch:  6862 , current epoch train loss:  0.0019370883237570524\n","Epoch:  6863 , current epoch train loss:  0.0017701496835798025\n","Epoch:  6864 , current epoch train loss:  0.001726302201859653\n","Epoch:  6865 , current epoch train loss:  0.0018159052124246955\n","Epoch:  6866 , current epoch train loss:  0.0016711894422769547\n","Epoch:  6867 , current epoch train loss:  0.0017128706676885486\n","Epoch:  6868 , current epoch train loss:  0.0016783520113676786\n","Epoch:  6869 , current epoch train loss:  0.001681037130765617\n","Epoch:  6870 , current epoch train loss:  0.001739921746775508\n","Epoch:  6871 , current epoch train loss:  0.0016916266176849604\n","Epoch:  6872 , current epoch train loss:  0.0017020972445607185\n","Epoch:  6873 , current epoch train loss:  0.0016768047353252769\n","Epoch:  6874 , current epoch train loss:  0.0016450959956273437\n","Epoch:  6875 , current epoch train loss:  0.001671378850005567\n","Epoch:  6876 , current epoch train loss:  0.0017006088746711612\n","Epoch:  6877 , current epoch train loss:  0.0016572591848671436\n","Epoch:  6878 , current epoch train loss:  0.0017030996968969703\n","Epoch:  6879 , current epoch train loss:  0.0017535481601953506\n","Epoch:  6880 , current epoch train loss:  0.001730067073367536\n","Epoch:  6881 , current epoch train loss:  0.0017377517651766539\n","Epoch:  6882 , current epoch train loss:  0.001655197935178876\n","Epoch:  6883 , current epoch train loss:  0.0016774488613009453\n","Epoch:  6884 , current epoch train loss:  0.001644845586270094\n","Epoch:  6885 , current epoch train loss:  0.001683088717982173\n","Epoch:  6886 , current epoch train loss:  0.0016552455490455031\n","Epoch:  6887 , current epoch train loss:  0.0016612932085990906\n","Epoch:  6888 , current epoch train loss:  0.0017705427017062902\n","Epoch:  6889 , current epoch train loss:  0.0016641623806208372\n","Epoch:  6890 , current epoch train loss:  0.001686137868091464\n","Epoch:  6891 , current epoch train loss:  0.0016897234600037336\n","Epoch:  6892 , current epoch train loss:  0.0017449010629206896\n","Epoch:  6893 , current epoch train loss:  0.0017635291442275047\n","Epoch:  6894 , current epoch train loss:  0.0017325968947261572\n","Epoch:  6895 , current epoch train loss:  0.00170833186712116\n","Epoch:  6896 , current epoch train loss:  0.0016874396242201328\n","Epoch:  6897 , current epoch train loss:  0.0016997016500681639\n","Epoch:  6898 , current epoch train loss:  0.0016576079651713371\n","Epoch:  6899 , current epoch train loss:  0.0016670019831508398\n","Epoch:  6900 , current epoch train loss:  0.001924402778968215\n","Epoch:  6901 , current epoch train loss:  0.0017736390000209212\n","Epoch:  6902 , current epoch train loss:  0.0018996758153662086\n","Epoch:  6903 , current epoch train loss:  0.0016511771827936172\n","Epoch:  6904 , current epoch train loss:  0.0018430324271321297\n","Epoch:  6905 , current epoch train loss:  0.0017098488751798868\n","Epoch:  6906 , current epoch train loss:  0.001751482835970819\n","Epoch:  6907 , current epoch train loss:  0.0017520934343338013\n","Epoch:  6908 , current epoch train loss:  0.001666269963607192\n","Epoch:  6909 , current epoch train loss:  0.001735948259010911\n","Epoch:  6910 , current epoch train loss:  0.0018386233132332563\n","Epoch:  6911 , current epoch train loss:  0.0017427433049306273\n","Epoch:  6912 , current epoch train loss:  0.0017192865489050746\n","Epoch:  6913 , current epoch train loss:  0.001738734426908195\n","Epoch:  6914 , current epoch train loss:  0.0018324283882975578\n","Epoch:  6915 , current epoch train loss:  0.0015870945062488317\n","Epoch:  6916 , current epoch train loss:  0.0016953054582700133\n","Epoch:  6917 , current epoch train loss:  0.001699242158792913\n","Epoch:  6918 , current epoch train loss:  0.0016104576643556356\n","Epoch:  6919 , current epoch train loss:  0.0017819125205278397\n","Epoch:  6920 , current epoch train loss:  0.0016630589962005615\n","Epoch:  6921 , current epoch train loss:  0.0017046331195160747\n","Epoch:  6922 , current epoch train loss:  0.0017722910270094872\n","Epoch:  6923 , current epoch train loss:  0.0016204755520448089\n","Epoch:  6924 , current epoch train loss:  0.0017823977395892143\n","Epoch:  6925 , current epoch train loss:  0.0018848668551072478\n","Epoch:  6926 , current epoch train loss:  0.0018121995963156223\n","Epoch:  6927 , current epoch train loss:  0.0016671835910528898\n","Epoch:  6928 , current epoch train loss:  0.001650568563491106\n","Epoch:  6929 , current epoch train loss:  0.0017201979644596577\n","Epoch:  6930 , current epoch train loss:  0.0016544408863410354\n","Epoch:  6931 , current epoch train loss:  0.0017216395353898406\n","Epoch:  6932 , current epoch train loss:  0.0017481347313150764\n","Epoch:  6933 , current epoch train loss:  0.0016982664819806814\n","Epoch:  6934 , current epoch train loss:  0.0016702193534001708\n","Epoch:  6935 , current epoch train loss:  0.001678871689364314\n","Epoch:  6936 , current epoch train loss:  0.001709998119622469\n","Epoch:  6937 , current epoch train loss:  0.0016233323840424418\n","Epoch:  6938 , current epoch train loss:  0.0016248271567746997\n","Epoch:  6939 , current epoch train loss:  0.0016394311096519232\n","Epoch:  6940 , current epoch train loss:  0.001617133617401123\n","Epoch:  6941 , current epoch train loss:  0.0018389009637758136\n","Epoch:  6942 , current epoch train loss:  0.0017282066401094198\n","Epoch:  6943 , current epoch train loss:  0.0016959374770522118\n","Epoch:  6944 , current epoch train loss:  0.001736280508339405\n","Epoch:  6945 , current epoch train loss:  0.0016203951090574265\n","Epoch:  6946 , current epoch train loss:  0.0017009952571243048\n","Epoch:  6947 , current epoch train loss:  0.0016929225530475378\n","Epoch:  6948 , current epoch train loss:  0.001696197665296495\n","Epoch:  6949 , current epoch train loss:  0.0017381592188030481\n","Epoch:  6950 , current epoch train loss:  0.001653249841183424\n","Epoch:  6951 , current epoch train loss:  0.0017861949745565653\n","Epoch:  6952 , current epoch train loss:  0.0017306823283433914\n","Epoch:  6953 , current epoch train loss:  0.0018212463473901153\n","Epoch:  6954 , current epoch train loss:  0.0019196805078536272\n","Epoch:  6955 , current epoch train loss:  0.0016670690383762121\n","Epoch:  6956 , current epoch train loss:  0.0016223753336817026\n","Epoch:  6957 , current epoch train loss:  0.001737508806400001\n","Epoch:  6958 , current epoch train loss:  0.001750999130308628\n","Epoch:  6959 , current epoch train loss:  0.001723448047414422\n","Epoch:  6960 , current epoch train loss:  0.001828075386583805\n","Epoch:  6961 , current epoch train loss:  0.0017015011981129646\n","Epoch:  6962 , current epoch train loss:  0.0016960632055997849\n","Epoch:  6963 , current epoch train loss:  0.0017420410877093673\n","Epoch:  6964 , current epoch train loss:  0.0016771977534517646\n","Epoch:  6965 , current epoch train loss:  0.001641848823055625\n","Epoch:  6966 , current epoch train loss:  0.001656862790696323\n","Epoch:  6967 , current epoch train loss:  0.0016315471148118377\n","Epoch:  6968 , current epoch train loss:  0.0016598610673099756\n","Epoch:  6969 , current epoch train loss:  0.0018215137533843517\n","Epoch:  6970 , current epoch train loss:  0.0017597547266632318\n","Epoch:  6971 , current epoch train loss:  0.001671935198828578\n","Epoch:  6972 , current epoch train loss:  0.0017813488375395536\n","Epoch:  6973 , current epoch train loss:  0.001849842956289649\n","Epoch:  6974 , current epoch train loss:  0.0016413776902481914\n","Epoch:  6975 , current epoch train loss:  0.001722061075270176\n","Epoch:  6976 , current epoch train loss:  0.0020321952179074287\n","Epoch:  6977 , current epoch train loss:  0.0016579334624111652\n","Epoch:  6978 , current epoch train loss:  0.0017253963742405176\n","Epoch:  6979 , current epoch train loss:  0.0017540514236316085\n","Epoch:  6980 , current epoch train loss:  0.0016990748699754477\n","Epoch:  6981 , current epoch train loss:  0.0017208809731528163\n","Epoch:  6982 , current epoch train loss:  0.0016689521726220846\n","Epoch:  6983 , current epoch train loss:  0.0017170717474073172\n","Epoch:  6984 , current epoch train loss:  0.001765857799910009\n","Epoch:  6985 , current epoch train loss:  0.0018301847157999873\n","Epoch:  6986 , current epoch train loss:  0.001806164626032114\n","Epoch:  6987 , current epoch train loss:  0.0019284550799056888\n","Epoch:  6988 , current epoch train loss:  0.0017664529150351882\n","Epoch:  6989 , current epoch train loss:  0.0017087580636143684\n","Epoch:  6990 , current epoch train loss:  0.0016617681831121445\n","Epoch:  6991 , current epoch train loss:  0.0017237584106624126\n","Epoch:  6992 , current epoch train loss:  0.0017849374562501907\n","Epoch:  6993 , current epoch train loss:  0.0017143418081104755\n","Epoch:  6994 , current epoch train loss:  0.0017010967712849379\n","Epoch:  6995 , current epoch train loss:  0.0016981421504169703\n","Epoch:  6996 , current epoch train loss:  0.0016987643903121352\n","Epoch:  6997 , current epoch train loss:  0.0017399722710251808\n","Epoch:  6998 , current epoch train loss:  0.001735415542498231\n","Epoch:  6999 , current epoch train loss:  0.0016947140684351325\n","Epoch:  7000 , current epoch train loss:  0.0017947556916624308\n","Epoch:  7001 , current epoch train loss:  0.0018221228383481503\n","Epoch:  7002 , current epoch train loss:  0.001763703185133636\n","Epoch:  7003 , current epoch train loss:  0.001773117808625102\n","Epoch:  7004 , current epoch train loss:  0.00198914622887969\n","Epoch:  7005 , current epoch train loss:  0.0018485256005078554\n","Epoch:  7006 , current epoch train loss:  0.001884783385321498\n","Epoch:  7007 , current epoch train loss:  0.0017770351842045784\n","Epoch:  7008 , current epoch train loss:  0.0017763314535841346\n","Epoch:  7009 , current epoch train loss:  0.001991506200283766\n","Epoch:  7010 , current epoch train loss:  0.0018129864474758506\n","Epoch:  7011 , current epoch train loss:  0.0017789987614378333\n","Epoch:  7012 , current epoch train loss:  0.0017677925061434507\n","Epoch:  7013 , current epoch train loss:  0.001957217464223504\n","Epoch:  7014 , current epoch train loss:  0.0019502982031553984\n","Epoch:  7015 , current epoch train loss:  0.0016776600386947393\n","Epoch:  7016 , current epoch train loss:  0.0017512096092104912\n","Epoch:  7017 , current epoch train loss:  0.0018284720135852695\n","Epoch:  7018 , current epoch train loss:  0.0019059436162933707\n","Epoch:  7019 , current epoch train loss:  0.0018177529564127326\n","Epoch:  7020 , current epoch train loss:  0.001818389631807804\n","Epoch:  7021 , current epoch train loss:  0.001799100893549621\n","Epoch:  7022 , current epoch train loss:  0.001897848560474813\n","Epoch:  7023 , current epoch train loss:  0.0018645547097548842\n","Epoch:  7024 , current epoch train loss:  0.001744221430271864\n","Epoch:  7025 , current epoch train loss:  0.0017382248770445585\n","Epoch:  7026 , current epoch train loss:  0.0017073133494704962\n","Epoch:  7027 , current epoch train loss:  0.001841632416471839\n","Epoch:  7028 , current epoch train loss:  0.001808494096621871\n","Epoch:  7029 , current epoch train loss:  0.0018074539257213473\n","Epoch:  7030 , current epoch train loss:  0.0016730965580791235\n","Epoch:  7031 , current epoch train loss:  0.0016773599199950695\n","Epoch:  7032 , current epoch train loss:  0.001709398697130382\n","Epoch:  7033 , current epoch train loss:  0.001781362108886242\n","Epoch:  7034 , current epoch train loss:  0.0017558671534061432\n","Epoch:  7035 , current epoch train loss:  0.0016759100835770369\n","Epoch:  7036 , current epoch train loss:  0.0016814637929201126\n","Epoch:  7037 , current epoch train loss:  0.0017230709781870246\n","Epoch:  7038 , current epoch train loss:  0.0016664726426824927\n","Epoch:  7039 , current epoch train loss:  0.0016147918067872524\n","Epoch:  7040 , current epoch train loss:  0.0015987048391252756\n","Epoch:  7041 , current epoch train loss:  0.0016427552327513695\n","Epoch:  7042 , current epoch train loss:  0.0016527678817510605\n","Epoch:  7043 , current epoch train loss:  0.0016583234537392855\n","Epoch:  7044 , current epoch train loss:  0.0016401337925344706\n","Epoch:  7045 , current epoch train loss:  0.0016684600850567222\n","Epoch:  7046 , current epoch train loss:  0.0017997905379161239\n","Epoch:  7047 , current epoch train loss:  0.0016792826354503632\n","Epoch:  7048 , current epoch train loss:  0.001860853168182075\n","Epoch:  7049 , current epoch train loss:  0.001764370477758348\n","Epoch:  7050 , current epoch train loss:  0.0016387099167332053\n","Epoch:  7051 , current epoch train loss:  0.0016700172564014792\n","Epoch:  7052 , current epoch train loss:  0.0016992272576317191\n","Epoch:  7053 , current epoch train loss:  0.0016434196149930358\n","Epoch:  7054 , current epoch train loss:  0.0016346427146345377\n","Epoch:  7055 , current epoch train loss:  0.001676834188401699\n","Epoch:  7056 , current epoch train loss:  0.0017643574392423034\n","Epoch:  7057 , current epoch train loss:  0.0016722206491976976\n","Epoch:  7058 , current epoch train loss:  0.001642448827624321\n","Epoch:  7059 , current epoch train loss:  0.001882571610622108\n","Epoch:  7060 , current epoch train loss:  0.001629554433748126\n","Epoch:  7061 , current epoch train loss:  0.001650130725465715\n","Epoch:  7062 , current epoch train loss:  0.0016999226063489914\n","Epoch:  7063 , current epoch train loss:  0.0017662947066128254\n","Epoch:  7064 , current epoch train loss:  0.0017150634666904807\n","Epoch:  7065 , current epoch train loss:  0.0016638323431834579\n","Epoch:  7066 , current epoch train loss:  0.0017369072884321213\n","Epoch:  7067 , current epoch train loss:  0.00186650431714952\n","Epoch:  7068 , current epoch train loss:  0.00168566161300987\n","Epoch:  7069 , current epoch train loss:  0.0016345841577276587\n","Epoch:  7070 , current epoch train loss:  0.0017892410978674889\n","Epoch:  7071 , current epoch train loss:  0.0016014453722164035\n","Epoch:  7072 , current epoch train loss:  0.001889253268018365\n","Epoch:  7073 , current epoch train loss:  0.001793807139620185\n","Epoch:  7074 , current epoch train loss:  0.001743936911225319\n","Epoch:  7075 , current epoch train loss:  0.0017452039755880833\n","Epoch:  7076 , current epoch train loss:  0.0016376592684537172\n","Epoch:  7077 , current epoch train loss:  0.0018184217624366283\n","Epoch:  7078 , current epoch train loss:  0.0017031007446348667\n","Epoch:  7079 , current epoch train loss:  0.001601274125277996\n","Epoch:  7080 , current epoch train loss:  0.0019039775943383574\n","Epoch:  7081 , current epoch train loss:  0.0018271426670253277\n","Epoch:  7082 , current epoch train loss:  0.0017524368595331907\n","Epoch:  7083 , current epoch train loss:  0.0016578494105488062\n","Epoch:  7084 , current epoch train loss:  0.0017060115933418274\n","Epoch:  7085 , current epoch train loss:  0.00178050238173455\n","Epoch:  7086 , current epoch train loss:  0.0019192902836948633\n","Epoch:  7087 , current epoch train loss:  0.0017455894267186522\n","Epoch:  7088 , current epoch train loss:  0.001672204933129251\n","Epoch:  7089 , current epoch train loss:  0.0017166774487122893\n","Epoch:  7090 , current epoch train loss:  0.0017354487208649516\n","Epoch:  7091 , current epoch train loss:  0.0017016336787492037\n","Epoch:  7092 , current epoch train loss:  0.001763313775882125\n","Epoch:  7093 , current epoch train loss:  0.0017802458023652434\n","Epoch:  7094 , current epoch train loss:  0.001765999710187316\n","Epoch:  7095 , current epoch train loss:  0.0018382817506790161\n","Epoch:  7096 , current epoch train loss:  0.0017239039298146963\n","Epoch:  7097 , current epoch train loss:  0.0017281797481700778\n","Epoch:  7098 , current epoch train loss:  0.0016866022488102317\n","Epoch:  7099 , current epoch train loss:  0.0017335083102807403\n","Epoch:  7100 , current epoch train loss:  0.0016811627428978682\n","Epoch:  7101 , current epoch train loss:  0.0018244804814457893\n","Epoch:  7102 , current epoch train loss:  0.0017512624617666006\n","Epoch:  7103 , current epoch train loss:  0.001757609425112605\n","Epoch:  7104 , current epoch train loss:  0.0016615166096016765\n","Epoch:  7105 , current epoch train loss:  0.0017379680648446083\n","Epoch:  7106 , current epoch train loss:  0.0017463583499193192\n","Epoch:  7107 , current epoch train loss:  0.0018574530258774757\n","Epoch:  7108 , current epoch train loss:  0.0017198938876390457\n","Epoch:  7109 , current epoch train loss:  0.001755843753926456\n","Epoch:  7110 , current epoch train loss:  0.001874053617939353\n","Epoch:  7111 , current epoch train loss:  0.0017829330172389746\n","Epoch:  7112 , current epoch train loss:  0.0016509195556864142\n","Epoch:  7113 , current epoch train loss:  0.001917361281812191\n","Epoch:  7114 , current epoch train loss:  0.0017281530890613794\n","Epoch:  7115 , current epoch train loss:  0.0016799652948975563\n","Epoch:  7116 , current epoch train loss:  0.0017367710825055838\n","Epoch:  7117 , current epoch train loss:  0.0017277415608987212\n","Epoch:  7118 , current epoch train loss:  0.001680013258010149\n","Epoch:  7119 , current epoch train loss:  0.0016944434028118849\n","Epoch:  7120 , current epoch train loss:  0.0016860996838659048\n","Epoch:  7121 , current epoch train loss:  0.0017345281084999442\n","Epoch:  7122 , current epoch train loss:  0.001830938272178173\n","Epoch:  7123 , current epoch train loss:  0.0016117710620164871\n","Epoch:  7124 , current epoch train loss:  0.001688887714408338\n","Epoch:  7125 , current epoch train loss:  0.0018357926746830344\n","Epoch:  7126 , current epoch train loss:  0.0016441679326817393\n","Epoch:  7127 , current epoch train loss:  0.0020002806559205055\n","Epoch:  7128 , current epoch train loss:  0.0016698235413059592\n","Epoch:  7129 , current epoch train loss:  0.001600915566086769\n","Epoch:  7130 , current epoch train loss:  0.001762733212672174\n","Epoch:  7131 , current epoch train loss:  0.0016770074144005775\n","Epoch:  7132 , current epoch train loss:  0.0018750743474811316\n","Epoch:  7133 , current epoch train loss:  0.0016751585062593222\n","Epoch:  7134 , current epoch train loss:  0.0016404320485889912\n","Epoch:  7135 , current epoch train loss:  0.0016755899414420128\n","Epoch:  7136 , current epoch train loss:  0.001668861834332347\n","Epoch:  7137 , current epoch train loss:  0.001679283450357616\n","Epoch:  7138 , current epoch train loss:  0.0017452072352170944\n","Epoch:  7139 , current epoch train loss:  0.00162573903799057\n","Epoch:  7140 , current epoch train loss:  0.0017352713039144874\n","Epoch:  7141 , current epoch train loss:  0.0018097830470651388\n","Epoch:  7142 , current epoch train loss:  0.0016390205128118396\n","Epoch:  7143 , current epoch train loss:  0.0016969756688922644\n","Epoch:  7144 , current epoch train loss:  0.0017138803377747536\n","Epoch:  7145 , current epoch train loss:  0.001822756021283567\n","Epoch:  7146 , current epoch train loss:  0.001652474864386022\n","Epoch:  7147 , current epoch train loss:  0.0017875712364912033\n","Epoch:  7148 , current epoch train loss:  0.0018364469287917018\n","Epoch:  7149 , current epoch train loss:  0.0017975671216845512\n","Epoch:  7150 , current epoch train loss:  0.0016380781307816505\n","Epoch:  7151 , current epoch train loss:  0.0017620017752051353\n","Epoch:  7152 , current epoch train loss:  0.0018116257851943374\n","Epoch:  7153 , current epoch train loss:  0.0022178867366164923\n","Epoch:  7154 , current epoch train loss:  0.0016783736646175385\n","Epoch:  7155 , current epoch train loss:  0.0018271401058882475\n","Epoch:  7156 , current epoch train loss:  0.0017705293139442801\n","Epoch:  7157 , current epoch train loss:  0.0017546552699059248\n","Epoch:  7158 , current epoch train loss:  0.001655768370255828\n","Epoch:  7159 , current epoch train loss:  0.0016896624583750963\n","Epoch:  7160 , current epoch train loss:  0.001824352890253067\n","Epoch:  7161 , current epoch train loss:  0.0017027109861373901\n","Epoch:  7162 , current epoch train loss:  0.0017999311676248908\n","Epoch:  7163 , current epoch train loss:  0.001699326909147203\n","Epoch:  7164 , current epoch train loss:  0.001702491077594459\n","Epoch:  7165 , current epoch train loss:  0.0019096198957413435\n","Epoch:  7166 , current epoch train loss:  0.0018076718552038074\n","Epoch:  7167 , current epoch train loss:  0.001692814053967595\n","Epoch:  7168 , current epoch train loss:  0.0017858780920505524\n","Epoch:  7169 , current epoch train loss:  0.00167702732142061\n","Epoch:  7170 , current epoch train loss:  0.0016509473789483309\n","Epoch:  7171 , current epoch train loss:  0.0016295708483085036\n","Epoch:  7172 , current epoch train loss:  0.0016813816037029028\n","Epoch:  7173 , current epoch train loss:  0.0016682036221027374\n","Epoch:  7174 , current epoch train loss:  0.0017318879254162312\n","Epoch:  7175 , current epoch train loss:  0.0017818063497543335\n","Epoch:  7176 , current epoch train loss:  0.0017097934614866972\n","Epoch:  7177 , current epoch train loss:  0.001881174510344863\n","Epoch:  7178 , current epoch train loss:  0.0018152024131268263\n","Epoch:  7179 , current epoch train loss:  0.001718080136924982\n","Epoch:  7180 , current epoch train loss:  0.0016720639541745186\n","Epoch:  7181 , current epoch train loss:  0.0016772558446973562\n","Epoch:  7182 , current epoch train loss:  0.0017040816601365805\n","Epoch:  7183 , current epoch train loss:  0.0016829654341563582\n","Epoch:  7184 , current epoch train loss:  0.0017180487047880888\n","Epoch:  7185 , current epoch train loss:  0.0016868229722604156\n","Epoch:  7186 , current epoch train loss:  0.0016444388311356306\n","Epoch:  7187 , current epoch train loss:  0.0016946131363511086\n","Epoch:  7188 , current epoch train loss:  0.0017179276328533888\n","Epoch:  7189 , current epoch train loss:  0.0017267245566472411\n","Epoch:  7190 , current epoch train loss:  0.0016983177047222853\n","Epoch:  7191 , current epoch train loss:  0.0016528340056538582\n","Epoch:  7192 , current epoch train loss:  0.0016888012178242207\n","Epoch:  7193 , current epoch train loss:  0.0017496852669864893\n","Epoch:  7194 , current epoch train loss:  0.0017933374037966132\n","Epoch:  7195 , current epoch train loss:  0.0017064651474356651\n","Epoch:  7196 , current epoch train loss:  0.0016720816493034363\n","Epoch:  7197 , current epoch train loss:  0.001677564112469554\n","Epoch:  7198 , current epoch train loss:  0.0018979220185428858\n","Epoch:  7199 , current epoch train loss:  0.0017037142533808947\n","Epoch:  7200 , current epoch train loss:  0.0018798650708049536\n","Epoch:  7201 , current epoch train loss:  0.001688641496002674\n","Epoch:  7202 , current epoch train loss:  0.0016777677228674293\n","Epoch:  7203 , current epoch train loss:  0.0019529362907633185\n","Epoch:  7204 , current epoch train loss:  0.0016781998565420508\n","Epoch:  7205 , current epoch train loss:  0.002057452918961644\n","Epoch:  7206 , current epoch train loss:  0.0016672605415806174\n","Epoch:  7207 , current epoch train loss:  0.0016516795149073005\n","Epoch:  7208 , current epoch train loss:  0.001813290873542428\n","Epoch:  7209 , current epoch train loss:  0.0017389723798260093\n","Epoch:  7210 , current epoch train loss:  0.0016902829520404339\n","Epoch:  7211 , current epoch train loss:  0.0017531337216496468\n","Epoch:  7212 , current epoch train loss:  0.0017075047362595797\n","Epoch:  7213 , current epoch train loss:  0.0017590358620509505\n","Epoch:  7214 , current epoch train loss:  0.0017608623020350933\n","Epoch:  7215 , current epoch train loss:  0.0017533217323943973\n","Epoch:  7216 , current epoch train loss:  0.001763002248480916\n","Epoch:  7217 , current epoch train loss:  0.0016916871536523104\n","Epoch:  7218 , current epoch train loss:  0.0017746705561876297\n","Epoch:  7219 , current epoch train loss:  0.0017253495752811432\n","Epoch:  7220 , current epoch train loss:  0.0016118278726935387\n","Epoch:  7221 , current epoch train loss:  0.0016549830324947834\n","Epoch:  7222 , current epoch train loss:  0.0016957761254161596\n","Epoch:  7223 , current epoch train loss:  0.0017032250761985779\n","Epoch:  7224 , current epoch train loss:  0.0016659058164805174\n","Epoch:  7225 , current epoch train loss:  0.001709610573016107\n","Epoch:  7226 , current epoch train loss:  0.0018061641603708267\n","Epoch:  7227 , current epoch train loss:  0.0016674059443175793\n","Epoch:  7228 , current epoch train loss:  0.0016828898806124926\n","Epoch:  7229 , current epoch train loss:  0.0016155792400240898\n","Epoch:  7230 , current epoch train loss:  0.001727159135043621\n","Epoch:  7231 , current epoch train loss:  0.001643335446715355\n","Epoch:  7232 , current epoch train loss:  0.0016646835720166564\n","Epoch:  7233 , current epoch train loss:  0.0016625954303890467\n","Epoch:  7234 , current epoch train loss:  0.0016588184516876936\n","Epoch:  7235 , current epoch train loss:  0.0016701826825737953\n","Epoch:  7236 , current epoch train loss:  0.0016733882948756218\n","Epoch:  7237 , current epoch train loss:  0.0016100872308015823\n","Epoch:  7238 , current epoch train loss:  0.0016132298624143004\n","Epoch:  7239 , current epoch train loss:  0.0017378316260874271\n","Epoch:  7240 , current epoch train loss:  0.0017015552148222923\n","Epoch:  7241 , current epoch train loss:  0.0017377467593178153\n","Epoch:  7242 , current epoch train loss:  0.0016696762759238482\n","Epoch:  7243 , current epoch train loss:  0.0016899702604860067\n","Epoch:  7244 , current epoch train loss:  0.0017082961276173592\n","Epoch:  7245 , current epoch train loss:  0.001830508466809988\n","Epoch:  7246 , current epoch train loss:  0.0019395058043301105\n","Epoch:  7247 , current epoch train loss:  0.0017005614936351776\n","Epoch:  7248 , current epoch train loss:  0.0016629050951451063\n","Epoch:  7249 , current epoch train loss:  0.0016445049550384283\n","Epoch:  7250 , current epoch train loss:  0.0016609688755124807\n","Epoch:  7251 , current epoch train loss:  0.0017096910160034895\n","Epoch:  7252 , current epoch train loss:  0.001742243766784668\n","Epoch:  7253 , current epoch train loss:  0.0016918465262278914\n","Epoch:  7254 , current epoch train loss:  0.0016667633317410946\n","Epoch:  7255 , current epoch train loss:  0.0017278653103858232\n","Epoch:  7256 , current epoch train loss:  0.0017309164395555854\n","Epoch:  7257 , current epoch train loss:  0.00166989304125309\n","Epoch:  7258 , current epoch train loss:  0.0017077613156288862\n","Epoch:  7259 , current epoch train loss:  0.0016569786239415407\n","Epoch:  7260 , current epoch train loss:  0.0016392925754189491\n","Epoch:  7261 , current epoch train loss:  0.0016362861497327685\n","Epoch:  7262 , current epoch train loss:  0.0016420406755059958\n","Epoch:  7263 , current epoch train loss:  0.0016236415831372142\n","Epoch:  7264 , current epoch train loss:  0.0016083077061921358\n","Epoch:  7265 , current epoch train loss:  0.0015761218965053558\n","Epoch:  7266 , current epoch train loss:  0.0016313185915350914\n","Epoch:  7267 , current epoch train loss:  0.001712461351417005\n","Epoch:  7268 , current epoch train loss:  0.0016471142880618572\n","Epoch:  7269 , current epoch train loss:  0.0016531487926840782\n","Epoch:  7270 , current epoch train loss:  0.0019128706771880388\n","Epoch:  7271 , current epoch train loss:  0.0016805287450551987\n","Epoch:  7272 , current epoch train loss:  0.001730570918880403\n","Epoch:  7273 , current epoch train loss:  0.0016914973966777325\n","Epoch:  7274 , current epoch train loss:  0.0018076226115226746\n","Epoch:  7275 , current epoch train loss:  0.0016667223535478115\n","Epoch:  7276 , current epoch train loss:  0.0017169847851619124\n","Epoch:  7277 , current epoch train loss:  0.0019145223777741194\n","Epoch:  7278 , current epoch train loss:  0.0019236429361626506\n","Epoch:  7279 , current epoch train loss:  0.001609823084436357\n","Epoch:  7280 , current epoch train loss:  0.0017070102039724588\n","Epoch:  7281 , current epoch train loss:  0.0016764852916821837\n","Epoch:  7282 , current epoch train loss:  0.001764463959261775\n","Epoch:  7283 , current epoch train loss:  0.0016678606625646353\n","Epoch:  7284 , current epoch train loss:  0.001652332372032106\n","Epoch:  7285 , current epoch train loss:  0.0019368160283192992\n","Epoch:  7286 , current epoch train loss:  0.0018290652660652995\n","Epoch:  7287 , current epoch train loss:  0.0016453072894364595\n","Epoch:  7288 , current epoch train loss:  0.0016578598879277706\n","Epoch:  7289 , current epoch train loss:  0.0016816963907331228\n","Epoch:  7290 , current epoch train loss:  0.0016277963295578957\n","Epoch:  7291 , current epoch train loss:  0.001608286052942276\n","Epoch:  7292 , current epoch train loss:  0.0015697897179052234\n","Epoch:  7293 , current epoch train loss:  0.0016462694620713592\n","Epoch:  7294 , current epoch train loss:  0.001728108967654407\n","Epoch:  7295 , current epoch train loss:  0.001661796821281314\n","Epoch:  7296 , current epoch train loss:  0.0016545633552595973\n","Epoch:  7297 , current epoch train loss:  0.001663173083215952\n","Epoch:  7298 , current epoch train loss:  0.0016351845115423203\n","Epoch:  7299 , current epoch train loss:  0.0016560647636651993\n","Epoch:  7300 , current epoch train loss:  0.0016456603771075606\n","Epoch:  7301 , current epoch train loss:  0.0016658840468153358\n","Epoch:  7302 , current epoch train loss:  0.0016212606569752097\n","Epoch:  7303 , current epoch train loss:  0.001662728376686573\n","Epoch:  7304 , current epoch train loss:  0.001688681310042739\n","Epoch:  7305 , current epoch train loss:  0.0016140820225700736\n","Epoch:  7306 , current epoch train loss:  0.0016605416312813759\n","Epoch:  7307 , current epoch train loss:  0.0016902657225728035\n","Epoch:  7308 , current epoch train loss:  0.0016388961812481284\n","Epoch:  7309 , current epoch train loss:  0.001582208089530468\n","Epoch:  7310 , current epoch train loss:  0.0017065260326489806\n","Epoch:  7311 , current epoch train loss:  0.0016500994097441435\n","Epoch:  7312 , current epoch train loss:  0.0016334110405296087\n","Epoch:  7313 , current epoch train loss:  0.001651646918617189\n","Epoch:  7314 , current epoch train loss:  0.0017109381733462214\n","Epoch:  7315 , current epoch train loss:  0.0017370934365317225\n","Epoch:  7316 , current epoch train loss:  0.0016631338512524962\n","Epoch:  7317 , current epoch train loss:  0.0016571653541177511\n","Epoch:  7318 , current epoch train loss:  0.0016438732855021954\n","Epoch:  7319 , current epoch train loss:  0.0017627847846597433\n","Epoch:  7320 , current epoch train loss:  0.0018894842360168695\n","Epoch:  7321 , current epoch train loss:  0.0016543061938136816\n","Epoch:  7322 , current epoch train loss:  0.0016941989306360483\n","Epoch:  7323 , current epoch train loss:  0.0018145448993891478\n","Epoch:  7324 , current epoch train loss:  0.0016834340058267117\n","Epoch:  7325 , current epoch train loss:  0.0016553871100768447\n","Epoch:  7326 , current epoch train loss:  0.0016561195952817798\n","Epoch:  7327 , current epoch train loss:  0.0017921696417033672\n","Epoch:  7328 , current epoch train loss:  0.0018241142388433218\n","Epoch:  7329 , current epoch train loss:  0.0016335019608959556\n","Epoch:  7330 , current epoch train loss:  0.00164070725440979\n","Epoch:  7331 , current epoch train loss:  0.001745932619087398\n","Epoch:  7332 , current epoch train loss:  0.001765896799042821\n","Epoch:  7333 , current epoch train loss:  0.0015945693012326956\n","Epoch:  7334 , current epoch train loss:  0.0016491947462782264\n","Epoch:  7335 , current epoch train loss:  0.0016847125953063369\n","Epoch:  7336 , current epoch train loss:  0.0016853659180924296\n","Epoch:  7337 , current epoch train loss:  0.0016098504420369864\n","Epoch:  7338 , current epoch train loss:  0.0016490494599565864\n","Epoch:  7339 , current epoch train loss:  0.0017941389232873917\n","Epoch:  7340 , current epoch train loss:  0.001674887491390109\n","Epoch:  7341 , current epoch train loss:  0.0018015075474977493\n","Epoch:  7342 , current epoch train loss:  0.0016630756435915828\n","Epoch:  7343 , current epoch train loss:  0.001766716130077839\n","Epoch:  7344 , current epoch train loss:  0.001646941527724266\n","Epoch:  7345 , current epoch train loss:  0.0016880988841876388\n","Epoch:  7346 , current epoch train loss:  0.0017823026282712817\n","Epoch:  7347 , current epoch train loss:  0.0016474060248583555\n","Epoch:  7348 , current epoch train loss:  0.0016828248044475913\n","Epoch:  7349 , current epoch train loss:  0.0017622262239456177\n","Epoch:  7350 , current epoch train loss:  0.001729317707940936\n","Epoch:  7351 , current epoch train loss:  0.0017265536589547992\n","Epoch:  7352 , current epoch train loss:  0.0016067044343799353\n","Epoch:  7353 , current epoch train loss:  0.0016441443003714085\n","Epoch:  7354 , current epoch train loss:  0.0016480818158015609\n","Epoch:  7355 , current epoch train loss:  0.0016287716571241617\n","Epoch:  7356 , current epoch train loss:  0.0017004564870148897\n","Epoch:  7357 , current epoch train loss:  0.0017077317461371422\n","Epoch:  7358 , current epoch train loss:  0.0016556832706555724\n","Epoch:  7359 , current epoch train loss:  0.0017064372077584267\n","Epoch:  7360 , current epoch train loss:  0.0016934986924752593\n","Epoch:  7361 , current epoch train loss:  0.0018562417244538665\n","Epoch:  7362 , current epoch train loss:  0.001738873077556491\n","Epoch:  7363 , current epoch train loss:  0.0017885281704366207\n","Epoch:  7364 , current epoch train loss:  0.0016462195198982954\n","Epoch:  7365 , current epoch train loss:  0.0016066757962107658\n","Epoch:  7366 , current epoch train loss:  0.0017141051357612014\n","Epoch:  7367 , current epoch train loss:  0.0016413829289376736\n","Epoch:  7368 , current epoch train loss:  0.0016367932548746467\n","Epoch:  7369 , current epoch train loss:  0.0016306054312735796\n","Epoch:  7370 , current epoch train loss:  0.0017314845463261008\n","Epoch:  7371 , current epoch train loss:  0.0017839388456195593\n","Epoch:  7372 , current epoch train loss:  0.0016720336861908436\n","Epoch:  7373 , current epoch train loss:  0.0018256697803735733\n","Epoch:  7374 , current epoch train loss:  0.0016859520692378283\n","Epoch:  7375 , current epoch train loss:  0.0017399698263034225\n","Epoch:  7376 , current epoch train loss:  0.0017433024477213621\n","Epoch:  7377 , current epoch train loss:  0.0016516752075403929\n","Epoch:  7378 , current epoch train loss:  0.001674411236308515\n","Epoch:  7379 , current epoch train loss:  0.0017043538391590118\n","Epoch:  7380 , current epoch train loss:  0.0016812782268971205\n","Epoch:  7381 , current epoch train loss:  0.0017245596973225474\n","Epoch:  7382 , current epoch train loss:  0.0016793699469417334\n","Epoch:  7383 , current epoch train loss:  0.0016858767485246062\n","Epoch:  7384 , current epoch train loss:  0.001680724322795868\n","Epoch:  7385 , current epoch train loss:  0.0016244698781520128\n","Epoch:  7386 , current epoch train loss:  0.0017788668628782034\n","Epoch:  7387 , current epoch train loss:  0.0016697042156010866\n","Epoch:  7388 , current epoch train loss:  0.0016448682872578502\n","Epoch:  7389 , current epoch train loss:  0.001612544059753418\n","Epoch:  7390 , current epoch train loss:  0.001615169458091259\n","Epoch:  7391 , current epoch train loss:  0.00167856365442276\n","Epoch:  7392 , current epoch train loss:  0.001724868780001998\n","Epoch:  7393 , current epoch train loss:  0.0017478405497968197\n","Epoch:  7394 , current epoch train loss:  0.0018398802494630218\n","Epoch:  7395 , current epoch train loss:  0.001796723110601306\n","Epoch:  7396 , current epoch train loss:  0.0016681181732565165\n","Epoch:  7397 , current epoch train loss:  0.0016628917073830962\n","Epoch:  7398 , current epoch train loss:  0.001716609112918377\n","Epoch:  7399 , current epoch train loss:  0.0016561555676162243\n","Epoch:  7400 , current epoch train loss:  0.0020643037278205156\n","Epoch:  7401 , current epoch train loss:  0.0017060337122529745\n","Epoch:  7402 , current epoch train loss:  0.0016888119280338287\n","Epoch:  7403 , current epoch train loss:  0.001765287364833057\n","Epoch:  7404 , current epoch train loss:  0.001663918956182897\n","Epoch:  7405 , current epoch train loss:  0.001683913986198604\n","Epoch:  7406 , current epoch train loss:  0.0017852250020951033\n","Epoch:  7407 , current epoch train loss:  0.0017381161451339722\n","Epoch:  7408 , current epoch train loss:  0.0016293212538585067\n","Epoch:  7409 , current epoch train loss:  0.001749119139276445\n","Epoch:  7410 , current epoch train loss:  0.001660341746173799\n","Epoch:  7411 , current epoch train loss:  0.0016389258671551943\n","Epoch:  7412 , current epoch train loss:  0.0017288107192143798\n","Epoch:  7413 , current epoch train loss:  0.0016396380960941315\n","Epoch:  7414 , current epoch train loss:  0.0016498033655807376\n","Epoch:  7415 , current epoch train loss:  0.0016770630609244108\n","Epoch:  7416 , current epoch train loss:  0.0017007155111059546\n","Epoch:  7417 , current epoch train loss:  0.001672790152952075\n","Epoch:  7418 , current epoch train loss:  0.0016367991920560598\n","Epoch:  7419 , current epoch train loss:  0.001631703577004373\n","Epoch:  7420 , current epoch train loss:  0.0016824053600430489\n","Epoch:  7421 , current epoch train loss:  0.0016566779231652617\n","Epoch:  7422 , current epoch train loss:  0.0016775038093328476\n","Epoch:  7423 , current epoch train loss:  0.0016058534383773804\n","Epoch:  7424 , current epoch train loss:  0.0016334853135049343\n","Epoch:  7425 , current epoch train loss:  0.0016401985194534063\n","Epoch:  7426 , current epoch train loss:  0.0015740630915388465\n","Epoch:  7427 , current epoch train loss:  0.001639440655708313\n","Epoch:  7428 , current epoch train loss:  0.001607197686098516\n","Epoch:  7429 , current epoch train loss:  0.001702020294032991\n","Epoch:  7430 , current epoch train loss:  0.0016347523778676987\n","Epoch:  7431 , current epoch train loss:  0.0016438220627605915\n","Epoch:  7432 , current epoch train loss:  0.0016325967153534293\n","Epoch:  7433 , current epoch train loss:  0.001752213342115283\n","Epoch:  7434 , current epoch train loss:  0.0016592806205153465\n","Epoch:  7435 , current epoch train loss:  0.001668414450250566\n","Epoch:  7436 , current epoch train loss:  0.0016295849345624447\n","Epoch:  7437 , current epoch train loss:  0.0016394400736317039\n","Epoch:  7438 , current epoch train loss:  0.0016848644008859992\n","Epoch:  7439 , current epoch train loss:  0.0016344672767445445\n","Epoch:  7440 , current epoch train loss:  0.0016388202784582973\n","Epoch:  7441 , current epoch train loss:  0.0016546616097912192\n","Epoch:  7442 , current epoch train loss:  0.0017511410405859351\n","Epoch:  7443 , current epoch train loss:  0.0017509409226477146\n","Epoch:  7444 , current epoch train loss:  0.0017038666410371661\n","Epoch:  7445 , current epoch train loss:  0.0016910405829548836\n","Epoch:  7446 , current epoch train loss:  0.0016966387629508972\n","Epoch:  7447 , current epoch train loss:  0.0017021943349391222\n","Epoch:  7448 , current epoch train loss:  0.0016109537100419402\n","Epoch:  7449 , current epoch train loss:  0.001628124970011413\n","Epoch:  7450 , current epoch train loss:  0.0016137806233018637\n","Epoch:  7451 , current epoch train loss:  0.0016031110426411033\n","Epoch:  7452 , current epoch train loss:  0.0016330982325598598\n","Epoch:  7453 , current epoch train loss:  0.0016033321153372526\n","Epoch:  7454 , current epoch train loss:  0.0016181672690436244\n","Epoch:  7455 , current epoch train loss:  0.001746476162225008\n","Epoch:  7456 , current epoch train loss:  0.0016455240547657013\n","Epoch:  7457 , current epoch train loss:  0.0017108613392338157\n","Epoch:  7458 , current epoch train loss:  0.0016431985422968864\n","Epoch:  7459 , current epoch train loss:  0.0016748092602938414\n","Epoch:  7460 , current epoch train loss:  0.001803372404538095\n","Epoch:  7461 , current epoch train loss:  0.0016861585900187492\n","Epoch:  7462 , current epoch train loss:  0.0016679475083947182\n","Epoch:  7463 , current epoch train loss:  0.0018344996497035027\n","Epoch:  7464 , current epoch train loss:  0.0016613602638244629\n","Epoch:  7465 , current epoch train loss:  0.001601682510226965\n","Epoch:  7466 , current epoch train loss:  0.0016960592474788427\n","Epoch:  7467 , current epoch train loss:  0.0017226703930646181\n","Epoch:  7468 , current epoch train loss:  0.001713874051347375\n","Epoch:  7469 , current epoch train loss:  0.001641891896724701\n","Epoch:  7470 , current epoch train loss:  0.0016290837666019797\n","Epoch:  7471 , current epoch train loss:  0.0016525433165952563\n","Epoch:  7472 , current epoch train loss:  0.0017243655165657401\n","Epoch:  7473 , current epoch train loss:  0.0017034857301041484\n","Epoch:  7474 , current epoch train loss:  0.0016930308192968369\n","Epoch:  7475 , current epoch train loss:  0.0016792056849226356\n","Epoch:  7476 , current epoch train loss:  0.001694545499049127\n","Epoch:  7477 , current epoch train loss:  0.0016266119200736284\n","Epoch:  7478 , current epoch train loss:  0.0018506426131352782\n","Epoch:  7479 , current epoch train loss:  0.001612694701179862\n","Epoch:  7480 , current epoch train loss:  0.0016669512260705233\n","Epoch:  7481 , current epoch train loss:  0.0016236582305282354\n","Epoch:  7482 , current epoch train loss:  0.0016607409343123436\n","Epoch:  7483 , current epoch train loss:  0.0016093277372419834\n","Epoch:  7484 , current epoch train loss:  0.0016057811444625258\n","Epoch:  7485 , current epoch train loss:  0.0017146493773907423\n","Epoch:  7486 , current epoch train loss:  0.0015911266673356295\n","Epoch:  7487 , current epoch train loss:  0.001661175163462758\n","Epoch:  7488 , current epoch train loss:  0.001602771459147334\n","Epoch:  7489 , current epoch train loss:  0.0016449008835479617\n","Epoch:  7490 , current epoch train loss:  0.0016170984599739313\n","Epoch:  7491 , current epoch train loss:  0.0017526966985315084\n","Epoch:  7492 , current epoch train loss:  0.0016304816817864776\n","Epoch:  7493 , current epoch train loss:  0.0016496805474162102\n","Epoch:  7494 , current epoch train loss:  0.0016280608251690865\n","Epoch:  7495 , current epoch train loss:  0.0016003818018361926\n","Epoch:  7496 , current epoch train loss:  0.0016107528936117887\n","Epoch:  7497 , current epoch train loss:  0.001663109054788947\n","Epoch:  7498 , current epoch train loss:  0.0017226075287908316\n","Epoch:  7499 , current epoch train loss:  0.0016695903614163399\n","Epoch:  7500 , current epoch train loss:  0.0017954008653759956\n","Epoch:  7501 , current epoch train loss:  0.0016692010685801506\n","Epoch:  7502 , current epoch train loss:  0.0017134007066488266\n","Epoch:  7503 , current epoch train loss:  0.0016873914282768965\n","Epoch:  7504 , current epoch train loss:  0.0016433154232800007\n","Epoch:  7505 , current epoch train loss:  0.001613776432350278\n","Epoch:  7506 , current epoch train loss:  0.0016448523383587599\n","Epoch:  7507 , current epoch train loss:  0.0017740217735990882\n","Epoch:  7508 , current epoch train loss:  0.0016720311250537634\n","Epoch:  7509 , current epoch train loss:  0.00162741809617728\n","Epoch:  7510 , current epoch train loss:  0.0016187347937375307\n","Epoch:  7511 , current epoch train loss:  0.0016567760612815619\n","Epoch:  7512 , current epoch train loss:  0.0016291345236822963\n","Epoch:  7513 , current epoch train loss:  0.0016356933629140258\n","Epoch:  7514 , current epoch train loss:  0.0016696262173354626\n","Epoch:  7515 , current epoch train loss:  0.001649058423936367\n","Epoch:  7516 , current epoch train loss:  0.0018119453452527523\n","Epoch:  7517 , current epoch train loss:  0.0016239879187196493\n","Epoch:  7518 , current epoch train loss:  0.0017772854771465063\n","Epoch:  7519 , current epoch train loss:  0.0016792034730315208\n","Epoch:  7520 , current epoch train loss:  0.0016231706831604242\n","Epoch:  7521 , current epoch train loss:  0.0016320202266797423\n","Epoch:  7522 , current epoch train loss:  0.0016857435693964362\n","Epoch:  7523 , current epoch train loss:  0.0016755301039665937\n","Epoch:  7524 , current epoch train loss:  0.0016457734163850546\n","Epoch:  7525 , current epoch train loss:  0.0016446708468720317\n","Epoch:  7526 , current epoch train loss:  0.0016400553286075592\n","Epoch:  7527 , current epoch train loss:  0.0017075359355658293\n","Epoch:  7528 , current epoch train loss:  0.0016682669520378113\n","Epoch:  7529 , current epoch train loss:  0.001709025353193283\n","Epoch:  7530 , current epoch train loss:  0.0016229774337261915\n","Epoch:  7531 , current epoch train loss:  0.0016175182536244392\n","Epoch:  7532 , current epoch train loss:  0.0016234626527875662\n","Epoch:  7533 , current epoch train loss:  0.0015895550604909658\n","Epoch:  7534 , current epoch train loss:  0.0016406027134507895\n","Epoch:  7535 , current epoch train loss:  0.0016816649585962296\n","Epoch:  7536 , current epoch train loss:  0.001626352546736598\n","Epoch:  7537 , current epoch train loss:  0.001731439377181232\n","Epoch:  7538 , current epoch train loss:  0.0016882945783436298\n","Epoch:  7539 , current epoch train loss:  0.0016228945460170507\n","Epoch:  7540 , current epoch train loss:  0.0016189449233934283\n","Epoch:  7541 , current epoch train loss:  0.0017852479359135032\n","Epoch:  7542 , current epoch train loss:  0.0016151790041476488\n","Epoch:  7543 , current epoch train loss:  0.001675692736171186\n","Epoch:  7544 , current epoch train loss:  0.001620229217223823\n","Epoch:  7545 , current epoch train loss:  0.001763917040079832\n","Epoch:  7546 , current epoch train loss:  0.00161261938046664\n","Epoch:  7547 , current epoch train loss:  0.0016349375946447253\n","Epoch:  7548 , current epoch train loss:  0.0017005661502480507\n","Epoch:  7549 , current epoch train loss:  0.0016236830269917846\n","Epoch:  7550 , current epoch train loss:  0.0017310106195509434\n","Epoch:  7551 , current epoch train loss:  0.0016680711414664984\n","Epoch:  7552 , current epoch train loss:  0.0016171259339898825\n","Epoch:  7553 , current epoch train loss:  0.00161928019952029\n","Epoch:  7554 , current epoch train loss:  0.0017351433634757996\n","Epoch:  7555 , current epoch train loss:  0.0016820586752146482\n","Epoch:  7556 , current epoch train loss:  0.0016766255721449852\n","Epoch:  7557 , current epoch train loss:  0.001644227420911193\n","Epoch:  7558 , current epoch train loss:  0.0016022964846342802\n","Epoch:  7559 , current epoch train loss:  0.0016537446063011885\n","Epoch:  7560 , current epoch train loss:  0.001692627789452672\n","Epoch:  7561 , current epoch train loss:  0.0015931049128994346\n","Epoch:  7562 , current epoch train loss:  0.001691565616056323\n","Epoch:  7563 , current epoch train loss:  0.0016201193211600184\n","Epoch:  7564 , current epoch train loss:  0.0016438430175185204\n","Epoch:  7565 , current epoch train loss:  0.0015734591288492084\n","Epoch:  7566 , current epoch train loss:  0.001735321944579482\n","Epoch:  7567 , current epoch train loss:  0.0016178649384528399\n","Epoch:  7568 , current epoch train loss:  0.0016134176403284073\n","Epoch:  7569 , current epoch train loss:  0.0016377826686948538\n","Epoch:  7570 , current epoch train loss:  0.0016228151507675648\n","Epoch:  7571 , current epoch train loss:  0.00164000503718853\n","Epoch:  7572 , current epoch train loss:  0.0016596385976299644\n","Epoch:  7573 , current epoch train loss:  0.0020894883200526237\n","Epoch:  7574 , current epoch train loss:  0.0016586689744144678\n","Epoch:  7575 , current epoch train loss:  0.0016754246316850185\n","Epoch:  7576 , current epoch train loss:  0.0016425512731075287\n","Epoch:  7577 , current epoch train loss:  0.0016240363474935293\n","Epoch:  7578 , current epoch train loss:  0.0016661261906847358\n","Epoch:  7579 , current epoch train loss:  0.0017620064318180084\n","Epoch:  7580 , current epoch train loss:  0.0016336182598024607\n","Epoch:  7581 , current epoch train loss:  0.0017081907717511058\n","Epoch:  7582 , current epoch train loss:  0.001622833777219057\n","Epoch:  7583 , current epoch train loss:  0.0017646923661231995\n","Epoch:  7584 , current epoch train loss:  0.00166721036657691\n","Epoch:  7585 , current epoch train loss:  0.0016510633286088705\n","Epoch:  7586 , current epoch train loss:  0.0017418626230210066\n","Epoch:  7587 , current epoch train loss:  0.0016430828254669905\n","Epoch:  7588 , current epoch train loss:  0.001727067632600665\n","Epoch:  7589 , current epoch train loss:  0.0016507835825905204\n","Epoch:  7590 , current epoch train loss:  0.0015941653400659561\n","Epoch:  7591 , current epoch train loss:  0.001660277834162116\n","Epoch:  7592 , current epoch train loss:  0.001639053924009204\n","Epoch:  7593 , current epoch train loss:  0.0017096363008022308\n","Epoch:  7594 , current epoch train loss:  0.0016100559150800109\n","Epoch:  7595 , current epoch train loss:  0.001758436905220151\n","Epoch:  7596 , current epoch train loss:  0.0016072075814008713\n","Epoch:  7597 , current epoch train loss:  0.0016181773971766233\n","Epoch:  7598 , current epoch train loss:  0.0016240807017311454\n","Epoch:  7599 , current epoch train loss:  0.001611904939636588\n","Epoch:  7600 , current epoch train loss:  0.0016324555035680532\n","Epoch:  7601 , current epoch train loss:  0.0016247456660494208\n","Epoch:  7602 , current epoch train loss:  0.0016442787600681186\n","Epoch:  7603 , current epoch train loss:  0.0017414004541933537\n","Epoch:  7604 , current epoch train loss:  0.0015814173966646194\n","Epoch:  7605 , current epoch train loss:  0.001784748863428831\n","Epoch:  7606 , current epoch train loss:  0.0015980899333953857\n","Epoch:  7607 , current epoch train loss:  0.0016753056552261114\n","Epoch:  7608 , current epoch train loss:  0.0016084298258647323\n","Epoch:  7609 , current epoch train loss:  0.00164514291100204\n","Epoch:  7610 , current epoch train loss:  0.0016291660722345114\n","Epoch:  7611 , current epoch train loss:  0.0016908529214560986\n","Epoch:  7612 , current epoch train loss:  0.0016263670986518264\n","Epoch:  7613 , current epoch train loss:  0.0016799926524981856\n","Epoch:  7614 , current epoch train loss:  0.0015898256096988916\n","Epoch:  7615 , current epoch train loss:  0.001851627603173256\n","Epoch:  7616 , current epoch train loss:  0.001630325336009264\n","Epoch:  7617 , current epoch train loss:  0.001718269195407629\n","Epoch:  7618 , current epoch train loss:  0.0016538923373445868\n","Epoch:  7619 , current epoch train loss:  0.0016985766123980284\n","Epoch:  7620 , current epoch train loss:  0.0016313338419422507\n","Epoch:  7621 , current epoch train loss:  0.001614428823813796\n","Epoch:  7622 , current epoch train loss:  0.0016862351913005114\n","Epoch:  7623 , current epoch train loss:  0.001636126427911222\n","Epoch:  7624 , current epoch train loss:  0.0015869186026975513\n","Epoch:  7625 , current epoch train loss:  0.001640735543332994\n","Epoch:  7626 , current epoch train loss:  0.0015967832878232002\n","Epoch:  7627 , current epoch train loss:  0.001728530041873455\n","Epoch:  7628 , current epoch train loss:  0.001819729688577354\n","Epoch:  7629 , current epoch train loss:  0.0015799369430169463\n","Epoch:  7630 , current epoch train loss:  0.0017855268670246005\n","Epoch:  7631 , current epoch train loss:  0.0017242113826796412\n","Epoch:  7632 , current epoch train loss:  0.0016546212136745453\n","Epoch:  7633 , current epoch train loss:  0.0016538179479539394\n","Epoch:  7634 , current epoch train loss:  0.0016606394201517105\n","Epoch:  7635 , current epoch train loss:  0.0018390759360045195\n","Epoch:  7636 , current epoch train loss:  0.001694351783953607\n","Epoch:  7637 , current epoch train loss:  0.0016863927012309432\n","Epoch:  7638 , current epoch train loss:  0.0016164341941475868\n","Epoch:  7639 , current epoch train loss:  0.0016582561656832695\n","Epoch:  7640 , current epoch train loss:  0.0017398186028003693\n","Epoch:  7641 , current epoch train loss:  0.001649380661547184\n","Epoch:  7642 , current epoch train loss:  0.001663528149947524\n","Epoch:  7643 , current epoch train loss:  0.0016922501381486654\n","Epoch:  7644 , current epoch train loss:  0.0016984380781650543\n","Epoch:  7645 , current epoch train loss:  0.0016026580706238747\n","Epoch:  7646 , current epoch train loss:  0.0016607427969574928\n","Epoch:  7647 , current epoch train loss:  0.0016865476500242949\n","Epoch:  7648 , current epoch train loss:  0.001655378844588995\n","Epoch:  7649 , current epoch train loss:  0.001653605024330318\n","Epoch:  7650 , current epoch train loss:  0.0016528377309441566\n","Epoch:  7651 , current epoch train loss:  0.0018070533405989408\n","Epoch:  7652 , current epoch train loss:  0.0017167482292279601\n","Epoch:  7653 , current epoch train loss:  0.001619867980480194\n","Epoch:  7654 , current epoch train loss:  0.0016027743695303798\n","Epoch:  7655 , current epoch train loss:  0.001621697098016739\n","Epoch:  7656 , current epoch train loss:  0.001692234887741506\n","Epoch:  7657 , current epoch train loss:  0.0016909018158912659\n","Epoch:  7658 , current epoch train loss:  0.0016503063961863518\n","Epoch:  7659 , current epoch train loss:  0.0016958232736214995\n","Epoch:  7660 , current epoch train loss:  0.0016566956182941794\n","Epoch:  7661 , current epoch train loss:  0.0016205426072701812\n","Epoch:  7662 , current epoch train loss:  0.0017172477673739195\n","Epoch:  7663 , current epoch train loss:  0.0016996313352137804\n","Epoch:  7664 , current epoch train loss:  0.0016050515696406364\n","Epoch:  7665 , current epoch train loss:  0.0016516731120646\n","Epoch:  7666 , current epoch train loss:  0.0017109428299590945\n","Epoch:  7667 , current epoch train loss:  0.0016489074332639575\n","Epoch:  7668 , current epoch train loss:  0.0016403578920289874\n","Epoch:  7669 , current epoch train loss:  0.0018509549554437399\n","Epoch:  7670 , current epoch train loss:  0.0018178363097831607\n","Epoch:  7671 , current epoch train loss:  0.0016417752485722303\n","Epoch:  7672 , current epoch train loss:  0.0016269659390673041\n","Epoch:  7673 , current epoch train loss:  0.0017402854282408953\n","Epoch:  7674 , current epoch train loss:  0.0017310816328972578\n","Epoch:  7675 , current epoch train loss:  0.0016649755416437984\n","Epoch:  7676 , current epoch train loss:  0.0016619566595181823\n","Epoch:  7677 , current epoch train loss:  0.0016591328894719481\n","Epoch:  7678 , current epoch train loss:  0.0016146372072398663\n","Epoch:  7679 , current epoch train loss:  0.0017378879711031914\n","Epoch:  7680 , current epoch train loss:  0.001640315167605877\n","Epoch:  7681 , current epoch train loss:  0.0015864865854382515\n","Epoch:  7682 , current epoch train loss:  0.0016273800283670425\n","Epoch:  7683 , current epoch train loss:  0.0017456177156418562\n","Epoch:  7684 , current epoch train loss:  0.0016543007222935557\n","Epoch:  7685 , current epoch train loss:  0.001604706048965454\n","Epoch:  7686 , current epoch train loss:  0.0015915741678327322\n","Epoch:  7687 , current epoch train loss:  0.0017289519309997559\n","Epoch:  7688 , current epoch train loss:  0.0016343134921044111\n","Epoch:  7689 , current epoch train loss:  0.0016392695251852274\n","Epoch:  7690 , current epoch train loss:  0.0016664830036461353\n","Epoch:  7691 , current epoch train loss:  0.00163670489564538\n","Epoch:  7692 , current epoch train loss:  0.0016618275549262762\n","Epoch:  7693 , current epoch train loss:  0.001659009954892099\n","Epoch:  7694 , current epoch train loss:  0.0015662303194403648\n","Epoch:  7695 , current epoch train loss:  0.0016321323346346617\n","Epoch:  7696 , current epoch train loss:  0.001650896854698658\n","Epoch:  7697 , current epoch train loss:  0.001643827185034752\n","Epoch:  7698 , current epoch train loss:  0.0017938142409548163\n","Epoch:  7699 , current epoch train loss:  0.0015921052545309067\n","Epoch:  7700 , current epoch train loss:  0.0016746523324400187\n","Epoch:  7701 , current epoch train loss:  0.0016227465821430087\n","Epoch:  7702 , current epoch train loss:  0.001649622805416584\n","Epoch:  7703 , current epoch train loss:  0.0016184308333322406\n","Epoch:  7704 , current epoch train loss:  0.0016147149726748466\n","Epoch:  7705 , current epoch train loss:  0.0016370618250221014\n","Epoch:  7706 , current epoch train loss:  0.0017127811443060637\n","Epoch:  7707 , current epoch train loss:  0.0015945739578455687\n","Epoch:  7708 , current epoch train loss:  0.0015927939675748348\n","Epoch:  7709 , current epoch train loss:  0.0016259474214166403\n","Epoch:  7710 , current epoch train loss:  0.0016416612779721618\n","Epoch:  7711 , current epoch train loss:  0.0016071232967078686\n","Epoch:  7712 , current epoch train loss:  0.0016308818012475967\n","Epoch:  7713 , current epoch train loss:  0.0016502588987350464\n","Epoch:  7714 , current epoch train loss:  0.0017022769898176193\n","Epoch:  7715 , current epoch train loss:  0.001608577324077487\n","Epoch:  7716 , current epoch train loss:  0.001645594253204763\n","Epoch:  7717 , current epoch train loss:  0.0016040893970057368\n","Epoch:  7718 , current epoch train loss:  0.0016494313022121787\n","Epoch:  7719 , current epoch train loss:  0.0016387369250878692\n","Epoch:  7720 , current epoch train loss:  0.0016233643982559443\n","Epoch:  7721 , current epoch train loss:  0.001626904122531414\n","Epoch:  7722 , current epoch train loss:  0.0016686832532286644\n","Epoch:  7723 , current epoch train loss:  0.001655974192544818\n","Epoch:  7724 , current epoch train loss:  0.001582663506269455\n","Epoch:  7725 , current epoch train loss:  0.0017127948813140392\n","Epoch:  7726 , current epoch train loss:  0.001807552995160222\n","Epoch:  7727 , current epoch train loss:  0.0015968331135809422\n","Epoch:  7728 , current epoch train loss:  0.0016029741382226348\n","Epoch:  7729 , current epoch train loss:  0.0016790542285889387\n","Epoch:  7730 , current epoch train loss:  0.0016072512371465564\n","Epoch:  7731 , current epoch train loss:  0.0017616315744817257\n","Epoch:  7732 , current epoch train loss:  0.0016301237046718597\n","Epoch:  7733 , current epoch train loss:  0.0016508823027834296\n","Epoch:  7734 , current epoch train loss:  0.0016530777793377638\n","Epoch:  7735 , current epoch train loss:  0.0017614865209907293\n","Epoch:  7736 , current epoch train loss:  0.0016025679651647806\n","Epoch:  7737 , current epoch train loss:  0.0016258968971669674\n","Epoch:  7738 , current epoch train loss:  0.001609322032891214\n","Epoch:  7739 , current epoch train loss:  0.0016264012083411217\n","Epoch:  7740 , current epoch train loss:  0.0016034410800784826\n","Epoch:  7741 , current epoch train loss:  0.0017120856791734695\n","Epoch:  7742 , current epoch train loss:  0.0016556631308048964\n","Epoch:  7743 , current epoch train loss:  0.0016144359251484275\n","Epoch:  7744 , current epoch train loss:  0.0016020055627450347\n","Epoch:  7745 , current epoch train loss:  0.0015870447969064116\n","Epoch:  7746 , current epoch train loss:  0.0016397361177951097\n","Epoch:  7747 , current epoch train loss:  0.001637415261939168\n","Epoch:  7748 , current epoch train loss:  0.0016114943427965045\n","Epoch:  7749 , current epoch train loss:  0.0016193455085158348\n","Epoch:  7750 , current epoch train loss:  0.001601850031875074\n","Epoch:  7751 , current epoch train loss:  0.001614646054804325\n","Epoch:  7752 , current epoch train loss:  0.0016439197352156043\n","Epoch:  7753 , current epoch train loss:  0.0017059491947293282\n","Epoch:  7754 , current epoch train loss:  0.001625784090720117\n","Epoch:  7755 , current epoch train loss:  0.0015534930862486362\n","Epoch:  7756 , current epoch train loss:  0.0016424022614955902\n","Epoch:  7757 , current epoch train loss:  0.0016060606576502323\n","Epoch:  7758 , current epoch train loss:  0.0016165857668966055\n","Epoch:  7759 , current epoch train loss:  0.0015650566201657057\n","Epoch:  7760 , current epoch train loss:  0.001572206150740385\n","Epoch:  7761 , current epoch train loss:  0.0016422815388068557\n","Epoch:  7762 , current epoch train loss:  0.0016157826175913215\n","Epoch:  7763 , current epoch train loss:  0.0015766983851790428\n","Epoch:  7764 , current epoch train loss:  0.001740241190418601\n","Epoch:  7765 , current epoch train loss:  0.0016473112627863884\n","Epoch:  7766 , current epoch train loss:  0.0016022701747715473\n","Epoch:  7767 , current epoch train loss:  0.0016724930610507727\n","Epoch:  7768 , current epoch train loss:  0.001665385439991951\n","Epoch:  7769 , current epoch train loss:  0.0019098510965704918\n","Epoch:  7770 , current epoch train loss:  0.0016096706967800856\n","Epoch:  7771 , current epoch train loss:  0.0016234320355579257\n","Epoch:  7772 , current epoch train loss:  0.0016707750037312508\n","Epoch:  7773 , current epoch train loss:  0.0017400061478838325\n","Epoch:  7774 , current epoch train loss:  0.0016266764141619205\n","Epoch:  7775 , current epoch train loss:  0.0016614309279248118\n","Epoch:  7776 , current epoch train loss:  0.0016169669106602669\n","Epoch:  7777 , current epoch train loss:  0.001702379435300827\n","Epoch:  7778 , current epoch train loss:  0.0017384481616318226\n","Epoch:  7779 , current epoch train loss:  0.0016189911402761936\n","Epoch:  7780 , current epoch train loss:  0.001661867368966341\n","Epoch:  7781 , current epoch train loss:  0.0016167943831533194\n","Epoch:  7782 , current epoch train loss:  0.001656451029703021\n","Epoch:  7783 , current epoch train loss:  0.0017440997762605548\n","Epoch:  7784 , current epoch train loss:  0.001690685166977346\n","Epoch:  7785 , current epoch train loss:  0.0016591030871495605\n","Epoch:  7786 , current epoch train loss:  0.001637256471440196\n","Epoch:  7787 , current epoch train loss:  0.0016594737535342574\n","Epoch:  7788 , current epoch train loss:  0.0017315888544544578\n","Epoch:  7789 , current epoch train loss:  0.001640930655412376\n","Epoch:  7790 , current epoch train loss:  0.0016127591952681541\n","Epoch:  7791 , current epoch train loss:  0.0016103772213682532\n","Epoch:  7792 , current epoch train loss:  0.0016924975207075477\n","Epoch:  7793 , current epoch train loss:  0.001639060559682548\n","Epoch:  7794 , current epoch train loss:  0.0016433631535619497\n","Epoch:  7795 , current epoch train loss:  0.0016558726783841848\n","Epoch:  7796 , current epoch train loss:  0.0016220330726355314\n","Epoch:  7797 , current epoch train loss:  0.001614576205611229\n","Epoch:  7798 , current epoch train loss:  0.0016682632267475128\n","Epoch:  7799 , current epoch train loss:  0.0016532859299331903\n","Epoch:  7800 , current epoch train loss:  0.0016516301548108459\n","Epoch:  7801 , current epoch train loss:  0.001625404111109674\n","Epoch:  7802 , current epoch train loss:  0.0016619383823126554\n","Epoch:  7803 , current epoch train loss:  0.0017405267572030425\n","Epoch:  7804 , current epoch train loss:  0.0016352365491911769\n","Epoch:  7805 , current epoch train loss:  0.0015961905010044575\n","Epoch:  7806 , current epoch train loss:  0.0016067152610048652\n","Epoch:  7807 , current epoch train loss:  0.0017165974713861942\n","Epoch:  7808 , current epoch train loss:  0.001634952612221241\n","Epoch:  7809 , current epoch train loss:  0.0016056963941082358\n","Epoch:  7810 , current epoch train loss:  0.0016603299882262945\n","Epoch:  7811 , current epoch train loss:  0.0015804963186383247\n","Epoch:  7812 , current epoch train loss:  0.0016619464149698615\n","Epoch:  7813 , current epoch train loss:  0.0016176113858819008\n","Epoch:  7814 , current epoch train loss:  0.00161194137763232\n","Epoch:  7815 , current epoch train loss:  0.0015727458521723747\n","Epoch:  7816 , current epoch train loss:  0.001571335713379085\n","Epoch:  7817 , current epoch train loss:  0.001663329778239131\n","Epoch:  7818 , current epoch train loss:  0.0016522395890206099\n","Epoch:  7819 , current epoch train loss:  0.0016690402990207076\n","Epoch:  7820 , current epoch train loss:  0.0015961406752467155\n","Epoch:  7821 , current epoch train loss:  0.0016212286427617073\n","Epoch:  7822 , current epoch train loss:  0.0016278823604807258\n","Epoch:  7823 , current epoch train loss:  0.0015706708654761314\n","Epoch:  7824 , current epoch train loss:  0.0015983900520950556\n","Epoch:  7825 , current epoch train loss:  0.0017143625300377607\n","Epoch:  7826 , current epoch train loss:  0.001604794873856008\n","Epoch:  7827 , current epoch train loss:  0.0015673486050218344\n","Epoch:  7828 , current epoch train loss:  0.0015871664509177208\n","Epoch:  7829 , current epoch train loss:  0.0016019826289266348\n","Epoch:  7830 , current epoch train loss:  0.001614545239135623\n","Epoch:  7831 , current epoch train loss:  0.001621445408090949\n","Epoch:  7832 , current epoch train loss:  0.0015940646408125758\n","Epoch:  7833 , current epoch train loss:  0.0015863374574109912\n","Epoch:  7834 , current epoch train loss:  0.0016573922475799918\n","Epoch:  7835 , current epoch train loss:  0.0016361565794795752\n","Epoch:  7836 , current epoch train loss:  0.0016274964436888695\n","Epoch:  7837 , current epoch train loss:  0.001587370177730918\n","Epoch:  7838 , current epoch train loss:  0.0016227004816755652\n","Epoch:  7839 , current epoch train loss:  0.001734744175337255\n","Epoch:  7840 , current epoch train loss:  0.001678269705735147\n","Epoch:  7841 , current epoch train loss:  0.0016453282441943884\n","Epoch:  7842 , current epoch train loss:  0.0017043406842276454\n","Epoch:  7843 , current epoch train loss:  0.0016217108350247145\n","Epoch:  7844 , current epoch train loss:  0.0016379309818148613\n","Epoch:  7845 , current epoch train loss:  0.0015998294111341238\n","Epoch:  7846 , current epoch train loss:  0.0016023083589971066\n","Epoch:  7847 , current epoch train loss:  0.0016207951121032238\n","Epoch:  7848 , current epoch train loss:  0.0016767991473898292\n","Epoch:  7849 , current epoch train loss:  0.0016245300648733974\n","Epoch:  7850 , current epoch train loss:  0.0016656349180266261\n","Epoch:  7851 , current epoch train loss:  0.0016970656579360366\n","Epoch:  7852 , current epoch train loss:  0.0016260979464277625\n","Epoch:  7853 , current epoch train loss:  0.0015851551434025168\n","Epoch:  7854 , current epoch train loss:  0.0016249362379312515\n","Epoch:  7855 , current epoch train loss:  0.001633957028388977\n","Epoch:  7856 , current epoch train loss:  0.0015740630915388465\n","Epoch:  7857 , current epoch train loss:  0.0016300975112244487\n","Epoch:  7858 , current epoch train loss:  0.0016240584664046764\n","Epoch:  7859 , current epoch train loss:  0.0016651262994855642\n","Epoch:  7860 , current epoch train loss:  0.0017012280877679586\n","Epoch:  7861 , current epoch train loss:  0.0016900368500500917\n","Epoch:  7862 , current epoch train loss:  0.0016236088704317808\n","Epoch:  7863 , current epoch train loss:  0.0015706182457506657\n","Epoch:  7864 , current epoch train loss:  0.0016501559875905514\n","Epoch:  7865 , current epoch train loss:  0.0016858878079801798\n","Epoch:  7866 , current epoch train loss:  0.0016112010926008224\n","Epoch:  7867 , current epoch train loss:  0.001701791537925601\n","Epoch:  7868 , current epoch train loss:  0.0016085931565612555\n","Epoch:  7869 , current epoch train loss:  0.0016043133800849319\n","Epoch:  7870 , current epoch train loss:  0.0015972089022397995\n","Epoch:  7871 , current epoch train loss:  0.0017032015603035688\n","Epoch:  7872 , current epoch train loss:  0.0015890974318608642\n","Epoch:  7873 , current epoch train loss:  0.0016383274924010038\n","Epoch:  7874 , current epoch train loss:  0.001582038588821888\n","Epoch:  7875 , current epoch train loss:  0.001681098947301507\n","Epoch:  7876 , current epoch train loss:  0.0016433985438197851\n","Epoch:  7877 , current epoch train loss:  0.0016557651106268167\n","Epoch:  7878 , current epoch train loss:  0.0015863494481891394\n","Epoch:  7879 , current epoch train loss:  0.001592516782693565\n","Epoch:  7880 , current epoch train loss:  0.0016021020710468292\n","Epoch:  7881 , current epoch train loss:  0.001574410474859178\n","Epoch:  7882 , current epoch train loss:  0.001620220486074686\n","Epoch:  7883 , current epoch train loss:  0.0016221471596509218\n","Epoch:  7884 , current epoch train loss:  0.0016411906108260155\n","Epoch:  7885 , current epoch train loss:  0.0016227653250098228\n","Epoch:  7886 , current epoch train loss:  0.0016402441542595625\n","Epoch:  7887 , current epoch train loss:  0.0015632169088348746\n","Epoch:  7888 , current epoch train loss:  0.0016664060531184077\n","Epoch:  7889 , current epoch train loss:  0.0015646619722247124\n","Epoch:  7890 , current epoch train loss:  0.0016224365681409836\n","Epoch:  7891 , current epoch train loss:  0.0017273130360990763\n","Epoch:  7892 , current epoch train loss:  0.001737236394546926\n","Epoch:  7893 , current epoch train loss:  0.0016704434528946877\n","Epoch:  7894 , current epoch train loss:  0.0016198892844840884\n","Epoch:  7895 , current epoch train loss:  0.0016656266525387764\n","Epoch:  7896 , current epoch train loss:  0.0017052041366696358\n","Epoch:  7897 , current epoch train loss:  0.0016209816094487906\n","Epoch:  7898 , current epoch train loss:  0.0015373367350548506\n","Epoch:  7899 , current epoch train loss:  0.001617272850126028\n","Epoch:  7900 , current epoch train loss:  0.0015732890460640192\n","Epoch:  7901 , current epoch train loss:  0.0016916956519708037\n","Epoch:  7902 , current epoch train loss:  0.001587719889357686\n","Epoch:  7903 , current epoch train loss:  0.001627454999834299\n","Epoch:  7904 , current epoch train loss:  0.0017604989698156714\n","Epoch:  7905 , current epoch train loss:  0.0015917501877993345\n","Epoch:  7906 , current epoch train loss:  0.001667834585532546\n","Epoch:  7907 , current epoch train loss:  0.0017879464430734515\n","Epoch:  7908 , current epoch train loss:  0.0017029168084263802\n","Epoch:  7909 , current epoch train loss:  0.0016717601101845503\n","Epoch:  7910 , current epoch train loss:  0.0016201952239498496\n","Epoch:  7911 , current epoch train loss:  0.0015851747011765838\n","Epoch:  7912 , current epoch train loss:  0.0016628786688670516\n","Epoch:  7913 , current epoch train loss:  0.0016257576644420624\n","Epoch:  7914 , current epoch train loss:  0.00160245131701231\n","Epoch:  7915 , current epoch train loss:  0.0015643634833395481\n","Epoch:  7916 , current epoch train loss:  0.0016721521969884634\n","Epoch:  7917 , current epoch train loss:  0.0016186239663511515\n","Epoch:  7918 , current epoch train loss:  0.0016949726268649101\n","Epoch:  7919 , current epoch train loss:  0.001599196344614029\n","Epoch:  7920 , current epoch train loss:  0.0016701726708561182\n","Epoch:  7921 , current epoch train loss:  0.0016232070047408342\n","Epoch:  7922 , current epoch train loss:  0.0016384361078962684\n","Epoch:  7923 , current epoch train loss:  0.0016563711687922478\n","Epoch:  7924 , current epoch train loss:  0.0015563133638352156\n","Epoch:  7925 , current epoch train loss:  0.00157077272888273\n","Epoch:  7926 , current epoch train loss:  0.0016224812716245651\n","Epoch:  7927 , current epoch train loss:  0.0017288944218307734\n","Epoch:  7928 , current epoch train loss:  0.0016238519456237555\n","Epoch:  7929 , current epoch train loss:  0.0015621604397892952\n","Epoch:  7930 , current epoch train loss:  0.0016273324145004153\n","Epoch:  7931 , current epoch train loss:  0.0016115423059090972\n","Epoch:  7932 , current epoch train loss:  0.0016228442545980215\n","Epoch:  7933 , current epoch train loss:  0.0016141043743118644\n","Epoch:  7934 , current epoch train loss:  0.0016017657471820712\n","Epoch:  7935 , current epoch train loss:  0.001675306586548686\n","Epoch:  7936 , current epoch train loss:  0.0016601545503363013\n","Epoch:  7937 , current epoch train loss:  0.0015855047386139631\n","Epoch:  7938 , current epoch train loss:  0.001603563199751079\n","Epoch:  7939 , current epoch train loss:  0.0016460223123431206\n","Epoch:  7940 , current epoch train loss:  0.0015871841460466385\n","Epoch:  7941 , current epoch train loss:  0.001612301915884018\n","Epoch:  7942 , current epoch train loss:  0.001638803631067276\n","Epoch:  7943 , current epoch train loss:  0.0016129461582750082\n","Epoch:  7944 , current epoch train loss:  0.0016474088188260794\n","Epoch:  7945 , current epoch train loss:  0.0016785853076726198\n","Epoch:  7946 , current epoch train loss:  0.0017369058914482594\n","Epoch:  7947 , current epoch train loss:  0.0016399807063862681\n","Epoch:  7948 , current epoch train loss:  0.0016263684956356883\n","Epoch:  7949 , current epoch train loss:  0.0016104624373838305\n","Epoch:  7950 , current epoch train loss:  0.0016374047845602036\n","Epoch:  7951 , current epoch train loss:  0.0018285977421328425\n","Epoch:  7952 , current epoch train loss:  0.001571286702528596\n","Epoch:  7953 , current epoch train loss:  0.0016416527796536684\n","Epoch:  7954 , current epoch train loss:  0.0016127018025144935\n","Epoch:  7955 , current epoch train loss:  0.001606030622497201\n","Epoch:  7956 , current epoch train loss:  0.0015980179887264967\n","Epoch:  7957 , current epoch train loss:  0.001595757668837905\n","Epoch:  7958 , current epoch train loss:  0.0016242584679275751\n","Epoch:  7959 , current epoch train loss:  0.0016302969306707382\n","Epoch:  7960 , current epoch train loss:  0.001617207657545805\n","Epoch:  7961 , current epoch train loss:  0.0016369323711842299\n","Epoch:  7962 , current epoch train loss:  0.0016133006429299712\n","Epoch:  7963 , current epoch train loss:  0.0015695728361606598\n","Epoch:  7964 , current epoch train loss:  0.001610923558473587\n","Epoch:  7965 , current epoch train loss:  0.0016918012406677008\n","Epoch:  7966 , current epoch train loss:  0.00167867261916399\n","Epoch:  7967 , current epoch train loss:  0.001649029552936554\n","Epoch:  7968 , current epoch train loss:  0.0015993285924196243\n","Epoch:  7969 , current epoch train loss:  0.0016019577160477638\n","Epoch:  7970 , current epoch train loss:  0.0016410833923146129\n","Epoch:  7971 , current epoch train loss:  0.0016038059256970882\n","Epoch:  7972 , current epoch train loss:  0.0015883826417848468\n","Epoch:  7973 , current epoch train loss:  0.0017182326409965754\n","Epoch:  7974 , current epoch train loss:  0.0016502387588843703\n","Epoch:  7975 , current epoch train loss:  0.0016599975060671568\n","Epoch:  7976 , current epoch train loss:  0.0016064532101154327\n","Epoch:  7977 , current epoch train loss:  0.0015659768832847476\n","Epoch:  7978 , current epoch train loss:  0.001601282972842455\n","Epoch:  7979 , current epoch train loss:  0.0016812196699902415\n","Epoch:  7980 , current epoch train loss:  0.0016731199575588107\n","Epoch:  7981 , current epoch train loss:  0.0015976575668901205\n","Epoch:  7982 , current epoch train loss:  0.0016390595119446516\n","Epoch:  7983 , current epoch train loss:  0.0015805650036782026\n","Epoch:  7984 , current epoch train loss:  0.0016130910953506827\n","Epoch:  7985 , current epoch train loss:  0.001641933573409915\n","Epoch:  7986 , current epoch train loss:  0.0017429213039577007\n","Epoch:  7987 , current epoch train loss:  0.001605291268788278\n","Epoch:  7988 , current epoch train loss:  0.0016176834469661117\n","Epoch:  7989 , current epoch train loss:  0.001625093980692327\n","Epoch:  7990 , current epoch train loss:  0.0016045933589339256\n","Epoch:  7991 , current epoch train loss:  0.001561234937980771\n","Epoch:  7992 , current epoch train loss:  0.001606622477993369\n","Epoch:  7993 , current epoch train loss:  0.0016074375016614795\n","Epoch:  7994 , current epoch train loss:  0.001574573921971023\n","Epoch:  7995 , current epoch train loss:  0.001618859707377851\n","Epoch:  7996 , current epoch train loss:  0.0015755207277834415\n","Epoch:  7997 , current epoch train loss:  0.00157412129919976\n","Epoch:  7998 , current epoch train loss:  0.0015950154047459364\n","Epoch:  7999 , current epoch train loss:  0.0015742543619126081\n","Epoch:  8000 , current epoch train loss:  0.0015947148203849792\n","Epoch:  8001 , current epoch train loss:  0.0016105398535728455\n","Epoch:  8002 , current epoch train loss:  0.001616865280084312\n","Epoch:  8003 , current epoch train loss:  0.0016988958232104778\n","Epoch:  8004 , current epoch train loss:  0.0015864711022004485\n","Epoch:  8005 , current epoch train loss:  0.0015873422380536795\n","Epoch:  8006 , current epoch train loss:  0.0016271383501589298\n","Epoch:  8007 , current epoch train loss:  0.00164519133977592\n","Epoch:  8008 , current epoch train loss:  0.0016268905019387603\n","Epoch:  8009 , current epoch train loss:  0.0017849961295723915\n","Epoch:  8010 , current epoch train loss:  0.0016452418640255928\n","Epoch:  8011 , current epoch train loss:  0.00161397410556674\n","Epoch:  8012 , current epoch train loss:  0.0015797971282154322\n","Epoch:  8013 , current epoch train loss:  0.001624909695237875\n","Epoch:  8014 , current epoch train loss:  0.001696762046776712\n","Epoch:  8015 , current epoch train loss:  0.0016504270024597645\n","Epoch:  8016 , current epoch train loss:  0.0016186180291697383\n","Epoch:  8017 , current epoch train loss:  0.0017040614038705826\n","Epoch:  8018 , current epoch train loss:  0.0016253727953881025\n","Epoch:  8019 , current epoch train loss:  0.0016432249685749412\n","Epoch:  8020 , current epoch train loss:  0.0015896542463451624\n","Epoch:  8021 , current epoch train loss:  0.0017049129819497466\n","Epoch:  8022 , current epoch train loss:  0.001598277478478849\n","Epoch:  8023 , current epoch train loss:  0.0016134059987962246\n","Epoch:  8024 , current epoch train loss:  0.0016325456090271473\n","Epoch:  8025 , current epoch train loss:  0.001603012322448194\n","Epoch:  8026 , current epoch train loss:  0.0015967388171702623\n","Epoch:  8027 , current epoch train loss:  0.0016512995352968574\n","Epoch:  8028 , current epoch train loss:  0.0017445174744352698\n","Epoch:  8029 , current epoch train loss:  0.0015806888695806265\n","Epoch:  8030 , current epoch train loss:  0.001575665082782507\n","Epoch:  8031 , current epoch train loss:  0.0015740706585347652\n","Epoch:  8032 , current epoch train loss:  0.001632011029869318\n","Epoch:  8033 , current epoch train loss:  0.0016001013573259115\n","Epoch:  8034 , current epoch train loss:  0.0015861255815252662\n","Epoch:  8035 , current epoch train loss:  0.0016265077283605933\n","Epoch:  8036 , current epoch train loss:  0.0016042202478274703\n","Epoch:  8037 , current epoch train loss:  0.0017484119161963463\n","Epoch:  8038 , current epoch train loss:  0.0016276149544864893\n","Epoch:  8039 , current epoch train loss:  0.0015460647409781814\n","Epoch:  8040 , current epoch train loss:  0.001730336807668209\n","Epoch:  8041 , current epoch train loss:  0.0015874998643994331\n","Epoch:  8042 , current epoch train loss:  0.0015951994573697448\n","Epoch:  8043 , current epoch train loss:  0.0015828507021069527\n","Epoch:  8044 , current epoch train loss:  0.0016203438863158226\n","Epoch:  8045 , current epoch train loss:  0.0016412499826401472\n","Epoch:  8046 , current epoch train loss:  0.0015980424359440804\n","Epoch:  8047 , current epoch train loss:  0.0016589872539043427\n","Epoch:  8048 , current epoch train loss:  0.0016365716001018882\n","Epoch:  8049 , current epoch train loss:  0.00167025753762573\n","Epoch:  8050 , current epoch train loss:  0.0016741681611165404\n","Epoch:  8051 , current epoch train loss:  0.0017053298652172089\n","Epoch:  8052 , current epoch train loss:  0.0016231824411079288\n","Epoch:  8053 , current epoch train loss:  0.001580632757395506\n","Epoch:  8054 , current epoch train loss:  0.001653647399507463\n","Epoch:  8055 , current epoch train loss:  0.0015972781693562865\n","Epoch:  8056 , current epoch train loss:  0.0016366532072424889\n","Epoch:  8057 , current epoch train loss:  0.0017303917557001114\n","Epoch:  8058 , current epoch train loss:  0.0016666705487295985\n","Epoch:  8059 , current epoch train loss:  0.001642546965740621\n","Epoch:  8060 , current epoch train loss:  0.0015934252878651023\n","Epoch:  8061 , current epoch train loss:  0.00165538617875427\n","Epoch:  8062 , current epoch train loss:  0.0016203392297029495\n","Epoch:  8063 , current epoch train loss:  0.00164631602820009\n","Epoch:  8064 , current epoch train loss:  0.0016716233221814036\n","Epoch:  8065 , current epoch train loss:  0.0015500764129683375\n","Epoch:  8066 , current epoch train loss:  0.0016845788341015577\n","Epoch:  8067 , current epoch train loss:  0.0016253761714324355\n","Epoch:  8068 , current epoch train loss:  0.0016195293283089995\n","Epoch:  8069 , current epoch train loss:  0.001575008500367403\n","Epoch:  8070 , current epoch train loss:  0.0016194181516766548\n","Epoch:  8071 , current epoch train loss:  0.0015667169354856014\n","Epoch:  8072 , current epoch train loss:  0.001601172611117363\n","Epoch:  8073 , current epoch train loss:  0.0016426255460828543\n","Epoch:  8074 , current epoch train loss:  0.0016310460632666945\n","Epoch:  8075 , current epoch train loss:  0.0015574491117149591\n","Epoch:  8076 , current epoch train loss:  0.0016167676076292992\n","Epoch:  8077 , current epoch train loss:  0.0016421498730778694\n","Epoch:  8078 , current epoch train loss:  0.001586277037858963\n","Epoch:  8079 , current epoch train loss:  0.0016095623141154647\n","Epoch:  8080 , current epoch train loss:  0.0015931882662698627\n","Epoch:  8081 , current epoch train loss:  0.0016242088750004768\n","Epoch:  8082 , current epoch train loss:  0.0016603933181613684\n","Epoch:  8083 , current epoch train loss:  0.001587136648595333\n","Epoch:  8084 , current epoch train loss:  0.0016112967859953642\n","Epoch:  8085 , current epoch train loss:  0.0015768853481858969\n","Epoch:  8086 , current epoch train loss:  0.0015984022757038474\n","Epoch:  8087 , current epoch train loss:  0.0016262246062979102\n","Epoch:  8088 , current epoch train loss:  0.001611315063200891\n","Epoch:  8089 , current epoch train loss:  0.0016469400143250823\n","Epoch:  8090 , current epoch train loss:  0.0015735261840745807\n","Epoch:  8091 , current epoch train loss:  0.0018144053174182773\n","Epoch:  8092 , current epoch train loss:  0.0015658451011404395\n","Epoch:  8093 , current epoch train loss:  0.0016629656311124563\n","Epoch:  8094 , current epoch train loss:  0.0016171340830624104\n","Epoch:  8095 , current epoch train loss:  0.0016934301238507032\n","Epoch:  8096 , current epoch train loss:  0.0016124641988426447\n","Epoch:  8097 , current epoch train loss:  0.0015983730554580688\n","Epoch:  8098 , current epoch train loss:  0.0015785186551511288\n","Epoch:  8099 , current epoch train loss:  0.001654636929742992\n","Epoch:  8100 , current epoch train loss:  0.0016028303653001785\n","Epoch:  8101 , current epoch train loss:  0.0016315530519932508\n","Epoch:  8102 , current epoch train loss:  0.0016773914685472846\n","Epoch:  8103 , current epoch train loss:  0.0016029708785936236\n","Epoch:  8104 , current epoch train loss:  0.0015638109762221575\n","Epoch:  8105 , current epoch train loss:  0.0016351047670468688\n","Epoch:  8106 , current epoch train loss:  0.001668514683842659\n","Epoch:  8107 , current epoch train loss:  0.001644733245484531\n","Epoch:  8108 , current epoch train loss:  0.0015776738291606307\n","Epoch:  8109 , current epoch train loss:  0.001605459488928318\n","Epoch:  8110 , current epoch train loss:  0.0015876346733421087\n","Epoch:  8111 , current epoch train loss:  0.0015972403343766928\n","Epoch:  8112 , current epoch train loss:  0.0016087613767012954\n","Epoch:  8113 , current epoch train loss:  0.0016303674783557653\n","Epoch:  8114 , current epoch train loss:  0.0016498285112902522\n","Epoch:  8115 , current epoch train loss:  0.0015668397536501288\n","Epoch:  8116 , current epoch train loss:  0.0017848301213234663\n","Epoch:  8117 , current epoch train loss:  0.0016384300542995334\n","Epoch:  8118 , current epoch train loss:  0.0015952622052282095\n","Epoch:  8119 , current epoch train loss:  0.001646106829866767\n","Epoch:  8120 , current epoch train loss:  0.0016058736946433783\n","Epoch:  8121 , current epoch train loss:  0.00159376859664917\n","Epoch:  8122 , current epoch train loss:  0.0016533785965293646\n","Epoch:  8123 , current epoch train loss:  0.0015685282414779067\n","Epoch:  8124 , current epoch train loss:  0.0016199618112295866\n","Epoch:  8125 , current epoch train loss:  0.0016028776299208403\n","Epoch:  8126 , current epoch train loss:  0.0016056681051850319\n","Epoch:  8127 , current epoch train loss:  0.0016780066071078181\n","Epoch:  8128 , current epoch train loss:  0.0016312141669914126\n","Epoch:  8129 , current epoch train loss:  0.001609854749403894\n","Epoch:  8130 , current epoch train loss:  0.001645426033064723\n","Epoch:  8131 , current epoch train loss:  0.0015872681979089975\n","Epoch:  8132 , current epoch train loss:  0.0015934398397803307\n","Epoch:  8133 , current epoch train loss:  0.00158548797480762\n","Epoch:  8134 , current epoch train loss:  0.001562574878334999\n","Epoch:  8135 , current epoch train loss:  0.0016319381538778543\n","Epoch:  8136 , current epoch train loss:  0.0015859953127801418\n","Epoch:  8137 , current epoch train loss:  0.0016208788147196174\n","Epoch:  8138 , current epoch train loss:  0.0015855219680815935\n","Epoch:  8139 , current epoch train loss:  0.0016147636342793703\n","Epoch:  8140 , current epoch train loss:  0.0015814577927812934\n","Epoch:  8141 , current epoch train loss:  0.0015605853404849768\n","Epoch:  8142 , current epoch train loss:  0.0015849447809159756\n","Epoch:  8143 , current epoch train loss:  0.0016415368299931288\n","Epoch:  8144 , current epoch train loss:  0.0016490272246301174\n","Epoch:  8145 , current epoch train loss:  0.0015895683318376541\n","Epoch:  8146 , current epoch train loss:  0.0015838902909308672\n","Epoch:  8147 , current epoch train loss:  0.0015998732997104526\n","Epoch:  8148 , current epoch train loss:  0.0015637980541214347\n","Epoch:  8149 , current epoch train loss:  0.0015866225585341454\n","Epoch:  8150 , current epoch train loss:  0.0015275863697752357\n","Epoch:  8151 , current epoch train loss:  0.0016183233819901943\n","Epoch:  8152 , current epoch train loss:  0.0016201221151277423\n","Epoch:  8153 , current epoch train loss:  0.0016076372703537345\n","Epoch:  8154 , current epoch train loss:  0.0015890778740867972\n","Epoch:  8155 , current epoch train loss:  0.0015645651146769524\n","Epoch:  8156 , current epoch train loss:  0.0016534561291337013\n","Epoch:  8157 , current epoch train loss:  0.0016050117556005716\n","Epoch:  8158 , current epoch train loss:  0.0015679288189858198\n","Epoch:  8159 , current epoch train loss:  0.001560457400046289\n","Epoch:  8160 , current epoch train loss:  0.0015978578012436628\n","Epoch:  8161 , current epoch train loss:  0.001624896191060543\n","Epoch:  8162 , current epoch train loss:  0.0015729926526546478\n","Epoch:  8163 , current epoch train loss:  0.001596708782017231\n","Epoch:  8164 , current epoch train loss:  0.0015779318055137992\n","Epoch:  8165 , current epoch train loss:  0.0016410073731094599\n","Epoch:  8166 , current epoch train loss:  0.0016033404972404242\n","Epoch:  8167 , current epoch train loss:  0.0016208251472562551\n","Epoch:  8168 , current epoch train loss:  0.0016282147262245417\n","Epoch:  8169 , current epoch train loss:  0.0016473438590765\n","Epoch:  8170 , current epoch train loss:  0.0015868194168433547\n","Epoch:  8171 , current epoch train loss:  0.0015668254345655441\n","Epoch:  8172 , current epoch train loss:  0.0015886398032307625\n","Epoch:  8173 , current epoch train loss:  0.001779463142156601\n","Epoch:  8174 , current epoch train loss:  0.001608230173587799\n","Epoch:  8175 , current epoch train loss:  0.0016661820700392127\n","Epoch:  8176 , current epoch train loss:  0.0015739218797534704\n","Epoch:  8177 , current epoch train loss:  0.0015961052849888802\n","Epoch:  8178 , current epoch train loss:  0.001576557056978345\n","Epoch:  8179 , current epoch train loss:  0.0015732621541246772\n","Epoch:  8180 , current epoch train loss:  0.0016246337909251451\n","Epoch:  8181 , current epoch train loss:  0.0016613395418971777\n","Epoch:  8182 , current epoch train loss:  0.0015840766718611121\n","Epoch:  8183 , current epoch train loss:  0.0016131469747051597\n","Epoch:  8184 , current epoch train loss:  0.0015391986817121506\n","Epoch:  8185 , current epoch train loss:  0.001646437682211399\n","Epoch:  8186 , current epoch train loss:  0.001607238664291799\n","Epoch:  8187 , current epoch train loss:  0.0016426090151071548\n","Epoch:  8188 , current epoch train loss:  0.001633412903174758\n","Epoch:  8189 , current epoch train loss:  0.001705532893538475\n","Epoch:  8190 , current epoch train loss:  0.001566244289278984\n","Epoch:  8191 , current epoch train loss:  0.001643905881792307\n","Epoch:  8192 , current epoch train loss:  0.0016567865386605263\n","Epoch:  8193 , current epoch train loss:  0.0016025900840759277\n","Epoch:  8194 , current epoch train loss:  0.0015907561173662543\n","Epoch:  8195 , current epoch train loss:  0.001638368470594287\n","Epoch:  8196 , current epoch train loss:  0.0016208095476031303\n","Epoch:  8197 , current epoch train loss:  0.0016171480529010296\n","Epoch:  8198 , current epoch train loss:  0.0015723234973847866\n","Epoch:  8199 , current epoch train loss:  0.0016400530003011227\n","Epoch:  8200 , current epoch train loss:  0.0016208894085139036\n","Epoch:  8201 , current epoch train loss:  0.0016671465709805489\n","Epoch:  8202 , current epoch train loss:  0.0016584586119279265\n","Epoch:  8203 , current epoch train loss:  0.00163162755779922\n","Epoch:  8204 , current epoch train loss:  0.0015826951712369919\n","Epoch:  8205 , current epoch train loss:  0.0016129044815897942\n","Epoch:  8206 , current epoch train loss:  0.001619481947273016\n","Epoch:  8207 , current epoch train loss:  0.0015885712346062064\n","Epoch:  8208 , current epoch train loss:  0.0016498167533427477\n","Epoch:  8209 , current epoch train loss:  0.001748611219227314\n","Epoch:  8210 , current epoch train loss:  0.001603549113497138\n","Epoch:  8211 , current epoch train loss:  0.0015835369704291224\n","Epoch:  8212 , current epoch train loss:  0.001557490206323564\n","Epoch:  8213 , current epoch train loss:  0.0016303047304973006\n","Epoch:  8214 , current epoch train loss:  0.001585975056514144\n","Epoch:  8215 , current epoch train loss:  0.0015901781152933836\n","Epoch:  8216 , current epoch train loss:  0.00174532865639776\n","Epoch:  8217 , current epoch train loss:  0.0015876709949225187\n","Epoch:  8218 , current epoch train loss:  0.0016137973871082067\n","Epoch:  8219 , current epoch train loss:  0.0017050325404852629\n","Epoch:  8220 , current epoch train loss:  0.0015821519773453474\n","Epoch:  8221 , current epoch train loss:  0.0015627419343218207\n","Epoch:  8222 , current epoch train loss:  0.0015820113476365805\n","Epoch:  8223 , current epoch train loss:  0.0015436284011229873\n","Epoch:  8224 , current epoch train loss:  0.001533285598270595\n","Epoch:  8225 , current epoch train loss:  0.0016577269416302443\n","Epoch:  8226 , current epoch train loss:  0.001594635541550815\n","Epoch:  8227 , current epoch train loss:  0.0015442415606230497\n","Epoch:  8228 , current epoch train loss:  0.001578441122546792\n","Epoch:  8229 , current epoch train loss:  0.0017147486796602607\n","Epoch:  8230 , current epoch train loss:  0.0015689115971326828\n","Epoch:  8231 , current epoch train loss:  0.0016387605573982\n","Epoch:  8232 , current epoch train loss:  0.0017812836449593306\n","Epoch:  8233 , current epoch train loss:  0.0016815564595162868\n","Epoch:  8234 , current epoch train loss:  0.0015753642655909061\n","Epoch:  8235 , current epoch train loss:  0.0015924712643027306\n","Epoch:  8236 , current epoch train loss:  0.0016010062536224723\n","Epoch:  8237 , current epoch train loss:  0.0016386331990361214\n","Epoch:  8238 , current epoch train loss:  0.0016056476160883904\n","Epoch:  8239 , current epoch train loss:  0.001581239397637546\n","Epoch:  8240 , current epoch train loss:  0.0016513976734131575\n","Epoch:  8241 , current epoch train loss:  0.001596818445250392\n","Epoch:  8242 , current epoch train loss:  0.0016238633543252945\n","Epoch:  8243 , current epoch train loss:  0.0016169426962733269\n","Epoch:  8244 , current epoch train loss:  0.001607229351066053\n","Epoch:  8245 , current epoch train loss:  0.0016613553743809462\n","Epoch:  8246 , current epoch train loss:  0.0016189388697966933\n","Epoch:  8247 , current epoch train loss:  0.0016593949403613806\n","Epoch:  8248 , current epoch train loss:  0.0016756851691752672\n","Epoch:  8249 , current epoch train loss:  0.0015837687533348799\n","Epoch:  8250 , current epoch train loss:  0.0016074123559519649\n","Epoch:  8251 , current epoch train loss:  0.0016347856726497412\n","Epoch:  8252 , current epoch train loss:  0.0016269597690552473\n","Epoch:  8253 , current epoch train loss:  0.0015685707330703735\n","Epoch:  8254 , current epoch train loss:  0.001644477597437799\n","Epoch:  8255 , current epoch train loss:  0.0016288993647322059\n","Epoch:  8256 , current epoch train loss:  0.0016032768180593848\n","Epoch:  8257 , current epoch train loss:  0.001608441467396915\n","Epoch:  8258 , current epoch train loss:  0.0016061555361375213\n","Epoch:  8259 , current epoch train loss:  0.001810009009204805\n","Epoch:  8260 , current epoch train loss:  0.0016809775261208415\n","Epoch:  8261 , current epoch train loss:  0.0015773510094732046\n","Epoch:  8262 , current epoch train loss:  0.001647463534027338\n","Epoch:  8263 , current epoch train loss:  0.0015770180616527796\n","Epoch:  8264 , current epoch train loss:  0.0015864737797528505\n","Epoch:  8265 , current epoch train loss:  0.0016219213139265776\n","Epoch:  8266 , current epoch train loss:  0.0016173978801816702\n","Epoch:  8267 , current epoch train loss:  0.001674633240327239\n","Epoch:  8268 , current epoch train loss:  0.0016119952779263258\n","Epoch:  8269 , current epoch train loss:  0.001576907467097044\n","Epoch:  8270 , current epoch train loss:  0.0015901685692369938\n","Epoch:  8271 , current epoch train loss:  0.0016071069985628128\n","Epoch:  8272 , current epoch train loss:  0.0016586408019065857\n","Epoch:  8273 , current epoch train loss:  0.001615010085515678\n","Epoch:  8274 , current epoch train loss:  0.0015943758189678192\n","Epoch:  8275 , current epoch train loss:  0.0016356362029910088\n","Epoch:  8276 , current epoch train loss:  0.001627219608053565\n","Epoch:  8277 , current epoch train loss:  0.0015992610715329647\n","Epoch:  8278 , current epoch train loss:  0.001643453142605722\n","Epoch:  8279 , current epoch train loss:  0.0016069754492491484\n","Epoch:  8280 , current epoch train loss:  0.0015802579000592232\n","Epoch:  8281 , current epoch train loss:  0.0015635512536391616\n","Epoch:  8282 , current epoch train loss:  0.0015857932157814503\n","Epoch:  8283 , current epoch train loss:  0.001573739806190133\n","Epoch:  8284 , current epoch train loss:  0.0016128969145938754\n","Epoch:  8285 , current epoch train loss:  0.0015953382244333625\n","Epoch:  8286 , current epoch train loss:  0.0015992149710655212\n","Epoch:  8287 , current epoch train loss:  0.0016048001125454903\n","Epoch:  8288 , current epoch train loss:  0.001589398249052465\n","Epoch:  8289 , current epoch train loss:  0.0015664787497371435\n","Epoch:  8290 , current epoch train loss:  0.0015893969684839249\n","Epoch:  8291 , current epoch train loss:  0.0016906883101910353\n","Epoch:  8292 , current epoch train loss:  0.001591324806213379\n","Epoch:  8293 , current epoch train loss:  0.0016158614307641983\n","Epoch:  8294 , current epoch train loss:  0.001852648681961\n","Epoch:  8295 , current epoch train loss:  0.0016299360431730747\n","Epoch:  8296 , current epoch train loss:  0.00155708659440279\n","Epoch:  8297 , current epoch train loss:  0.0015606980305165052\n","Epoch:  8298 , current epoch train loss:  0.001595902955159545\n","Epoch:  8299 , current epoch train loss:  0.0016676568193361163\n","Epoch:  8300 , current epoch train loss:  0.001592931104823947\n","Epoch:  8301 , current epoch train loss:  0.0015386410523205996\n","Epoch:  8302 , current epoch train loss:  0.0016412632539868355\n","Epoch:  8303 , current epoch train loss:  0.001594273722730577\n","Epoch:  8304 , current epoch train loss:  0.0017085218569263816\n","Epoch:  8305 , current epoch train loss:  0.0016011239495128393\n","Epoch:  8306 , current epoch train loss:  0.001601998694241047\n","Epoch:  8307 , current epoch train loss:  0.0016497870674356818\n","Epoch:  8308 , current epoch train loss:  0.0016066263196989894\n","Epoch:  8309 , current epoch train loss:  0.0016542415833100677\n","Epoch:  8310 , current epoch train loss:  0.0015909713692963123\n","Epoch:  8311 , current epoch train loss:  0.0016112172743305564\n","Epoch:  8312 , current epoch train loss:  0.0016705228481441736\n","Epoch:  8313 , current epoch train loss:  0.0016480026533827186\n","Epoch:  8314 , current epoch train loss:  0.0016049448167905211\n","Epoch:  8315 , current epoch train loss:  0.0015696585178375244\n","Epoch:  8316 , current epoch train loss:  0.0015874761156737804\n","Epoch:  8317 , current epoch train loss:  0.001680100685916841\n","Epoch:  8318 , current epoch train loss:  0.001644378062337637\n","Epoch:  8319 , current epoch train loss:  0.0016652669291943312\n","Epoch:  8320 , current epoch train loss:  0.0015391496708616614\n","Epoch:  8321 , current epoch train loss:  0.0015987775987014174\n","Epoch:  8322 , current epoch train loss:  0.0016019046306610107\n","Epoch:  8323 , current epoch train loss:  0.0016030449187383056\n","Epoch:  8324 , current epoch train loss:  0.001725934213027358\n","Epoch:  8325 , current epoch train loss:  0.0016500444617122412\n","Epoch:  8326 , current epoch train loss:  0.00157553784083575\n","Epoch:  8327 , current epoch train loss:  0.0015995785361155868\n","Epoch:  8328 , current epoch train loss:  0.0015510825905948877\n","Epoch:  8329 , current epoch train loss:  0.001593870809301734\n","Epoch:  8330 , current epoch train loss:  0.0015774501953274012\n","Epoch:  8331 , current epoch train loss:  0.001584524055942893\n","Epoch:  8332 , current epoch train loss:  0.0015824083238840103\n","Epoch:  8333 , current epoch train loss:  0.0015700053190812469\n","Epoch:  8334 , current epoch train loss:  0.0016109507996588945\n","Epoch:  8335 , current epoch train loss:  0.0015547543298453093\n","Epoch:  8336 , current epoch train loss:  0.0016648704186081886\n","Epoch:  8337 , current epoch train loss:  0.0016904151998460293\n","Epoch:  8338 , current epoch train loss:  0.0015471016522496939\n","Epoch:  8339 , current epoch train loss:  0.00168844114523381\n","Epoch:  8340 , current epoch train loss:  0.0016014393186196685\n","Epoch:  8341 , current epoch train loss:  0.0016078769695013762\n","Epoch:  8342 , current epoch train loss:  0.0016066839452832937\n","Epoch:  8343 , current epoch train loss:  0.0015803159913048148\n","Epoch:  8344 , current epoch train loss:  0.0018474729731678963\n","Epoch:  8345 , current epoch train loss:  0.001604976481758058\n","Epoch:  8346 , current epoch train loss:  0.0016452583950012922\n","Epoch:  8347 , current epoch train loss:  0.0015913046663627028\n","Epoch:  8348 , current epoch train loss:  0.0015822509303689003\n","Epoch:  8349 , current epoch train loss:  0.0018250755965709686\n","Epoch:  8350 , current epoch train loss:  0.001609963714145124\n","Epoch:  8351 , current epoch train loss:  0.001620677998289466\n","Epoch:  8352 , current epoch train loss:  0.0016052251448854804\n","Epoch:  8353 , current epoch train loss:  0.0016578070353716612\n","Epoch:  8354 , current epoch train loss:  0.001564627862535417\n","Epoch:  8355 , current epoch train loss:  0.001616545021533966\n","Epoch:  8356 , current epoch train loss:  0.001569916494190693\n","Epoch:  8357 , current epoch train loss:  0.0016082716174423695\n","Epoch:  8358 , current epoch train loss:  0.0016020614420995116\n","Epoch:  8359 , current epoch train loss:  0.0015657662879675627\n","Epoch:  8360 , current epoch train loss:  0.0016161531675606966\n","Epoch:  8361 , current epoch train loss:  0.0015759896486997604\n","Epoch:  8362 , current epoch train loss:  0.0016001503681764007\n","Epoch:  8363 , current epoch train loss:  0.0015796609222888947\n","Epoch:  8364 , current epoch train loss:  0.0015392249915748835\n","Epoch:  8365 , current epoch train loss:  0.001582612982019782\n","Epoch:  8366 , current epoch train loss:  0.001545812701806426\n","Epoch:  8367 , current epoch train loss:  0.0016149900620803237\n","Epoch:  8368 , current epoch train loss:  0.0015976115828379989\n","Epoch:  8369 , current epoch train loss:  0.0015804215800017118\n","Epoch:  8370 , current epoch train loss:  0.0016218413366004825\n","Epoch:  8371 , current epoch train loss:  0.0016100367065519094\n","Epoch:  8372 , current epoch train loss:  0.0017008031718432903\n","Epoch:  8373 , current epoch train loss:  0.0016244326252490282\n","Epoch:  8374 , current epoch train loss:  0.0015778144588693976\n","Epoch:  8375 , current epoch train loss:  0.001542134559713304\n","Epoch:  8376 , current epoch train loss:  0.0016128923743963242\n","Epoch:  8377 , current epoch train loss:  0.0015684220707044005\n","Epoch:  8378 , current epoch train loss:  0.0015312236500903964\n","Epoch:  8379 , current epoch train loss:  0.0016191586619243026\n","Epoch:  8380 , current epoch train loss:  0.0015740576200187206\n","Epoch:  8381 , current epoch train loss:  0.0015506114577874541\n","Epoch:  8382 , current epoch train loss:  0.0016256971284747124\n","Epoch:  8383 , current epoch train loss:  0.0016246311133727431\n","Epoch:  8384 , current epoch train loss:  0.0015943541657179594\n","Epoch:  8385 , current epoch train loss:  0.0016731577925384045\n","Epoch:  8386 , current epoch train loss:  0.0015959476586431265\n","Epoch:  8387 , current epoch train loss:  0.0015441610012203455\n","Epoch:  8388 , current epoch train loss:  0.0016241418197751045\n","Epoch:  8389 , current epoch train loss:  0.0015707690035924315\n","Epoch:  8390 , current epoch train loss:  0.0015546716749668121\n","Epoch:  8391 , current epoch train loss:  0.0016814814880490303\n","Epoch:  8392 , current epoch train loss:  0.0015619525220245123\n","Epoch:  8393 , current epoch train loss:  0.001586279715411365\n","Epoch:  8394 , current epoch train loss:  0.0016429818933829665\n","Epoch:  8395 , current epoch train loss:  0.0015547291841357946\n","Epoch:  8396 , current epoch train loss:  0.001577777205966413\n","Epoch:  8397 , current epoch train loss:  0.0015998408198356628\n","Epoch:  8398 , current epoch train loss:  0.0015553459525108337\n","Epoch:  8399 , current epoch train loss:  0.001573954476043582\n","Epoch:  8400 , current epoch train loss:  0.001629795297048986\n","Epoch:  8401 , current epoch train loss:  0.0016836354043334723\n","Epoch:  8402 , current epoch train loss:  0.0015738442307338119\n","Epoch:  8403 , current epoch train loss:  0.001548039959743619\n","Epoch:  8404 , current epoch train loss:  0.0017267153598368168\n","Epoch:  8405 , current epoch train loss:  0.0016018877504393458\n","Epoch:  8406 , current epoch train loss:  0.0015505326446145773\n","Epoch:  8407 , current epoch train loss:  0.0015699643408879638\n","Epoch:  8408 , current epoch train loss:  0.001578467315994203\n","Epoch:  8409 , current epoch train loss:  0.001606702571734786\n","Epoch:  8410 , current epoch train loss:  0.001548353349789977\n","Epoch:  8411 , current epoch train loss:  0.0016285472083836794\n","Epoch:  8412 , current epoch train loss:  0.0016054663574323058\n","Epoch:  8413 , current epoch train loss:  0.0016148596769198775\n","Epoch:  8414 , current epoch train loss:  0.0016204796265810728\n","Epoch:  8415 , current epoch train loss:  0.0016020049806684256\n","Epoch:  8416 , current epoch train loss:  0.001614256645552814\n","Epoch:  8417 , current epoch train loss:  0.001579413888975978\n","Epoch:  8418 , current epoch train loss:  0.0016476133605465293\n","Epoch:  8419 , current epoch train loss:  0.0015492744278162718\n","Epoch:  8420 , current epoch train loss:  0.0015883240848779678\n","Epoch:  8421 , current epoch train loss:  0.001586656435392797\n","Epoch:  8422 , current epoch train loss:  0.0015582963824272156\n","Epoch:  8423 , current epoch train loss:  0.0015806787414476275\n","Epoch:  8424 , current epoch train loss:  0.001561630517244339\n","Epoch:  8425 , current epoch train loss:  0.0015867793699726462\n","Epoch:  8426 , current epoch train loss:  0.0015800943365320563\n","Epoch:  8427 , current epoch train loss:  0.0016069479752331972\n","Epoch:  8428 , current epoch train loss:  0.0015551892574876547\n","Epoch:  8429 , current epoch train loss:  0.0016841068863868713\n","Epoch:  8430 , current epoch train loss:  0.0016488509718328714\n","Epoch:  8431 , current epoch train loss:  0.0015680259093642235\n","Epoch:  8432 , current epoch train loss:  0.0015786964213475585\n","Epoch:  8433 , current epoch train loss:  0.0018673817394301295\n","Epoch:  8434 , current epoch train loss:  0.0016748234629631042\n","Epoch:  8435 , current epoch train loss:  0.0015995327848941088\n","Epoch:  8436 , current epoch train loss:  0.0015585513319820166\n","Epoch:  8437 , current epoch train loss:  0.0015692937886342406\n","Epoch:  8438 , current epoch train loss:  0.0015977872535586357\n","Epoch:  8439 , current epoch train loss:  0.0016021807678043842\n","Epoch:  8440 , current epoch train loss:  0.0015685707330703735\n","Epoch:  8441 , current epoch train loss:  0.0016089368145912886\n","Epoch:  8442 , current epoch train loss:  0.0015836604870855808\n","Epoch:  8443 , current epoch train loss:  0.0016426225192844868\n","Epoch:  8444 , current epoch train loss:  0.001558041200041771\n","Epoch:  8445 , current epoch train loss:  0.0015784524148330092\n","Epoch:  8446 , current epoch train loss:  0.0015720874071121216\n","Epoch:  8447 , current epoch train loss:  0.0016719106351956725\n","Epoch:  8448 , current epoch train loss:  0.0016263830475509167\n","Epoch:  8449 , current epoch train loss:  0.0016008595703169703\n","Epoch:  8450 , current epoch train loss:  0.0015371220652014017\n","Epoch:  8451 , current epoch train loss:  0.0016024144133552909\n","Epoch:  8452 , current epoch train loss:  0.0015526812057942152\n","Epoch:  8453 , current epoch train loss:  0.0015445360913872719\n","Epoch:  8454 , current epoch train loss:  0.001572024542838335\n","Epoch:  8455 , current epoch train loss:  0.0016153650358319283\n","Epoch:  8456 , current epoch train loss:  0.0016053291037678719\n","Epoch:  8457 , current epoch train loss:  0.0015815745573490858\n","Epoch:  8458 , current epoch train loss:  0.0015585883520543575\n","Epoch:  8459 , current epoch train loss:  0.0015879804268479347\n","Epoch:  8460 , current epoch train loss:  0.001572676352225244\n","Epoch:  8461 , current epoch train loss:  0.0016132172895595431\n","Epoch:  8462 , current epoch train loss:  0.0016224280698224902\n","Epoch:  8463 , current epoch train loss:  0.0015944037586450577\n","Epoch:  8464 , current epoch train loss:  0.0017129433108493686\n","Epoch:  8465 , current epoch train loss:  0.001555534079670906\n","Epoch:  8466 , current epoch train loss:  0.0015648293774574995\n","Epoch:  8467 , current epoch train loss:  0.0015550139360129833\n","Epoch:  8468 , current epoch train loss:  0.0015949541702866554\n","Epoch:  8469 , current epoch train loss:  0.0015680682845413685\n","Epoch:  8470 , current epoch train loss:  0.0015586866065859795\n","Epoch:  8471 , current epoch train loss:  0.0015477119013667107\n","Epoch:  8472 , current epoch train loss:  0.001572537119500339\n","Epoch:  8473 , current epoch train loss:  0.0015955808339640498\n","Epoch:  8474 , current epoch train loss:  0.0015516107669100165\n","Epoch:  8475 , current epoch train loss:  0.0015800524270161986\n","Epoch:  8476 , current epoch train loss:  0.0015989858657121658\n","Epoch:  8477 , current epoch train loss:  0.0016091326251626015\n","Epoch:  8478 , current epoch train loss:  0.0015879216371104121\n","Epoch:  8479 , current epoch train loss:  0.001575245289131999\n","Epoch:  8480 , current epoch train loss:  0.0016104381065815687\n","Epoch:  8481 , current epoch train loss:  0.0015965837519615889\n","Epoch:  8482 , current epoch train loss:  0.0015955888666212559\n","Epoch:  8483 , current epoch train loss:  0.0015999290626496077\n","Epoch:  8484 , current epoch train loss:  0.0016219352837651968\n","Epoch:  8485 , current epoch train loss:  0.0016014933353289962\n","Epoch:  8486 , current epoch train loss:  0.0016186375869438052\n","Epoch:  8487 , current epoch train loss:  0.0016019016038626432\n","Epoch:  8488 , current epoch train loss:  0.0015936761628836393\n","Epoch:  8489 , current epoch train loss:  0.0016167889116331935\n","Epoch:  8490 , current epoch train loss:  0.001614701352082193\n","Epoch:  8491 , current epoch train loss:  0.001647451426833868\n","Epoch:  8492 , current epoch train loss:  0.0015460833674296737\n","Epoch:  8493 , current epoch train loss:  0.0015750324819236994\n","Epoch:  8494 , current epoch train loss:  0.001665391493588686\n","Epoch:  8495 , current epoch train loss:  0.0015773519407957792\n","Epoch:  8496 , current epoch train loss:  0.001597308786585927\n","Epoch:  8497 , current epoch train loss:  0.001582292839884758\n","Epoch:  8498 , current epoch train loss:  0.0015508778160437942\n","Epoch:  8499 , current epoch train loss:  0.0015754569321870804\n","Epoch:  8500 , current epoch train loss:  0.0016365144401788712\n","Epoch:  8501 , current epoch train loss:  0.0015984710771590471\n","Epoch:  8502 , current epoch train loss:  0.0016057798638939857\n","Epoch:  8503 , current epoch train loss:  0.0015625045634806156\n","Epoch:  8504 , current epoch train loss:  0.0015486081829294562\n","Epoch:  8505 , current epoch train loss:  0.001622408046387136\n","Epoch:  8506 , current epoch train loss:  0.0015969484811648726\n","Epoch:  8507 , current epoch train loss:  0.0016110623255372047\n","Epoch:  8508 , current epoch train loss:  0.0016149323200806975\n","Epoch:  8509 , current epoch train loss:  0.0015861233696341515\n","Epoch:  8510 , current epoch train loss:  0.0015663522062823176\n","Epoch:  8511 , current epoch train loss:  0.001598508795723319\n","Epoch:  8512 , current epoch train loss:  0.0015777077060192823\n","Epoch:  8513 , current epoch train loss:  0.001539714983664453\n","Epoch:  8514 , current epoch train loss:  0.0015592072159051895\n","Epoch:  8515 , current epoch train loss:  0.001672497484833002\n","Epoch:  8516 , current epoch train loss:  0.0015817618696019053\n","Epoch:  8517 , current epoch train loss:  0.0016866307705640793\n","Epoch:  8518 , current epoch train loss:  0.0015607329551130533\n","Epoch:  8519 , current epoch train loss:  0.0015711940359324217\n","Epoch:  8520 , current epoch train loss:  0.0015999923925846815\n","Epoch:  8521 , current epoch train loss:  0.0016730218194425106\n","Epoch:  8522 , current epoch train loss:  0.0016239108517765999\n","Epoch:  8523 , current epoch train loss:  0.0016168858855962753\n","Epoch:  8524 , current epoch train loss:  0.0015770015306770802\n","Epoch:  8525 , current epoch train loss:  0.001637256471440196\n","Epoch:  8526 , current epoch train loss:  0.0016235214425250888\n","Epoch:  8527 , current epoch train loss:  0.0015929928049445152\n","Epoch:  8528 , current epoch train loss:  0.0016555357724428177\n","Epoch:  8529 , current epoch train loss:  0.0015734711196273565\n","Epoch:  8530 , current epoch train loss:  0.0015600677579641342\n","Epoch:  8531 , current epoch train loss:  0.0015925600891932845\n","Epoch:  8532 , current epoch train loss:  0.0015729557489976287\n","Epoch:  8533 , current epoch train loss:  0.0016026690136641264\n","Epoch:  8534 , current epoch train loss:  0.0015833656070753932\n","Epoch:  8535 , current epoch train loss:  0.001578324823640287\n","Epoch:  8536 , current epoch train loss:  0.001560761360451579\n","Epoch:  8537 , current epoch train loss:  0.001627997844479978\n","Epoch:  8538 , current epoch train loss:  0.0015732370084151626\n","Epoch:  8539 , current epoch train loss:  0.001595717272721231\n","Epoch:  8540 , current epoch train loss:  0.0015941666206344962\n","Epoch:  8541 , current epoch train loss:  0.0016177489887923002\n","Epoch:  8542 , current epoch train loss:  0.0015806814190000296\n","Epoch:  8543 , current epoch train loss:  0.0015963142504915595\n","Epoch:  8544 , current epoch train loss:  0.0015644466038793325\n","Epoch:  8545 , current epoch train loss:  0.0016233003698289394\n","Epoch:  8546 , current epoch train loss:  0.0017607667250558734\n","Epoch:  8547 , current epoch train loss:  0.0016338912537321448\n","Epoch:  8548 , current epoch train loss:  0.0015714842593297362\n","Epoch:  8549 , current epoch train loss:  0.0015267478302121162\n","Epoch:  8550 , current epoch train loss:  0.0016090013086795807\n","Epoch:  8551 , current epoch train loss:  0.001602941076271236\n","Epoch:  8552 , current epoch train loss:  0.0016752257943153381\n","Epoch:  8553 , current epoch train loss:  0.001721756998449564\n","Epoch:  8554 , current epoch train loss:  0.0015920584555715322\n","Epoch:  8555 , current epoch train loss:  0.00163339264690876\n","Epoch:  8556 , current epoch train loss:  0.0015746487770229578\n","Epoch:  8557 , current epoch train loss:  0.0016113158781081438\n","Epoch:  8558 , current epoch train loss:  0.001630025333724916\n","Epoch:  8559 , current epoch train loss:  0.001707758172415197\n","Epoch:  8560 , current epoch train loss:  0.0016031225677579641\n","Epoch:  8561 , current epoch train loss:  0.0015774208586663008\n","Epoch:  8562 , current epoch train loss:  0.0015684649115428329\n","Epoch:  8563 , current epoch train loss:  0.0016195911448448896\n","Epoch:  8564 , current epoch train loss:  0.0016110719880089164\n","Epoch:  8565 , current epoch train loss:  0.001558355987071991\n","Epoch:  8566 , current epoch train loss:  0.0015686373226344585\n","Epoch:  8567 , current epoch train loss:  0.001585664926096797\n","Epoch:  8568 , current epoch train loss:  0.001612507738173008\n","Epoch:  8569 , current epoch train loss:  0.0015933451941236854\n","Epoch:  8570 , current epoch train loss:  0.0015509109944105148\n","Epoch:  8571 , current epoch train loss:  0.0015502715250477195\n","Epoch:  8572 , current epoch train loss:  0.0015543312765657902\n","Epoch:  8573 , current epoch train loss:  0.0015795119106769562\n","Epoch:  8574 , current epoch train loss:  0.0015803361311554909\n","Epoch:  8575 , current epoch train loss:  0.0015670948196202517\n","Epoch:  8576 , current epoch train loss:  0.001544927479699254\n","Epoch:  8577 , current epoch train loss:  0.0015900337602943182\n","Epoch:  8578 , current epoch train loss:  0.0015734288608655334\n","Epoch:  8579 , current epoch train loss:  0.0015994219575077295\n","Epoch:  8580 , current epoch train loss:  0.0015635843155905604\n","Epoch:  8581 , current epoch train loss:  0.0016434936551377177\n","Epoch:  8582 , current epoch train loss:  0.001680903835222125\n","Epoch:  8583 , current epoch train loss:  0.0015909264329820871\n","Epoch:  8584 , current epoch train loss:  0.0015634851297363639\n","Epoch:  8585 , current epoch train loss:  0.0015886265318840742\n","Epoch:  8586 , current epoch train loss:  0.001611983752809465\n","Epoch:  8587 , current epoch train loss:  0.0015962086617946625\n","Epoch:  8588 , current epoch train loss:  0.0015351176261901855\n","Epoch:  8589 , current epoch train loss:  0.0016011535190045834\n","Epoch:  8590 , current epoch train loss:  0.0015779001405462623\n","Epoch:  8591 , current epoch train loss:  0.001675957813858986\n","Epoch:  8592 , current epoch train loss:  0.0015734871849417686\n","Epoch:  8593 , current epoch train loss:  0.001602469477802515\n","Epoch:  8594 , current epoch train loss:  0.0015581799671053886\n","Epoch:  8595 , current epoch train loss:  0.0015863518929108977\n","Epoch:  8596 , current epoch train loss:  0.0015902479644864798\n","Epoch:  8597 , current epoch train loss:  0.0015841992571949959\n","Epoch:  8598 , current epoch train loss:  0.0015998980961740017\n","Epoch:  8599 , current epoch train loss:  0.0016029428225010633\n","Epoch:  8600 , current epoch train loss:  0.0015520936576649547\n","Epoch:  8601 , current epoch train loss:  0.0015602863859385252\n","Epoch:  8602 , current epoch train loss:  0.0015785623108968139\n","Epoch:  8603 , current epoch train loss:  0.0016409013187512755\n","Epoch:  8604 , current epoch train loss:  0.0015557738952338696\n","Epoch:  8605 , current epoch train loss:  0.0015824263682588935\n","Epoch:  8606 , current epoch train loss:  0.0015622514765709639\n","Epoch:  8607 , current epoch train loss:  0.001556433504447341\n","Epoch:  8608 , current epoch train loss:  0.0015824346337467432\n","Epoch:  8609 , current epoch train loss:  0.001845813705585897\n","Epoch:  8610 , current epoch train loss:  0.0015332847833633423\n","Epoch:  8611 , current epoch train loss:  0.001657579094171524\n","Epoch:  8612 , current epoch train loss:  0.0016106156399473548\n","Epoch:  8613 , current epoch train loss:  0.0016707474132999778\n","Epoch:  8614 , current epoch train loss:  0.0015437237452715635\n","Epoch:  8615 , current epoch train loss:  0.0015581914922222495\n","Epoch:  8616 , current epoch train loss:  0.0015936787240207195\n","Epoch:  8617 , current epoch train loss:  0.001589539460837841\n","Epoch:  8618 , current epoch train loss:  0.001580078387632966\n","Epoch:  8619 , current epoch train loss:  0.0016307540936395526\n","Epoch:  8620 , current epoch train loss:  0.001576975453644991\n","Epoch:  8621 , current epoch train loss:  0.0015874974196776748\n","Epoch:  8622 , current epoch train loss:  0.0016589112346991897\n","Epoch:  8623 , current epoch train loss:  0.0016505613457411528\n","Epoch:  8624 , current epoch train loss:  0.001564777921885252\n","Epoch:  8625 , current epoch train loss:  0.0016094264574348927\n","Epoch:  8626 , current epoch train loss:  0.001579450210556388\n","Epoch:  8627 , current epoch train loss:  0.0015522370813414454\n","Epoch:  8628 , current epoch train loss:  0.0015911663649603724\n","Epoch:  8629 , current epoch train loss:  0.0016265446320176125\n","Epoch:  8630 , current epoch train loss:  0.001665429095737636\n","Epoch:  8631 , current epoch train loss:  0.0015661243814975023\n","Epoch:  8632 , current epoch train loss:  0.0016145051922649145\n","Epoch:  8633 , current epoch train loss:  0.0016827427316457033\n","Epoch:  8634 , current epoch train loss:  0.0015559452585875988\n","Epoch:  8635 , current epoch train loss:  0.0016022874042391777\n","Epoch:  8636 , current epoch train loss:  0.0015894919633865356\n","Epoch:  8637 , current epoch train loss:  0.0015787426382303238\n","Epoch:  8638 , current epoch train loss:  0.0015531564131379128\n","Epoch:  8639 , current epoch train loss:  0.0015725018456578255\n","Epoch:  8640 , current epoch train loss:  0.001602941076271236\n","Epoch:  8641 , current epoch train loss:  0.0015670692082494497\n","Epoch:  8642 , current epoch train loss:  0.0015560907777398825\n","Epoch:  8643 , current epoch train loss:  0.0015400259289890528\n","Epoch:  8644 , current epoch train loss:  0.001557918731123209\n","Epoch:  8645 , current epoch train loss:  0.0015824477886781096\n","Epoch:  8646 , current epoch train loss:  0.0016792126698419452\n","Epoch:  8647 , current epoch train loss:  0.001554332091473043\n","Epoch:  8648 , current epoch train loss:  0.001569725456647575\n","Epoch:  8649 , current epoch train loss:  0.0015474300598725677\n","Epoch:  8650 , current epoch train loss:  0.001551315188407898\n","Epoch:  8651 , current epoch train loss:  0.0015769728925079107\n","Epoch:  8652 , current epoch train loss:  0.0015984869096428156\n","Epoch:  8653 , current epoch train loss:  0.0015749394660815597\n","Epoch:  8654 , current epoch train loss:  0.0015984060009941459\n","Epoch:  8655 , current epoch train loss:  0.0015937634743750095\n","Epoch:  8656 , current epoch train loss:  0.00157035025767982\n","Epoch:  8657 , current epoch train loss:  0.0016031439881771803\n","Epoch:  8658 , current epoch train loss:  0.0016176167409867048\n","Epoch:  8659 , current epoch train loss:  0.0015376925002783537\n","Epoch:  8660 , current epoch train loss:  0.001602061791345477\n","Epoch:  8661 , current epoch train loss:  0.0015476110856980085\n","Epoch:  8662 , current epoch train loss:  0.0015716732013970613\n","Epoch:  8663 , current epoch train loss:  0.0015776455402374268\n","Epoch:  8664 , current epoch train loss:  0.0015984780620783567\n","Epoch:  8665 , current epoch train loss:  0.0016466694651171565\n","Epoch:  8666 , current epoch train loss:  0.001597310183569789\n","Epoch:  8667 , current epoch train loss:  0.0015499002765864134\n","Epoch:  8668 , current epoch train loss:  0.001672179438173771\n","Epoch:  8669 , current epoch train loss:  0.0016156230121850967\n","Epoch:  8670 , current epoch train loss:  0.0015596849843859673\n","Epoch:  8671 , current epoch train loss:  0.0015656459145247936\n","Epoch:  8672 , current epoch train loss:  0.00156284065451473\n","Epoch:  8673 , current epoch train loss:  0.0016374135157093406\n","Epoch:  8674 , current epoch train loss:  0.0015554087003692985\n","Epoch:  8675 , current epoch train loss:  0.0016229525208473206\n","Epoch:  8676 , current epoch train loss:  0.001563987578265369\n","Epoch:  8677 , current epoch train loss:  0.0015874756500124931\n","Epoch:  8678 , current epoch train loss:  0.001616485184058547\n","Epoch:  8679 , current epoch train loss:  0.0015386113664135337\n","Epoch:  8680 , current epoch train loss:  0.0015552345430478454\n","Epoch:  8681 , current epoch train loss:  0.0015719312941655517\n","Epoch:  8682 , current epoch train loss:  0.0015810811892151833\n","Epoch:  8683 , current epoch train loss:  0.001554881571792066\n","Epoch:  8684 , current epoch train loss:  0.0015883514424785972\n","Epoch:  8685 , current epoch train loss:  0.0015554990386590362\n","Epoch:  8686 , current epoch train loss:  0.001538916490972042\n","Epoch:  8687 , current epoch train loss:  0.001590277417562902\n","Epoch:  8688 , current epoch train loss:  0.00155509146861732\n","Epoch:  8689 , current epoch train loss:  0.0015317884972319007\n","Epoch:  8690 , current epoch train loss:  0.0016113629098981619\n","Epoch:  8691 , current epoch train loss:  0.001550976070575416\n","Epoch:  8692 , current epoch train loss:  0.001584700308740139\n","Epoch:  8693 , current epoch train loss:  0.0015553183620795608\n","Epoch:  8694 , current epoch train loss:  0.0015382005367428064\n","Epoch:  8695 , current epoch train loss:  0.0015541368629783392\n","Epoch:  8696 , current epoch train loss:  0.001717155333608389\n","Epoch:  8697 , current epoch train loss:  0.0016033732099458575\n","Epoch:  8698 , current epoch train loss:  0.001589546911418438\n","Epoch:  8699 , current epoch train loss:  0.001553227542899549\n","Epoch:  8700 , current epoch train loss:  0.0016716218087822199\n","Epoch:  8701 , current epoch train loss:  0.001561250421218574\n","Epoch:  8702 , current epoch train loss:  0.0015549487434327602\n","Epoch:  8703 , current epoch train loss:  0.0015503386966884136\n","Epoch:  8704 , current epoch train loss:  0.0016259734984487295\n","Epoch:  8705 , current epoch train loss:  0.0015765781281515956\n","Epoch:  8706 , current epoch train loss:  0.0015567245427519083\n","Epoch:  8707 , current epoch train loss:  0.0015629309928044677\n","Epoch:  8708 , current epoch train loss:  0.0015518374275416136\n","Epoch:  8709 , current epoch train loss:  0.001553102396428585\n","Epoch:  8710 , current epoch train loss:  0.0016121622174978256\n","Epoch:  8711 , current epoch train loss:  0.0015911429654806852\n","Epoch:  8712 , current epoch train loss:  0.0015970615204423666\n","Epoch:  8713 , current epoch train loss:  0.0015577403828501701\n","Epoch:  8714 , current epoch train loss:  0.0015967923682183027\n","Epoch:  8715 , current epoch train loss:  0.001609928673133254\n","Epoch:  8716 , current epoch train loss:  0.0015906530898064375\n","Epoch:  8717 , current epoch train loss:  0.0016254154033958912\n","Epoch:  8718 , current epoch train loss:  0.0015543675981462002\n","Epoch:  8719 , current epoch train loss:  0.0015484158648177981\n","Epoch:  8720 , current epoch train loss:  0.0015469063073396683\n","Epoch:  8721 , current epoch train loss:  0.0015671737492084503\n","Epoch:  8722 , current epoch train loss:  0.001536485506221652\n","Epoch:  8723 , current epoch train loss:  0.0015384270809590816\n","Epoch:  8724 , current epoch train loss:  0.0016276610549539328\n","Epoch:  8725 , current epoch train loss:  0.0015621469356119633\n","Epoch:  8726 , current epoch train loss:  0.001618739916011691\n","Epoch:  8727 , current epoch train loss:  0.0015843699220567942\n","Epoch:  8728 , current epoch train loss:  0.0015555362915620208\n","Epoch:  8729 , current epoch train loss:  0.0016144202090799809\n","Epoch:  8730 , current epoch train loss:  0.0015437057008966804\n","Epoch:  8731 , current epoch train loss:  0.00157222431153059\n","Epoch:  8732 , current epoch train loss:  0.0016154575860127807\n","Epoch:  8733 , current epoch train loss:  0.0015772784827277064\n","Epoch:  8734 , current epoch train loss:  0.0015772621845826507\n","Epoch:  8735 , current epoch train loss:  0.0015872509684413671\n","Epoch:  8736 , current epoch train loss:  0.001603750977665186\n","Epoch:  8737 , current epoch train loss:  0.001565680606290698\n","Epoch:  8738 , current epoch train loss:  0.0016253665089607239\n","Epoch:  8739 , current epoch train loss:  0.001653582789003849\n","Epoch:  8740 , current epoch train loss:  0.0016035425942391157\n","Epoch:  8741 , current epoch train loss:  0.0015563479391857982\n","Epoch:  8742 , current epoch train loss:  0.0015572418924421072\n","Epoch:  8743 , current epoch train loss:  0.0015783521812409163\n","Epoch:  8744 , current epoch train loss:  0.0015584463253617287\n","Epoch:  8745 , current epoch train loss:  0.0015962778124958277\n","Epoch:  8746 , current epoch train loss:  0.0015999188181012869\n","Epoch:  8747 , current epoch train loss:  0.0015792277408763766\n","Epoch:  8748 , current epoch train loss:  0.001594813889823854\n","Epoch:  8749 , current epoch train loss:  0.0015892827650532126\n","Epoch:  8750 , current epoch train loss:  0.0015823281137272716\n","Epoch:  8751 , current epoch train loss:  0.0015571808908134699\n","Epoch:  8752 , current epoch train loss:  0.0016117948107421398\n","Epoch:  8753 , current epoch train loss:  0.0015817539533600211\n","Epoch:  8754 , current epoch train loss:  0.0015495333354920149\n","Epoch:  8755 , current epoch train loss:  0.0015896335244178772\n","Epoch:  8756 , current epoch train loss:  0.001561559154652059\n","Epoch:  8757 , current epoch train loss:  0.0016125546535477042\n","Epoch:  8758 , current epoch train loss:  0.0015625609084963799\n","Epoch:  8759 , current epoch train loss:  0.0015700397780165076\n","Epoch:  8760 , current epoch train loss:  0.0015575296711176634\n","Epoch:  8761 , current epoch train loss:  0.001650218735449016\n","Epoch:  8762 , current epoch train loss:  0.0016810998786240816\n","Epoch:  8763 , current epoch train loss:  0.0015527758514508605\n","Epoch:  8764 , current epoch train loss:  0.0015407013706862926\n","Epoch:  8765 , current epoch train loss:  0.0016195704229176044\n","Epoch:  8766 , current epoch train loss:  0.001624563941732049\n","Epoch:  8767 , current epoch train loss:  0.0015630729030817747\n","Epoch:  8768 , current epoch train loss:  0.0015707712154835463\n","Epoch:  8769 , current epoch train loss:  0.0015377087984234095\n","Epoch:  8770 , current epoch train loss:  0.0016155828488990664\n","Epoch:  8771 , current epoch train loss:  0.001543644699268043\n","Epoch:  8772 , current epoch train loss:  0.0015634596347808838\n","Epoch:  8773 , current epoch train loss:  0.0016420581378042698\n","Epoch:  8774 , current epoch train loss:  0.00157914066221565\n","Epoch:  8775 , current epoch train loss:  0.0016067004762589931\n","Epoch:  8776 , current epoch train loss:  0.0015860669082030654\n","Epoch:  8777 , current epoch train loss:  0.0016201030230149627\n","Epoch:  8778 , current epoch train loss:  0.0016038561007007957\n","Epoch:  8779 , current epoch train loss:  0.0015366876032203436\n","Epoch:  8780 , current epoch train loss:  0.001570079824887216\n","Epoch:  8781 , current epoch train loss:  0.0015246968250721693\n","Epoch:  8782 , current epoch train loss:  0.0016005716752260923\n","Epoch:  8783 , current epoch train loss:  0.0015936009585857391\n","Epoch:  8784 , current epoch train loss:  0.001555592636577785\n","Epoch:  8785 , current epoch train loss:  0.0015927280765026808\n","Epoch:  8786 , current epoch train loss:  0.001647799275815487\n","Epoch:  8787 , current epoch train loss:  0.0015282072126865387\n","Epoch:  8788 , current epoch train loss:  0.0015899016289040446\n","Epoch:  8789 , current epoch train loss:  0.0015837398823350668\n","Epoch:  8790 , current epoch train loss:  0.0015870578354224563\n","Epoch:  8791 , current epoch train loss:  0.0015324205160140991\n","Epoch:  8792 , current epoch train loss:  0.0016166309360414743\n","Epoch:  8793 , current epoch train loss:  0.0015780817484483123\n","Epoch:  8794 , current epoch train loss:  0.0015350086614489555\n","Epoch:  8795 , current epoch train loss:  0.0015391034539788961\n","Epoch:  8796 , current epoch train loss:  0.0015526445349678397\n","Epoch:  8797 , current epoch train loss:  0.0016151875024661422\n","Epoch:  8798 , current epoch train loss:  0.0015814746730029583\n","Epoch:  8799 , current epoch train loss:  0.0015676699113100767\n","Epoch:  8800 , current epoch train loss:  0.0015494668623432517\n","Epoch:  8801 , current epoch train loss:  0.001595368841663003\n","Epoch:  8802 , current epoch train loss:  0.0015326207503676414\n","Epoch:  8803 , current epoch train loss:  0.0016975726466625929\n","Epoch:  8804 , current epoch train loss:  0.0015737633220851421\n","Epoch:  8805 , current epoch train loss:  0.0015614198055118322\n","Epoch:  8806 , current epoch train loss:  0.0015325917629525065\n","Epoch:  8807 , current epoch train loss:  0.0016995755722746253\n","Epoch:  8808 , current epoch train loss:  0.0015542045002803206\n","Epoch:  8809 , current epoch train loss:  0.0016357784625142813\n","Epoch:  8810 , current epoch train loss:  0.001632198109291494\n","Epoch:  8811 , current epoch train loss:  0.0015485625481233\n","Epoch:  8812 , current epoch train loss:  0.0015751617029309273\n","Epoch:  8813 , current epoch train loss:  0.0015783344861119986\n","Epoch:  8814 , current epoch train loss:  0.0015946035273373127\n","Epoch:  8815 , current epoch train loss:  0.0015765296993777156\n","Epoch:  8816 , current epoch train loss:  0.0016304138116538525\n","Epoch:  8817 , current epoch train loss:  0.0016199338715523481\n","Epoch:  8818 , current epoch train loss:  0.0015424105804413557\n","Epoch:  8819 , current epoch train loss:  0.0016287397593259811\n","Epoch:  8820 , current epoch train loss:  0.0016035456210374832\n","Epoch:  8821 , current epoch train loss:  0.0015939216827973723\n","Epoch:  8822 , current epoch train loss:  0.001597879221662879\n","Epoch:  8823 , current epoch train loss:  0.0015592118725180626\n","Epoch:  8824 , current epoch train loss:  0.0015810010954737663\n","Epoch:  8825 , current epoch train loss:  0.0015995422145351768\n","Epoch:  8826 , current epoch train loss:  0.00162271480076015\n","Epoch:  8827 , current epoch train loss:  0.001568709034472704\n","Epoch:  8828 , current epoch train loss:  0.0016070003621280193\n","Epoch:  8829 , current epoch train loss:  0.0015309661393985152\n","Epoch:  8830 , current epoch train loss:  0.001544441096484661\n","Epoch:  8831 , current epoch train loss:  0.0015644098166376352\n","Epoch:  8832 , current epoch train loss:  0.0016778274439275265\n","Epoch:  8833 , current epoch train loss:  0.0015753608895465732\n","Epoch:  8834 , current epoch train loss:  0.00156163121573627\n","Epoch:  8835 , current epoch train loss:  0.001626678160391748\n","Epoch:  8836 , current epoch train loss:  0.0015535157872363925\n","Epoch:  8837 , current epoch train loss:  0.0015223994851112366\n","Epoch:  8838 , current epoch train loss:  0.0015767232980579138\n","Epoch:  8839 , current epoch train loss:  0.0015697302296757698\n","Epoch:  8840 , current epoch train loss:  0.0015891528455540538\n","Epoch:  8841 , current epoch train loss:  0.0015604847576469183\n","Epoch:  8842 , current epoch train loss:  0.0015718502691015601\n","Epoch:  8843 , current epoch train loss:  0.0015448906924575567\n","Epoch:  8844 , current epoch train loss:  0.0015645402017980814\n","Epoch:  8845 , current epoch train loss:  0.0015623903600499034\n","Epoch:  8846 , current epoch train loss:  0.001636053784750402\n","Epoch:  8847 , current epoch train loss:  0.0015360013348981738\n","Epoch:  8848 , current epoch train loss:  0.0015448664780706167\n","Epoch:  8849 , current epoch train loss:  0.0015831210184842348\n","Epoch:  8850 , current epoch train loss:  0.001550445333123207\n","Epoch:  8851 , current epoch train loss:  0.0015768490266054869\n","Epoch:  8852 , current epoch train loss:  0.0016444831853732467\n","Epoch:  8853 , current epoch train loss:  0.0015540601452812552\n","Epoch:  8854 , current epoch train loss:  0.001540919067338109\n","Epoch:  8855 , current epoch train loss:  0.0016417022561654449\n","Epoch:  8856 , current epoch train loss:  0.0015675793401896954\n","Epoch:  8857 , current epoch train loss:  0.0016330629587173462\n","Epoch:  8858 , current epoch train loss:  0.001577960210852325\n","Epoch:  8859 , current epoch train loss:  0.0015311934985220432\n","Epoch:  8860 , current epoch train loss:  0.0015508750220760703\n","Epoch:  8861 , current epoch train loss:  0.0015815560473129153\n","Epoch:  8862 , current epoch train loss:  0.001573377288877964\n","Epoch:  8863 , current epoch train loss:  0.0015466253971680999\n","Epoch:  8864 , current epoch train loss:  0.0015745945274829865\n","Epoch:  8865 , current epoch train loss:  0.0015671930741518736\n","Epoch:  8866 , current epoch train loss:  0.0015968154184520245\n","Epoch:  8867 , current epoch train loss:  0.0015536638675257564\n","Epoch:  8868 , current epoch train loss:  0.0015970433596521616\n","Epoch:  8869 , current epoch train loss:  0.001573692075908184\n","Epoch:  8870 , current epoch train loss:  0.0015647131949663162\n","Epoch:  8871 , current epoch train loss:  0.001550844288431108\n","Epoch:  8872 , current epoch train loss:  0.0015195830492302775\n","Epoch:  8873 , current epoch train loss:  0.0015788970049470663\n","Epoch:  8874 , current epoch train loss:  0.0016213038470596075\n","Epoch:  8875 , current epoch train loss:  0.001589184976182878\n","Epoch:  8876 , current epoch train loss:  0.0015820906264707446\n","Epoch:  8877 , current epoch train loss:  0.0015955533599480987\n","Epoch:  8878 , current epoch train loss:  0.0016065454110503197\n","Epoch:  8879 , current epoch train loss:  0.0016105937538668513\n","Epoch:  8880 , current epoch train loss:  0.00162527896463871\n","Epoch:  8881 , current epoch train loss:  0.0016206823056563735\n","Epoch:  8882 , current epoch train loss:  0.0016531306318938732\n","Epoch:  8883 , current epoch train loss:  0.0016195171047002077\n","Epoch:  8884 , current epoch train loss:  0.0015692254528403282\n","Epoch:  8885 , current epoch train loss:  0.0015354229835793376\n","Epoch:  8886 , current epoch train loss:  0.0016037565656006336\n","Epoch:  8887 , current epoch train loss:  0.0015541890170425177\n","Epoch:  8888 , current epoch train loss:  0.00153175403829664\n","Epoch:  8889 , current epoch train loss:  0.0015692011220380664\n","Epoch:  8890 , current epoch train loss:  0.0015757682267576456\n","Epoch:  8891 , current epoch train loss:  0.0015765773132443428\n","Epoch:  8892 , current epoch train loss:  0.0015816676896065474\n","Epoch:  8893 , current epoch train loss:  0.0015625169035047293\n","Epoch:  8894 , current epoch train loss:  0.0016036195447668433\n","Epoch:  8895 , current epoch train loss:  0.0015719918301329017\n","Epoch:  8896 , current epoch train loss:  0.001572516281157732\n","Epoch:  8897 , current epoch train loss:  0.0018057184061035514\n","Epoch:  8898 , current epoch train loss:  0.0019623776897788048\n","Epoch:  8899 , current epoch train loss:  0.0015929928049445152\n","Epoch:  8900 , current epoch train loss:  0.001550746033899486\n","Epoch:  8901 , current epoch train loss:  0.001503302133642137\n","Epoch:  8902 , current epoch train loss:  0.0015604686923325062\n","Epoch:  8903 , current epoch train loss:  0.0015992503613233566\n","Epoch:  8904 , current epoch train loss:  0.0016359532019123435\n","Epoch:  8905 , current epoch train loss:  0.0015701369848102331\n","Epoch:  8906 , current epoch train loss:  0.0015540895983576775\n","Epoch:  8907 , current epoch train loss:  0.0017329970141872764\n","Epoch:  8908 , current epoch train loss:  0.0016390187665820122\n","Epoch:  8909 , current epoch train loss:  0.0015519134467467666\n","Epoch:  8910 , current epoch train loss:  0.0016301912255585194\n","Epoch:  8911 , current epoch train loss:  0.0015833089128136635\n","Epoch:  8912 , current epoch train loss:  0.0016334950923919678\n","Epoch:  8913 , current epoch train loss:  0.0017755416920408607\n","Epoch:  8914 , current epoch train loss:  0.0016005266224965453\n","Epoch:  8915 , current epoch train loss:  0.0015739527298137546\n","Epoch:  8916 , current epoch train loss:  0.0016205557622015476\n","Epoch:  8917 , current epoch train loss:  0.0015402849530801177\n","Epoch:  8918 , current epoch train loss:  0.0015892413211986423\n","Epoch:  8919 , current epoch train loss:  0.0015491899102926254\n","Epoch:  8920 , current epoch train loss:  0.0015977792209014297\n","Epoch:  8921 , current epoch train loss:  0.0017637130804359913\n","Epoch:  8922 , current epoch train loss:  0.0016286633908748627\n","Epoch:  8923 , current epoch train loss:  0.0015313767362385988\n","Epoch:  8924 , current epoch train loss:  0.0017909755697473884\n","Epoch:  8925 , current epoch train loss:  0.001570624066516757\n","Epoch:  8926 , current epoch train loss:  0.0015748029109090567\n","Epoch:  8927 , current epoch train loss:  0.0015689510619267821\n","Epoch:  8928 , current epoch train loss:  0.0016597211360931396\n","Epoch:  8929 , current epoch train loss:  0.0015801548724994063\n","Epoch:  8930 , current epoch train loss:  0.001551259309053421\n","Epoch:  8931 , current epoch train loss:  0.0015294207260012627\n","Epoch:  8932 , current epoch train loss:  0.0015947034116834402\n","Epoch:  8933 , current epoch train loss:  0.0015645390376448631\n","Epoch:  8934 , current epoch train loss:  0.0015696611953899264\n","Epoch:  8935 , current epoch train loss:  0.0016627795994281769\n","Epoch:  8936 , current epoch train loss:  0.0015941666206344962\n","Epoch:  8937 , current epoch train loss:  0.001579958712682128\n","Epoch:  8938 , current epoch train loss:  0.0016714227385818958\n","Epoch:  8939 , current epoch train loss:  0.0015288786962628365\n","Epoch:  8940 , current epoch train loss:  0.0015338538214564323\n","Epoch:  8941 , current epoch train loss:  0.0015887211775407195\n","Epoch:  8942 , current epoch train loss:  0.0016227362211793661\n","Epoch:  8943 , current epoch train loss:  0.0015522356843575835\n","Epoch:  8944 , current epoch train loss:  0.0015953931724652648\n","Epoch:  8945 , current epoch train loss:  0.0015806149458512664\n","Epoch:  8946 , current epoch train loss:  0.0015794591745361686\n","Epoch:  8947 , current epoch train loss:  0.0016023203497752547\n","Epoch:  8948 , current epoch train loss:  0.0015908856876194477\n","Epoch:  8949 , current epoch train loss:  0.0015391195192933083\n","Epoch:  8950 , current epoch train loss:  0.0015423366567119956\n","Epoch:  8951 , current epoch train loss:  0.0015564262866973877\n","Epoch:  8952 , current epoch train loss:  0.001560550183057785\n","Epoch:  8953 , current epoch train loss:  0.0015308919828385115\n","Epoch:  8954 , current epoch train loss:  0.001535718678496778\n","Epoch:  8955 , current epoch train loss:  0.0015099032316356897\n","Epoch:  8956 , current epoch train loss:  0.001582985045388341\n","Epoch:  8957 , current epoch train loss:  0.001582928467541933\n","Epoch:  8958 , current epoch train loss:  0.001530362293124199\n","Epoch:  8959 , current epoch train loss:  0.0015688122948631644\n","Epoch:  8960 , current epoch train loss:  0.0015596707817167044\n","Epoch:  8961 , current epoch train loss:  0.0015355134382843971\n","Epoch:  8962 , current epoch train loss:  0.0016193073242902756\n","Epoch:  8963 , current epoch train loss:  0.0016093169106170535\n","Epoch:  8964 , current epoch train loss:  0.0015450672945007682\n","Epoch:  8965 , current epoch train loss:  0.0016028478275984526\n","Epoch:  8966 , current epoch train loss:  0.0015772649785503745\n","Epoch:  8967 , current epoch train loss:  0.001553880050778389\n","Epoch:  8968 , current epoch train loss:  0.0015799505636096\n","Epoch:  8969 , current epoch train loss:  0.0015802480047568679\n","Epoch:  8970 , current epoch train loss:  0.0015764542622491717\n","Epoch:  8971 , current epoch train loss:  0.0015672259032726288\n","Epoch:  8972 , current epoch train loss:  0.0015843517612665892\n","Epoch:  8973 , current epoch train loss:  0.001566944643855095\n","Epoch:  8974 , current epoch train loss:  0.0015409685438498855\n","Epoch:  8975 , current epoch train loss:  0.0016084624221548438\n","Epoch:  8976 , current epoch train loss:  0.0015480606816709042\n","Epoch:  8977 , current epoch train loss:  0.0015998727176338434\n","Epoch:  8978 , current epoch train loss:  0.0015714940382167697\n","Epoch:  8979 , current epoch train loss:  0.0016366380732506514\n","Epoch:  8980 , current epoch train loss:  0.0015698687639087439\n","Epoch:  8981 , current epoch train loss:  0.0015743690310046077\n","Epoch:  8982 , current epoch train loss:  0.0016425063367933035\n","Epoch:  8983 , current epoch train loss:  0.0015790242468938231\n","Epoch:  8984 , current epoch train loss:  0.00162330677267164\n","Epoch:  8985 , current epoch train loss:  0.0015534504782408476\n","Epoch:  8986 , current epoch train loss:  0.0015656150644645095\n","Epoch:  8987 , current epoch train loss:  0.0016756210243329406\n","Epoch:  8988 , current epoch train loss:  0.0015310549642890692\n","Epoch:  8989 , current epoch train loss:  0.0015637455508112907\n","Epoch:  8990 , current epoch train loss:  0.0015930039808154106\n","Epoch:  8991 , current epoch train loss:  0.0016114157624542713\n","Epoch:  8992 , current epoch train loss:  0.001544630154967308\n","Epoch:  8993 , current epoch train loss:  0.0015764734707772732\n","Epoch:  8994 , current epoch train loss:  0.001601292286068201\n","Epoch:  8995 , current epoch train loss:  0.0016148383729159832\n","Epoch:  8996 , current epoch train loss:  0.0015535223064944148\n","Epoch:  8997 , current epoch train loss:  0.001572033273987472\n","Epoch:  8998 , current epoch train loss:  0.0015866266330704093\n","Epoch:  8999 , current epoch train loss:  0.0015814630314707756\n","Epoch:  9000 , current epoch train loss:  0.0015313411131501198\n","Epoch:  9001 , current epoch train loss:  0.0016102148219943047\n","Epoch:  9002 , current epoch train loss:  0.0015817061066627502\n","Epoch:  9003 , current epoch train loss:  0.0015695312758907676\n","Epoch:  9004 , current epoch train loss:  0.001610182924196124\n","Epoch:  9005 , current epoch train loss:  0.0015807764139026403\n","Epoch:  9006 , current epoch train loss:  0.0016570654697716236\n","Epoch:  9007 , current epoch train loss:  0.001596995978616178\n","Epoch:  9008 , current epoch train loss:  0.001672983169555664\n","Epoch:  9009 , current epoch train loss:  0.0015731950988993049\n","Epoch:  9010 , current epoch train loss:  0.001626912271603942\n","Epoch:  9011 , current epoch train loss:  0.001552392728626728\n","Epoch:  9012 , current epoch train loss:  0.001595471752807498\n","Epoch:  9013 , current epoch train loss:  0.0015816650120541453\n","Epoch:  9014 , current epoch train loss:  0.0015509834047406912\n","Epoch:  9015 , current epoch train loss:  0.001538082375191152\n","Epoch:  9016 , current epoch train loss:  0.0015071001835167408\n","Epoch:  9017 , current epoch train loss:  0.00157031265553087\n","Epoch:  9018 , current epoch train loss:  0.0015922649763524532\n","Epoch:  9019 , current epoch train loss:  0.0015368179883807898\n","Epoch:  9020 , current epoch train loss:  0.0015543336048722267\n","Epoch:  9021 , current epoch train loss:  0.0016234524082392454\n","Epoch:  9022 , current epoch train loss:  0.001571707776747644\n","Epoch:  9023 , current epoch train loss:  0.001531110843643546\n","Epoch:  9024 , current epoch train loss:  0.0015789091121405363\n","Epoch:  9025 , current epoch train loss:  0.0015847634058445692\n","Epoch:  9026 , current epoch train loss:  0.001584958634339273\n","Epoch:  9027 , current epoch train loss:  0.0015471488004550338\n","Epoch:  9028 , current epoch train loss:  0.0015847984468564391\n","Epoch:  9029 , current epoch train loss:  0.001575266825966537\n","Epoch:  9030 , current epoch train loss:  0.0015784160932525992\n","Epoch:  9031 , current epoch train loss:  0.0015563050983473659\n","Epoch:  9032 , current epoch train loss:  0.0015293385367840528\n","Epoch:  9033 , current epoch train loss:  0.0015925965271890163\n","Epoch:  9034 , current epoch train loss:  0.0015968384686857462\n","Epoch:  9035 , current epoch train loss:  0.0015349017921835184\n","Epoch:  9036 , current epoch train loss:  0.001573728397488594\n","Epoch:  9037 , current epoch train loss:  0.0015990694519132376\n","Epoch:  9038 , current epoch train loss:  0.0016286650206893682\n","Epoch:  9039 , current epoch train loss:  0.0016465111402794719\n","Epoch:  9040 , current epoch train loss:  0.0015266890404745936\n","Epoch:  9041 , current epoch train loss:  0.0015367129817605019\n","Epoch:  9042 , current epoch train loss:  0.0015902373706921935\n","Epoch:  9043 , current epoch train loss:  0.0015828628093004227\n","Epoch:  9044 , current epoch train loss:  0.0015432173386216164\n","Epoch:  9045 , current epoch train loss:  0.0015531210228800774\n","Epoch:  9046 , current epoch train loss:  0.0015472405357286334\n","Epoch:  9047 , current epoch train loss:  0.0015741509851068258\n","Epoch:  9048 , current epoch train loss:  0.001564712030813098\n","Epoch:  9049 , current epoch train loss:  0.0015692728338763118\n","Epoch:  9050 , current epoch train loss:  0.0016951028956100345\n","Epoch:  9051 , current epoch train loss:  0.0015632826834917068\n","Epoch:  9052 , current epoch train loss:  0.00163846998475492\n","Epoch:  9053 , current epoch train loss:  0.0016207396984100342\n","Epoch:  9054 , current epoch train loss:  0.0016069298144429922\n","Epoch:  9055 , current epoch train loss:  0.001572211505845189\n","Epoch:  9056 , current epoch train loss:  0.0015763395931571722\n","Epoch:  9057 , current epoch train loss:  0.0017059965757653117\n","Epoch:  9058 , current epoch train loss:  0.0015999870374798775\n","Epoch:  9059 , current epoch train loss:  0.0015647949185222387\n","Epoch:  9060 , current epoch train loss:  0.0016572395106777549\n","Epoch:  9061 , current epoch train loss:  0.0015328520676121116\n","Epoch:  9062 , current epoch train loss:  0.0015197157626971602\n","Epoch:  9063 , current epoch train loss:  0.0016418981831520796\n","Epoch:  9064 , current epoch train loss:  0.001553419861011207\n","Epoch:  9065 , current epoch train loss:  0.0015438490081578493\n","Epoch:  9066 , current epoch train loss:  0.0015716548077762127\n","Epoch:  9067 , current epoch train loss:  0.001554229180328548\n","Epoch:  9068 , current epoch train loss:  0.0015340157551690936\n","Epoch:  9069 , current epoch train loss:  0.0016290483763441443\n","Epoch:  9070 , current epoch train loss:  0.0015614454168826342\n","Epoch:  9071 , current epoch train loss:  0.0015520065790042281\n","Epoch:  9072 , current epoch train loss:  0.0015524321934208274\n","Epoch:  9073 , current epoch train loss:  0.0015461116563528776\n","Epoch:  9074 , current epoch train loss:  0.0015774242347106338\n","Epoch:  9075 , current epoch train loss:  0.0016725001623854041\n","Epoch:  9076 , current epoch train loss:  0.0015478255227208138\n","Epoch:  9077 , current epoch train loss:  0.001704803085885942\n","Epoch:  9078 , current epoch train loss:  0.0016236389055848122\n","Epoch:  9079 , current epoch train loss:  0.001577934599481523\n","Epoch:  9080 , current epoch train loss:  0.001739267259836197\n","Epoch:  9081 , current epoch train loss:  0.0016543834935873747\n","Epoch:  9082 , current epoch train loss:  0.0015555564314126968\n","Epoch:  9083 , current epoch train loss:  0.001553740119561553\n","Epoch:  9084 , current epoch train loss:  0.0015797339146956801\n","Epoch:  9085 , current epoch train loss:  0.00154018122702837\n","Epoch:  9086 , current epoch train loss:  0.0015660831704735756\n","Epoch:  9087 , current epoch train loss:  0.0015426862519234419\n","Epoch:  9088 , current epoch train loss:  0.0015844146255403757\n","Epoch:  9089 , current epoch train loss:  0.0015338894445449114\n","Epoch:  9090 , current epoch train loss:  0.00160154327750206\n","Epoch:  9091 , current epoch train loss:  0.0015355255454778671\n","Epoch:  9092 , current epoch train loss:  0.0015575921861454844\n","Epoch:  9093 , current epoch train loss:  0.0015251091681420803\n","Epoch:  9094 , current epoch train loss:  0.001573002664372325\n","Epoch:  9095 , current epoch train loss:  0.0015475351829081774\n","Epoch:  9096 , current epoch train loss:  0.0016231117770075798\n","Epoch:  9097 , current epoch train loss:  0.0015827849274501204\n","Epoch:  9098 , current epoch train loss:  0.0016301012365147471\n","Epoch:  9099 , current epoch train loss:  0.001564766513183713\n","Epoch:  9100 , current epoch train loss:  0.0016028382815420628\n","Epoch:  9101 , current epoch train loss:  0.0015662666410207748\n","Epoch:  9102 , current epoch train loss:  0.0015455984976142645\n","Epoch:  9103 , current epoch train loss:  0.0015401188284158707\n","Epoch:  9104 , current epoch train loss:  0.001573280431330204\n","Epoch:  9105 , current epoch train loss:  0.001545206643640995\n","Epoch:  9106 , current epoch train loss:  0.0015641123754903674\n","Epoch:  9107 , current epoch train loss:  0.0015478553250432014\n","Epoch:  9108 , current epoch train loss:  0.0015614163130521774\n","Epoch:  9109 , current epoch train loss:  0.001829773187637329\n","Epoch:  9110 , current epoch train loss:  0.00154701282735914\n","Epoch:  9111 , current epoch train loss:  0.0015563899651169777\n","Epoch:  9112 , current epoch train loss:  0.0015455957036465406\n","Epoch:  9113 , current epoch train loss:  0.0016001234762370586\n","Epoch:  9114 , current epoch train loss:  0.0015412343200296164\n","Epoch:  9115 , current epoch train loss:  0.0015749078011140227\n","Epoch:  9116 , current epoch train loss:  0.0015696989139541984\n","Epoch:  9117 , current epoch train loss:  0.0015347907319664955\n","Epoch:  9118 , current epoch train loss:  0.001546236453577876\n","Epoch:  9119 , current epoch train loss:  0.0015571190742775798\n","Epoch:  9120 , current epoch train loss:  0.001592270447872579\n","Epoch:  9121 , current epoch train loss:  0.0015590228140354156\n","Epoch:  9122 , current epoch train loss:  0.001527569955214858\n","Epoch:  9123 , current epoch train loss:  0.0015406871680170298\n","Epoch:  9124 , current epoch train loss:  0.0015359580283984542\n","Epoch:  9125 , current epoch train loss:  0.0015794811770319939\n","Epoch:  9126 , current epoch train loss:  0.0015591271221637726\n","Epoch:  9127 , current epoch train loss:  0.0015632262220606208\n","Epoch:  9128 , current epoch train loss:  0.001567192724905908\n","Epoch:  9129 , current epoch train loss:  0.0015541436150670052\n","Epoch:  9130 , current epoch train loss:  0.001575179398059845\n","Epoch:  9131 , current epoch train loss:  0.0016910190461203456\n","Epoch:  9132 , current epoch train loss:  0.0015425689052790403\n","Epoch:  9133 , current epoch train loss:  0.0015607364475727081\n","Epoch:  9134 , current epoch train loss:  0.0016955407336354256\n","Epoch:  9135 , current epoch train loss:  0.0015722957905381918\n","Epoch:  9136 , current epoch train loss:  0.001585927209816873\n","Epoch:  9137 , current epoch train loss:  0.0016013330314308405\n","Epoch:  9138 , current epoch train loss:  0.0015726069686934352\n","Epoch:  9139 , current epoch train loss:  0.0015998096205294132\n","Epoch:  9140 , current epoch train loss:  0.0016167748253792524\n","Epoch:  9141 , current epoch train loss:  0.0015388492029160261\n","Epoch:  9142 , current epoch train loss:  0.0015558151062577963\n","Epoch:  9143 , current epoch train loss:  0.0017103723948821425\n","Epoch:  9144 , current epoch train loss:  0.001575555419549346\n","Epoch:  9145 , current epoch train loss:  0.0017102435231208801\n","Epoch:  9146 , current epoch train loss:  0.00159168872050941\n","Epoch:  9147 , current epoch train loss:  0.001606658915989101\n","Epoch:  9148 , current epoch train loss:  0.0015709217404946685\n","Epoch:  9149 , current epoch train loss:  0.0015899247955530882\n","Epoch:  9150 , current epoch train loss:  0.0016096055041998625\n","Epoch:  9151 , current epoch train loss:  0.001667035510763526\n","Epoch:  9152 , current epoch train loss:  0.001649755984544754\n","Epoch:  9153 , current epoch train loss:  0.0020632902160286903\n","Epoch:  9154 , current epoch train loss:  0.0016795673873275518\n","Epoch:  9155 , current epoch train loss:  0.0015779727837070823\n","Epoch:  9156 , current epoch train loss:  0.0015770753379911184\n","Epoch:  9157 , current epoch train loss:  0.0015784671995788813\n","Epoch:  9158 , current epoch train loss:  0.0016344923060387373\n","Epoch:  9159 , current epoch train loss:  0.0016367253847420216\n","Epoch:  9160 , current epoch train loss:  0.001563994912430644\n","Epoch:  9161 , current epoch train loss:  0.0015609752153977752\n","Epoch:  9162 , current epoch train loss:  0.001595502719283104\n","Epoch:  9163 , current epoch train loss:  0.001530033303424716\n","Epoch:  9164 , current epoch train loss:  0.0015687847044318914\n","Epoch:  9165 , current epoch train loss:  0.001547550200484693\n","Epoch:  9166 , current epoch train loss:  0.0015388451283797622\n","Epoch:  9167 , current epoch train loss:  0.0015612086281180382\n","Epoch:  9168 , current epoch train loss:  0.0015690913423895836\n","Epoch:  9169 , current epoch train loss:  0.0016083247028291225\n","Epoch:  9170 , current epoch train loss:  0.0015449442435055971\n","Epoch:  9171 , current epoch train loss:  0.0015845163725316525\n","Epoch:  9172 , current epoch train loss:  0.0015528419753536582\n","Epoch:  9173 , current epoch train loss:  0.0015151327243074775\n","Epoch:  9174 , current epoch train loss:  0.0015512490645051003\n","Epoch:  9175 , current epoch train loss:  0.0015697082271799445\n","Epoch:  9176 , current epoch train loss:  0.0015947769861668348\n","Epoch:  9177 , current epoch train loss:  0.0015507688513025641\n","Epoch:  9178 , current epoch train loss:  0.001557683339342475\n","Epoch:  9179 , current epoch train loss:  0.0015600932529196143\n","Epoch:  9180 , current epoch train loss:  0.001531069865450263\n","Epoch:  9181 , current epoch train loss:  0.0015509955119341612\n","Epoch:  9182 , current epoch train loss:  0.0015257716877385974\n","Epoch:  9183 , current epoch train loss:  0.0015593043062835932\n","Epoch:  9184 , current epoch train loss:  0.0015882154693827033\n","Epoch:  9185 , current epoch train loss:  0.00155153451487422\n","Epoch:  9186 , current epoch train loss:  0.0015816115774214268\n","Epoch:  9187 , current epoch train loss:  0.001530850655399263\n","Epoch:  9188 , current epoch train loss:  0.0015366350999101996\n","Epoch:  9189 , current epoch train loss:  0.0016126249684020877\n","Epoch:  9190 , current epoch train loss:  0.0015732788015156984\n","Epoch:  9191 , current epoch train loss:  0.0016463588690385222\n","Epoch:  9192 , current epoch train loss:  0.0015431161737069488\n","Epoch:  9193 , current epoch train loss:  0.0015845553716644645\n","Epoch:  9194 , current epoch train loss:  0.0015706145204603672\n","Epoch:  9195 , current epoch train loss:  0.001598947448655963\n","Epoch:  9196 , current epoch train loss:  0.0015627959510311484\n","Epoch:  9197 , current epoch train loss:  0.001549359760247171\n","Epoch:  9198 , current epoch train loss:  0.001544271013699472\n","Epoch:  9199 , current epoch train loss:  0.0015216468600556254\n","Epoch:  9200 , current epoch train loss:  0.0015429339837282896\n","Epoch:  9201 , current epoch train loss:  0.0015723936958238482\n","Epoch:  9202 , current epoch train loss:  0.0015601868508383632\n","Epoch:  9203 , current epoch train loss:  0.0015429927734658122\n","Epoch:  9204 , current epoch train loss:  0.0015278859063982964\n","Epoch:  9205 , current epoch train loss:  0.0015463511226698756\n","Epoch:  9206 , current epoch train loss:  0.0015450925566256046\n","Epoch:  9207 , current epoch train loss:  0.0015693295281380415\n","Epoch:  9208 , current epoch train loss:  0.0015734699554741383\n","Epoch:  9209 , current epoch train loss:  0.0015256861224770546\n","Epoch:  9210 , current epoch train loss:  0.0015512306708842516\n","Epoch:  9211 , current epoch train loss:  0.0015387197490781546\n","Epoch:  9212 , current epoch train loss:  0.0015504583716392517\n","Epoch:  9213 , current epoch train loss:  0.0015705841360613704\n","Epoch:  9214 , current epoch train loss:  0.001586499041877687\n","Epoch:  9215 , current epoch train loss:  0.0015849664341658354\n","Epoch:  9216 , current epoch train loss:  0.0015400791307911277\n","Epoch:  9217 , current epoch train loss:  0.0016236946685239673\n","Epoch:  9218 , current epoch train loss:  0.0016115871258080006\n","Epoch:  9219 , current epoch train loss:  0.0015756450593471527\n","Epoch:  9220 , current epoch train loss:  0.0015515544218942523\n","Epoch:  9221 , current epoch train loss:  0.0015771219041198492\n","Epoch:  9222 , current epoch train loss:  0.00153746223077178\n","Epoch:  9223 , current epoch train loss:  0.0016166664427146316\n","Epoch:  9224 , current epoch train loss:  0.0015929639339447021\n","Epoch:  9225 , current epoch train loss:  0.001547784311696887\n","Epoch:  9226 , current epoch train loss:  0.001543151680380106\n","Epoch:  9227 , current epoch train loss:  0.0015417024260386825\n","Epoch:  9228 , current epoch train loss:  0.0015326379798352718\n","Epoch:  9229 , current epoch train loss:  0.001557759358547628\n","Epoch:  9230 , current epoch train loss:  0.0016072796424850821\n","Epoch:  9231 , current epoch train loss:  0.0015269627328962088\n","Epoch:  9232 , current epoch train loss:  0.001574215479195118\n","Epoch:  9233 , current epoch train loss:  0.0015935630071908236\n","Epoch:  9234 , current epoch train loss:  0.0015572162810713053\n","Epoch:  9235 , current epoch train loss:  0.0016210017492994666\n","Epoch:  9236 , current epoch train loss:  0.0015910047804936767\n","Epoch:  9237 , current epoch train loss:  0.0016639407258480787\n","Epoch:  9238 , current epoch train loss:  0.001560372649691999\n","Epoch:  9239 , current epoch train loss:  0.0015855112578719854\n","Epoch:  9240 , current epoch train loss:  0.0015691656153649092\n","Epoch:  9241 , current epoch train loss:  0.001565322163514793\n","Epoch:  9242 , current epoch train loss:  0.0015413364162668586\n","Epoch:  9243 , current epoch train loss:  0.0015335418283939362\n","Epoch:  9244 , current epoch train loss:  0.0015586938243359327\n","Epoch:  9245 , current epoch train loss:  0.001572529086843133\n","Epoch:  9246 , current epoch train loss:  0.0015495502157136798\n","Epoch:  9247 , current epoch train loss:  0.0015169333200901747\n","Epoch:  9248 , current epoch train loss:  0.0015748823061585426\n","Epoch:  9249 , current epoch train loss:  0.0016103708185255527\n","Epoch:  9250 , current epoch train loss:  0.0016210591420531273\n","Epoch:  9251 , current epoch train loss:  0.0015627531101927161\n","Epoch:  9252 , current epoch train loss:  0.0015784823335707188\n","Epoch:  9253 , current epoch train loss:  0.0015785963041707873\n","Epoch:  9254 , current epoch train loss:  0.0015803060960024595\n","Epoch:  9255 , current epoch train loss:  0.0016422495245933533\n","Epoch:  9256 , current epoch train loss:  0.0015368461608886719\n","Epoch:  9257 , current epoch train loss:  0.0015770227182656527\n","Epoch:  9258 , current epoch train loss:  0.0016844061901792884\n","Epoch:  9259 , current epoch train loss:  0.0015800024848431349\n","Epoch:  9260 , current epoch train loss:  0.001555729890242219\n","Epoch:  9261 , current epoch train loss:  0.0015497216954827309\n","Epoch:  9262 , current epoch train loss:  0.0016251576598733664\n","Epoch:  9263 , current epoch train loss:  0.0015563223278149962\n","Epoch:  9264 , current epoch train loss:  0.0015313614858314395\n","Epoch:  9265 , current epoch train loss:  0.0015533939003944397\n","Epoch:  9266 , current epoch train loss:  0.001539534772746265\n","Epoch:  9267 , current epoch train loss:  0.0016348976641893387\n","Epoch:  9268 , current epoch train loss:  0.0015524523332715034\n","Epoch:  9269 , current epoch train loss:  0.0015673693269491196\n","Epoch:  9270 , current epoch train loss:  0.0016021651681512594\n","Epoch:  9271 , current epoch train loss:  0.0015524144982919097\n","Epoch:  9272 , current epoch train loss:  0.0016040513291954994\n","Epoch:  9273 , current epoch train loss:  0.0016182281542569399\n","Epoch:  9274 , current epoch train loss:  0.0015576013829559088\n","Epoch:  9275 , current epoch train loss:  0.0016015343135222793\n","Epoch:  9276 , current epoch train loss:  0.0017089344328269362\n","Epoch:  9277 , current epoch train loss:  0.0015805144794285297\n","Epoch:  9278 , current epoch train loss:  0.0016425957437604666\n","Epoch:  9279 , current epoch train loss:  0.001620061113499105\n","Epoch:  9280 , current epoch train loss:  0.001528468681499362\n","Epoch:  9281 , current epoch train loss:  0.0015976963331922889\n","Epoch:  9282 , current epoch train loss:  0.0015425416640937328\n","Epoch:  9283 , current epoch train loss:  0.0015863630687817931\n","Epoch:  9284 , current epoch train loss:  0.0015451268991455436\n","Epoch:  9285 , current epoch train loss:  0.001553690992295742\n","Epoch:  9286 , current epoch train loss:  0.001545740757137537\n","Epoch:  9287 , current epoch train loss:  0.001583246048539877\n","Epoch:  9288 , current epoch train loss:  0.0015551092801615596\n","Epoch:  9289 , current epoch train loss:  0.001541588339023292\n","Epoch:  9290 , current epoch train loss:  0.0016077847685664892\n","Epoch:  9291 , current epoch train loss:  0.0015462053706869483\n","Epoch:  9292 , current epoch train loss:  0.0017118891701102257\n","Epoch:  9293 , current epoch train loss:  0.0015614385483786464\n","Epoch:  9294 , current epoch train loss:  0.0015197244938462973\n","Epoch:  9295 , current epoch train loss:  0.0018860329873859882\n","Epoch:  9296 , current epoch train loss:  0.001580578275024891\n","Epoch:  9297 , current epoch train loss:  0.001562413526698947\n","Epoch:  9298 , current epoch train loss:  0.001542733982205391\n","Epoch:  9299 , current epoch train loss:  0.001605312223546207\n","Epoch:  9300 , current epoch train loss:  0.0015613972209393978\n","Epoch:  9301 , current epoch train loss:  0.0016570223961025476\n","Epoch:  9302 , current epoch train loss:  0.0015346318250522017\n","Epoch:  9303 , current epoch train loss:  0.0015595383010804653\n","Epoch:  9304 , current epoch train loss:  0.00157843844499439\n","Epoch:  9305 , current epoch train loss:  0.0017232339596375823\n","Epoch:  9306 , current epoch train loss:  0.0015550418756902218\n","Epoch:  9307 , current epoch train loss:  0.0015852740034461021\n","Epoch:  9308 , current epoch train loss:  0.0017391392029821873\n","Epoch:  9309 , current epoch train loss:  0.0015191377606242895\n","Epoch:  9310 , current epoch train loss:  0.0016088843112811446\n","Epoch:  9311 , current epoch train loss:  0.0017257615691050887\n","Epoch:  9312 , current epoch train loss:  0.0016099605709314346\n","Epoch:  9313 , current epoch train loss:  0.00161498854868114\n","Epoch:  9314 , current epoch train loss:  0.001680296380072832\n","Epoch:  9315 , current epoch train loss:  0.0015479037538170815\n","Epoch:  9316 , current epoch train loss:  0.001555097522214055\n","Epoch:  9317 , current epoch train loss:  0.001601107302121818\n","Epoch:  9318 , current epoch train loss:  0.0016965735703706741\n","Epoch:  9319 , current epoch train loss:  0.0015793663915246725\n","Epoch:  9320 , current epoch train loss:  0.0015523897018283606\n","Epoch:  9321 , current epoch train loss:  0.001541884965263307\n","Epoch:  9322 , current epoch train loss:  0.0015310286544263363\n","Epoch:  9323 , current epoch train loss:  0.0015830633928999305\n","Epoch:  9324 , current epoch train loss:  0.0015184258809313178\n","Epoch:  9325 , current epoch train loss:  0.001587249687872827\n","Epoch:  9326 , current epoch train loss:  0.0015469512436538935\n","Epoch:  9327 , current epoch train loss:  0.0015487750060856342\n","Epoch:  9328 , current epoch train loss:  0.00157186109572649\n","Epoch:  9329 , current epoch train loss:  0.001621448784135282\n","Epoch:  9330 , current epoch train loss:  0.0015431968495249748\n","Epoch:  9331 , current epoch train loss:  0.0015982575714588165\n","Epoch:  9332 , current epoch train loss:  0.001604375778697431\n","Epoch:  9333 , current epoch train loss:  0.0016592766623944044\n","Epoch:  9334 , current epoch train loss:  0.0016252815257757902\n","Epoch:  9335 , current epoch train loss:  0.0015966793289408088\n","Epoch:  9336 , current epoch train loss:  0.0015693072928115726\n","Epoch:  9337 , current epoch train loss:  0.0015622235368937254\n","Epoch:  9338 , current epoch train loss:  0.0015969447558745742\n","Epoch:  9339 , current epoch train loss:  0.0015954261180013418\n","Epoch:  9340 , current epoch train loss:  0.0016216009389609098\n","Epoch:  9341 , current epoch train loss:  0.0016302173025906086\n","Epoch:  9342 , current epoch train loss:  0.0015669515123590827\n","Epoch:  9343 , current epoch train loss:  0.0015300901141017675\n","Epoch:  9344 , current epoch train loss:  0.0015824971487745643\n","Epoch:  9345 , current epoch train loss:  0.0015567755326628685\n","Epoch:  9346 , current epoch train loss:  0.0015681203221902251\n","Epoch:  9347 , current epoch train loss:  0.0015634311130270362\n","Epoch:  9348 , current epoch train loss:  0.0015342140104621649\n","Epoch:  9349 , current epoch train loss:  0.0015567836817353964\n","Epoch:  9350 , current epoch train loss:  0.0016087319236248732\n","Epoch:  9351 , current epoch train loss:  0.0016347961500287056\n","Epoch:  9352 , current epoch train loss:  0.001530298381112516\n","Epoch:  9353 , current epoch train loss:  0.0015934424009174109\n","Epoch:  9354 , current epoch train loss:  0.0016094426391646266\n","Epoch:  9355 , current epoch train loss:  0.0015815645456314087\n","Epoch:  9356 , current epoch train loss:  0.0015623949002474546\n","Epoch:  9357 , current epoch train loss:  0.0015654315939173102\n","Epoch:  9358 , current epoch train loss:  0.001554875518195331\n","Epoch:  9359 , current epoch train loss:  0.0015485198237001896\n","Epoch:  9360 , current epoch train loss:  0.001586809055879712\n","Epoch:  9361 , current epoch train loss:  0.0015551226679235697\n","Epoch:  9362 , current epoch train loss:  0.001544840051792562\n","Epoch:  9363 , current epoch train loss:  0.00154835544526577\n","Epoch:  9364 , current epoch train loss:  0.0015569174429401755\n","Epoch:  9365 , current epoch train loss:  0.001597669324837625\n","Epoch:  9366 , current epoch train loss:  0.0015442766016349196\n","Epoch:  9367 , current epoch train loss:  0.0015545464120805264\n","Epoch:  9368 , current epoch train loss:  0.0015310775488615036\n","Epoch:  9369 , current epoch train loss:  0.0015811771154403687\n","Epoch:  9370 , current epoch train loss:  0.0016007678350433707\n","Epoch:  9371 , current epoch train loss:  0.0015434905653819442\n","Epoch:  9372 , current epoch train loss:  0.0015445187455043197\n","Epoch:  9373 , current epoch train loss:  0.001590109197422862\n","Epoch:  9374 , current epoch train loss:  0.0015727690188214183\n","Epoch:  9375 , current epoch train loss:  0.0015642208745703101\n","Epoch:  9376 , current epoch train loss:  0.0015332428738474846\n","Epoch:  9377 , current epoch train loss:  0.0015404729638248682\n","Epoch:  9378 , current epoch train loss:  0.001564929960295558\n","Epoch:  9379 , current epoch train loss:  0.0015253438614308834\n","Epoch:  9380 , current epoch train loss:  0.001569713931530714\n","Epoch:  9381 , current epoch train loss:  0.001554227783344686\n","Epoch:  9382 , current epoch train loss:  0.001588534447364509\n","Epoch:  9383 , current epoch train loss:  0.0015462972223758698\n","Epoch:  9384 , current epoch train loss:  0.0015955227427184582\n","Epoch:  9385 , current epoch train loss:  0.001558714546263218\n","Epoch:  9386 , current epoch train loss:  0.001537475036457181\n","Epoch:  9387 , current epoch train loss:  0.001592092914506793\n","Epoch:  9388 , current epoch train loss:  0.0015634311130270362\n","Epoch:  9389 , current epoch train loss:  0.0015581881161779165\n","Epoch:  9390 , current epoch train loss:  0.0015753570478409529\n","Epoch:  9391 , current epoch train loss:  0.0015417132526636124\n","Epoch:  9392 , current epoch train loss:  0.0015449586790055037\n","Epoch:  9393 , current epoch train loss:  0.0016419183230027556\n","Epoch:  9394 , current epoch train loss:  0.0015634777955710888\n","Epoch:  9395 , current epoch train loss:  0.0015840035630390048\n","Epoch:  9396 , current epoch train loss:  0.0015403395518660545\n","Epoch:  9397 , current epoch train loss:  0.0015721939271315932\n","Epoch:  9398 , current epoch train loss:  0.0015458669513463974\n","Epoch:  9399 , current epoch train loss:  0.0015544494381174445\n","Epoch:  9400 , current epoch train loss:  0.0015796482330188155\n","Epoch:  9401 , current epoch train loss:  0.0015695374459028244\n","Epoch:  9402 , current epoch train loss:  0.0015517458086833358\n","Epoch:  9403 , current epoch train loss:  0.0015167733654379845\n","Epoch:  9404 , current epoch train loss:  0.0015252893790602684\n","Epoch:  9405 , current epoch train loss:  0.0016655083745718002\n","Epoch:  9406 , current epoch train loss:  0.001544565660879016\n","Epoch:  9407 , current epoch train loss:  0.0016646994045004249\n","Epoch:  9408 , current epoch train loss:  0.0015976758440956473\n","Epoch:  9409 , current epoch train loss:  0.0015484326286241412\n","Epoch:  9410 , current epoch train loss:  0.0015374904032796621\n","Epoch:  9411 , current epoch train loss:  0.0015469094505533576\n","Epoch:  9412 , current epoch train loss:  0.0015888817142695189\n","Epoch:  9413 , current epoch train loss:  0.0015561841428279877\n","Epoch:  9414 , current epoch train loss:  0.0015869778580963612\n","Epoch:  9415 , current epoch train loss:  0.0016469135880470276\n","Epoch:  9416 , current epoch train loss:  0.001665125135332346\n","Epoch:  9417 , current epoch train loss:  0.001529904780909419\n","Epoch:  9418 , current epoch train loss:  0.0016267579048871994\n","Epoch:  9419 , current epoch train loss:  0.001615307293832302\n","Epoch:  9420 , current epoch train loss:  0.0016044906806200743\n","Epoch:  9421 , current epoch train loss:  0.001529505243524909\n","Epoch:  9422 , current epoch train loss:  0.0016737831756472588\n","Epoch:  9423 , current epoch train loss:  0.0015384538564831018\n","Epoch:  9424 , current epoch train loss:  0.0015686078695580363\n","Epoch:  9425 , current epoch train loss:  0.0015821594279259443\n","Epoch:  9426 , current epoch train loss:  0.0015898537822067738\n","Epoch:  9427 , current epoch train loss:  0.0016521121142432094\n","Epoch:  9428 , current epoch train loss:  0.0016224195715039968\n","Epoch:  9429 , current epoch train loss:  0.0017544371075928211\n","Epoch:  9430 , current epoch train loss:  0.0015672063454985619\n","Epoch:  9431 , current epoch train loss:  0.0015461414586752653\n","Epoch:  9432 , current epoch train loss:  0.0015744076808914542\n","Epoch:  9433 , current epoch train loss:  0.0016209817258641124\n","Epoch:  9434 , current epoch train loss:  0.001562965800985694\n","Epoch:  9435 , current epoch train loss:  0.0015629184199497104\n","Epoch:  9436 , current epoch train loss:  0.001570319291204214\n","Epoch:  9437 , current epoch train loss:  0.0015502943424507976\n","Epoch:  9438 , current epoch train loss:  0.0015666165854781866\n","Epoch:  9439 , current epoch train loss:  0.0016488537658005953\n","Epoch:  9440 , current epoch train loss:  0.0015799052780494094\n","Epoch:  9441 , current epoch train loss:  0.0015572390984743834\n","Epoch:  9442 , current epoch train loss:  0.001544029451906681\n","Epoch:  9443 , current epoch train loss:  0.0016089268028736115\n","Epoch:  9444 , current epoch train loss:  0.0015554357087239623\n","Epoch:  9445 , current epoch train loss:  0.001558686955831945\n","Epoch:  9446 , current epoch train loss:  0.0015266452683135867\n","Epoch:  9447 , current epoch train loss:  0.0015707267448306084\n","Epoch:  9448 , current epoch train loss:  0.001550331711769104\n","Epoch:  9449 , current epoch train loss:  0.0015545652713626623\n","Epoch:  9450 , current epoch train loss:  0.0015937576536089182\n","Epoch:  9451 , current epoch train loss:  0.0015637539327144623\n","Epoch:  9452 , current epoch train loss:  0.0015493262326344848\n","Epoch:  9453 , current epoch train loss:  0.0015818437095731497\n","Epoch:  9454 , current epoch train loss:  0.0015347687294706702\n","Epoch:  9455 , current epoch train loss:  0.0015407183673232794\n","Epoch:  9456 , current epoch train loss:  0.001526786363683641\n","Epoch:  9457 , current epoch train loss:  0.0016638990491628647\n","Epoch:  9458 , current epoch train loss:  0.0017056073993444443\n","Epoch:  9459 , current epoch train loss:  0.001543231657706201\n","Epoch:  9460 , current epoch train loss:  0.0015900356229394674\n","Epoch:  9461 , current epoch train loss:  0.0015605081571266055\n","Epoch:  9462 , current epoch train loss:  0.0016087652184069157\n","Epoch:  9463 , current epoch train loss:  0.001532020978629589\n","Epoch:  9464 , current epoch train loss:  0.0015569006791338325\n","Epoch:  9465 , current epoch train loss:  0.001547045772895217\n","Epoch:  9466 , current epoch train loss:  0.001576083479449153\n","Epoch:  9467 , current epoch train loss:  0.0015335464850068092\n","Epoch:  9468 , current epoch train loss:  0.0015381304547190666\n","Epoch:  9469 , current epoch train loss:  0.0015217792242765427\n","Epoch:  9470 , current epoch train loss:  0.0015245135873556137\n","Epoch:  9471 , current epoch train loss:  0.0015336486976593733\n","Epoch:  9472 , current epoch train loss:  0.0016057202592492104\n","Epoch:  9473 , current epoch train loss:  0.0015689281281083822\n","Epoch:  9474 , current epoch train loss:  0.0015919035067781806\n","Epoch:  9475 , current epoch train loss:  0.0015582095365971327\n","Epoch:  9476 , current epoch train loss:  0.001587174367159605\n","Epoch:  9477 , current epoch train loss:  0.0016185477143153548\n","Epoch:  9478 , current epoch train loss:  0.0015435470268130302\n","Epoch:  9479 , current epoch train loss:  0.0016408109804615378\n","Epoch:  9480 , current epoch train loss:  0.0015746456338092685\n","Epoch:  9481 , current epoch train loss:  0.0015344558050855994\n","Epoch:  9482 , current epoch train loss:  0.0015295743942260742\n","Epoch:  9483 , current epoch train loss:  0.0015685437247157097\n","Epoch:  9484 , current epoch train loss:  0.0015585547080263495\n","Epoch:  9485 , current epoch train loss:  0.001637949957512319\n","Epoch:  9486 , current epoch train loss:  0.001576610840857029\n","Epoch:  9487 , current epoch train loss:  0.0015499263536185026\n","Epoch:  9488 , current epoch train loss:  0.001548584084957838\n","Epoch:  9489 , current epoch train loss:  0.0015653222799301147\n","Epoch:  9490 , current epoch train loss:  0.0015543550252914429\n","Epoch:  9491 , current epoch train loss:  0.00152978440746665\n","Epoch:  9492 , current epoch train loss:  0.001559171243570745\n","Epoch:  9493 , current epoch train loss:  0.0015833748038858175\n","Epoch:  9494 , current epoch train loss:  0.0015074028633534908\n","Epoch:  9495 , current epoch train loss:  0.0016358005814254284\n","Epoch:  9496 , current epoch train loss:  0.0015567096415907145\n","Epoch:  9497 , current epoch train loss:  0.0015752640319988132\n","Epoch:  9498 , current epoch train loss:  0.0015537042636424303\n","Epoch:  9499 , current epoch train loss:  0.0015619223704561591\n","Epoch:  9500 , current epoch train loss:  0.0016637244261801243\n","Epoch:  9501 , current epoch train loss:  0.0016346387565135956\n","Epoch:  9502 , current epoch train loss:  0.0015504881739616394\n","Epoch:  9503 , current epoch train loss:  0.001561180455610156\n","Epoch:  9504 , current epoch train loss:  0.001565655111335218\n","Epoch:  9505 , current epoch train loss:  0.0015198346227407455\n","Epoch:  9506 , current epoch train loss:  0.0015872608637437224\n","Epoch:  9507 , current epoch train loss:  0.0016277262475341558\n","Epoch:  9508 , current epoch train loss:  0.00155460590030998\n","Epoch:  9509 , current epoch train loss:  0.0015548698138445616\n","Epoch:  9510 , current epoch train loss:  0.0015908535569906235\n","Epoch:  9511 , current epoch train loss:  0.0017538615502417088\n","Epoch:  9512 , current epoch train loss:  0.0015256928745657206\n","Epoch:  9513 , current epoch train loss:  0.0015397438546642661\n","Epoch:  9514 , current epoch train loss:  0.0015914689283818007\n","Epoch:  9515 , current epoch train loss:  0.0015711471205577254\n","Epoch:  9516 , current epoch train loss:  0.0015740005765110254\n","Epoch:  9517 , current epoch train loss:  0.001583102042786777\n","Epoch:  9518 , current epoch train loss:  0.0015499952714890242\n","Epoch:  9519 , current epoch train loss:  0.0015282451640814543\n","Epoch:  9520 , current epoch train loss:  0.001612459309399128\n","Epoch:  9521 , current epoch train loss:  0.0015242769150063396\n","Epoch:  9522 , current epoch train loss:  0.0015221027424558997\n","Epoch:  9523 , current epoch train loss:  0.001543162390589714\n","Epoch:  9524 , current epoch train loss:  0.0015398365212604403\n","Epoch:  9525 , current epoch train loss:  0.0015313391340896487\n","Epoch:  9526 , current epoch train loss:  0.0015417819377034903\n","Epoch:  9527 , current epoch train loss:  0.0016542829107493162\n","Epoch:  9528 , current epoch train loss:  0.0015626742970198393\n","Epoch:  9529 , current epoch train loss:  0.0015823566354811192\n","Epoch:  9530 , current epoch train loss:  0.0015739848604425788\n","Epoch:  9531 , current epoch train loss:  0.0015701608499512076\n","Epoch:  9532 , current epoch train loss:  0.0015583363128826022\n","Epoch:  9533 , current epoch train loss:  0.0015383524587377906\n","Epoch:  9534 , current epoch train loss:  0.0015606399392709136\n","Epoch:  9535 , current epoch train loss:  0.0015653762966394424\n","Epoch:  9536 , current epoch train loss:  0.0015467812772840261\n","Epoch:  9537 , current epoch train loss:  0.001618312089703977\n","Epoch:  9538 , current epoch train loss:  0.0015535000711679459\n","Epoch:  9539 , current epoch train loss:  0.0015495936386287212\n","Epoch:  9540 , current epoch train loss:  0.001548896892927587\n","Epoch:  9541 , current epoch train loss:  0.001558908959850669\n","Epoch:  9542 , current epoch train loss:  0.0015890853246673942\n","Epoch:  9543 , current epoch train loss:  0.0016609129961580038\n","Epoch:  9544 , current epoch train loss:  0.0015727870631963015\n","Epoch:  9545 , current epoch train loss:  0.001544696046039462\n","Epoch:  9546 , current epoch train loss:  0.0015696247573941946\n","Epoch:  9547 , current epoch train loss:  0.0015514404512941837\n","Epoch:  9548 , current epoch train loss:  0.0017112576169893146\n","Epoch:  9549 , current epoch train loss:  0.001571856439113617\n","Epoch:  9550 , current epoch train loss:  0.001580038107931614\n","Epoch:  9551 , current epoch train loss:  0.0015374708455055952\n","Epoch:  9552 , current epoch train loss:  0.0016189783345907927\n","Epoch:  9553 , current epoch train loss:  0.001575986621901393\n","Epoch:  9554 , current epoch train loss:  0.0015764825511723757\n","Epoch:  9555 , current epoch train loss:  0.001550617627799511\n","Epoch:  9556 , current epoch train loss:  0.0015595778822898865\n","Epoch:  9557 , current epoch train loss:  0.001553023001179099\n","Epoch:  9558 , current epoch train loss:  0.0015357795637100935\n","Epoch:  9559 , current epoch train loss:  0.0015449877828359604\n","Epoch:  9560 , current epoch train loss:  0.001546126208268106\n","Epoch:  9561 , current epoch train loss:  0.0015375785296782851\n","Epoch:  9562 , current epoch train loss:  0.0015411411877721548\n","Epoch:  9563 , current epoch train loss:  0.0015593718271702528\n","Epoch:  9564 , current epoch train loss:  0.00155811314471066\n","Epoch:  9565 , current epoch train loss:  0.0015946630155667663\n","Epoch:  9566 , current epoch train loss:  0.0015594352735206485\n","Epoch:  9567 , current epoch train loss:  0.0016182869439944625\n","Epoch:  9568 , current epoch train loss:  0.0016148060094565153\n","Epoch:  9569 , current epoch train loss:  0.0015540551394224167\n","Epoch:  9570 , current epoch train loss:  0.0015819629188627005\n","Epoch:  9571 , current epoch train loss:  0.0015442890580743551\n","Epoch:  9572 , current epoch train loss:  0.001570611959323287\n","Epoch:  9573 , current epoch train loss:  0.001559912576340139\n","Epoch:  9574 , current epoch train loss:  0.001535831717774272\n","Epoch:  9575 , current epoch train loss:  0.0015609830152243376\n","Epoch:  9576 , current epoch train loss:  0.0015507558127865195\n","Epoch:  9577 , current epoch train loss:  0.0015558901941403747\n","Epoch:  9578 , current epoch train loss:  0.0015477933920919895\n","Epoch:  9579 , current epoch train loss:  0.0015204008668661118\n","Epoch:  9580 , current epoch train loss:  0.0015451281797140837\n","Epoch:  9581 , current epoch train loss:  0.001553425332531333\n","Epoch:  9582 , current epoch train loss:  0.0015686986735090613\n","Epoch:  9583 , current epoch train loss:  0.0015967485960572958\n","Epoch:  9584 , current epoch train loss:  0.0016331844963133335\n","Epoch:  9585 , current epoch train loss:  0.00152989337220788\n","Epoch:  9586 , current epoch train loss:  0.001554580288939178\n","Epoch:  9587 , current epoch train loss:  0.0015527730574831367\n","Epoch:  9588 , current epoch train loss:  0.0015364761929959059\n","Epoch:  9589 , current epoch train loss:  0.001625737058930099\n","Epoch:  9590 , current epoch train loss:  0.001564252539537847\n","Epoch:  9591 , current epoch train loss:  0.0015823254361748695\n","Epoch:  9592 , current epoch train loss:  0.0015592256095260382\n","Epoch:  9593 , current epoch train loss:  0.0016080888453871012\n","Epoch:  9594 , current epoch train loss:  0.0015072723617777228\n","Epoch:  9595 , current epoch train loss:  0.0015887171030044556\n","Epoch:  9596 , current epoch train loss:  0.0015650729183107615\n","Epoch:  9597 , current epoch train loss:  0.0015207405667752028\n","Epoch:  9598 , current epoch train loss:  0.0016545271500945091\n","Epoch:  9599 , current epoch train loss:  0.0015339370584115386\n","Epoch:  9600 , current epoch train loss:  0.0015461900038644671\n","Epoch:  9601 , current epoch train loss:  0.0015404524747282267\n","Epoch:  9602 , current epoch train loss:  0.001611673622392118\n","Epoch:  9603 , current epoch train loss:  0.0016874363645911217\n","Epoch:  9604 , current epoch train loss:  0.0015837324317544699\n","Epoch:  9605 , current epoch train loss:  0.0017094237264245749\n","Epoch:  9606 , current epoch train loss:  0.0016390811651945114\n","Epoch:  9607 , current epoch train loss:  0.0015792269259691238\n","Epoch:  9608 , current epoch train loss:  0.0015434519154950976\n","Epoch:  9609 , current epoch train loss:  0.0015599599573761225\n","Epoch:  9610 , current epoch train loss:  0.001553067471832037\n","Epoch:  9611 , current epoch train loss:  0.0016016201116144657\n","Epoch:  9612 , current epoch train loss:  0.0017146760364994407\n","Epoch:  9613 , current epoch train loss:  0.0015831191558390856\n","Epoch:  9614 , current epoch train loss:  0.0016890957485884428\n","Epoch:  9615 , current epoch train loss:  0.001649958430789411\n","Epoch:  9616 , current epoch train loss:  0.0015527151990681887\n","Epoch:  9617 , current epoch train loss:  0.0015492424136027694\n","Epoch:  9618 , current epoch train loss:  0.0015479795401915908\n","Epoch:  9619 , current epoch train loss:  0.0015578590100631118\n","Epoch:  9620 , current epoch train loss:  0.0015588253736495972\n","Epoch:  9621 , current epoch train loss:  0.0015304782427847385\n","Epoch:  9622 , current epoch train loss:  0.001611520885489881\n","Epoch:  9623 , current epoch train loss:  0.0015700387302786112\n","Epoch:  9624 , current epoch train loss:  0.0015613138675689697\n","Epoch:  9625 , current epoch train loss:  0.0015467891935259104\n","Epoch:  9626 , current epoch train loss:  0.0015380693366751075\n","Epoch:  9627 , current epoch train loss:  0.0015408366452902555\n","Epoch:  9628 , current epoch train loss:  0.00157870480325073\n","Epoch:  9629 , current epoch train loss:  0.0016945548122748733\n","Epoch:  9630 , current epoch train loss:  0.0015193407889455557\n","Epoch:  9631 , current epoch train loss:  0.0016173736657947302\n","Epoch:  9632 , current epoch train loss:  0.0015955943381413817\n","Epoch:  9633 , current epoch train loss:  0.0015207815449684858\n","Epoch:  9634 , current epoch train loss:  0.0015420684358105063\n","Epoch:  9635 , current epoch train loss:  0.0015354091301560402\n","Epoch:  9636 , current epoch train loss:  0.001797382952645421\n","Epoch:  9637 , current epoch train loss:  0.0018295118352398276\n","Epoch:  9638 , current epoch train loss:  0.0015489724464714527\n","Epoch:  9639 , current epoch train loss:  0.0015436813700944185\n","Epoch:  9640 , current epoch train loss:  0.0015449540223926306\n","Epoch:  9641 , current epoch train loss:  0.0015296245692297816\n","Epoch:  9642 , current epoch train loss:  0.0015501657035201788\n","Epoch:  9643 , current epoch train loss:  0.0016090746503323317\n","Epoch:  9644 , current epoch train loss:  0.0016380890738219023\n","Epoch:  9645 , current epoch train loss:  0.0015737859066575766\n","Epoch:  9646 , current epoch train loss:  0.0015729002188891172\n","Epoch:  9647 , current epoch train loss:  0.0015489606885239482\n","Epoch:  9648 , current epoch train loss:  0.0015257144114002585\n","Epoch:  9649 , current epoch train loss:  0.0015542968176305294\n","Epoch:  9650 , current epoch train loss:  0.0015998638700693846\n","Epoch:  9651 , current epoch train loss:  0.0016809857916086912\n","Epoch:  9652 , current epoch train loss:  0.0016068171244114637\n","Epoch:  9653 , current epoch train loss:  0.0015906275948509574\n","Epoch:  9654 , current epoch train loss:  0.001559107331559062\n","Epoch:  9655 , current epoch train loss:  0.001593841123394668\n","Epoch:  9656 , current epoch train loss:  0.0016267220489680767\n","Epoch:  9657 , current epoch train loss:  0.0015156905865296721\n","Epoch:  9658 , current epoch train loss:  0.0015559907769784331\n","Epoch:  9659 , current epoch train loss:  0.001587166916579008\n","Epoch:  9660 , current epoch train loss:  0.0015405848389491439\n","Epoch:  9661 , current epoch train loss:  0.0015520792221650481\n","Epoch:  9662 , current epoch train loss:  0.0015753278275951743\n","Epoch:  9663 , current epoch train loss:  0.0015375444199889898\n","Epoch:  9664 , current epoch train loss:  0.0015346675645560026\n","Epoch:  9665 , current epoch train loss:  0.0015576193109154701\n","Epoch:  9666 , current epoch train loss:  0.0016223754500970244\n","Epoch:  9667 , current epoch train loss:  0.0015246754046529531\n","Epoch:  9668 , current epoch train loss:  0.0016435974976047873\n","Epoch:  9669 , current epoch train loss:  0.0015659861965104938\n","Epoch:  9670 , current epoch train loss:  0.0016007274389266968\n","Epoch:  9671 , current epoch train loss:  0.0015368073945865035\n","Epoch:  9672 , current epoch train loss:  0.0016369316726922989\n","Epoch:  9673 , current epoch train loss:  0.0015578565653413534\n","Epoch:  9674 , current epoch train loss:  0.001561876735650003\n","Epoch:  9675 , current epoch train loss:  0.0015611955896019936\n","Epoch:  9676 , current epoch train loss:  0.0015811038902029395\n","Epoch:  9677 , current epoch train loss:  0.0015502027235925198\n","Epoch:  9678 , current epoch train loss:  0.001576580572873354\n","Epoch:  9679 , current epoch train loss:  0.001526017440482974\n","Epoch:  9680 , current epoch train loss:  0.0016077643958851695\n","Epoch:  9681 , current epoch train loss:  0.0015391490887850523\n","Epoch:  9682 , current epoch train loss:  0.001536389347165823\n","Epoch:  9683 , current epoch train loss:  0.0015504797920584679\n","Epoch:  9684 , current epoch train loss:  0.0015410182531923056\n","Epoch:  9685 , current epoch train loss:  0.0015350519679486752\n","Epoch:  9686 , current epoch train loss:  0.0015177938621491194\n","Epoch:  9687 , current epoch train loss:  0.0015341751277446747\n","Epoch:  9688 , current epoch train loss:  0.0016112192533910275\n","Epoch:  9689 , current epoch train loss:  0.0015361616387963295\n","Epoch:  9690 , current epoch train loss:  0.001671433448791504\n","Epoch:  9691 , current epoch train loss:  0.0015515562845394015\n","Epoch:  9692 , current epoch train loss:  0.0017212159000337124\n","Epoch:  9693 , current epoch train loss:  0.0016791101079434156\n","Epoch:  9694 , current epoch train loss:  0.0015632628928869963\n","Epoch:  9695 , current epoch train loss:  0.0015335537027567625\n","Epoch:  9696 , current epoch train loss:  0.00168807792942971\n","Epoch:  9697 , current epoch train loss:  0.0015724010299891233\n","Epoch:  9698 , current epoch train loss:  0.0016041373601183295\n","Epoch:  9699 , current epoch train loss:  0.0015448378399014473\n","Epoch:  9700 , current epoch train loss:  0.0015897549455985427\n","Epoch:  9701 , current epoch train loss:  0.0015412699431180954\n","Epoch:  9702 , current epoch train loss:  0.001541093341074884\n","Epoch:  9703 , current epoch train loss:  0.001621199189685285\n","Epoch:  9704 , current epoch train loss:  0.0015786103904247284\n","Epoch:  9705 , current epoch train loss:  0.0017117004608735442\n","Epoch:  9706 , current epoch train loss:  0.0015884119784459472\n","Epoch:  9707 , current epoch train loss:  0.0015424450393766165\n","Epoch:  9708 , current epoch train loss:  0.001647437922656536\n","Epoch:  9709 , current epoch train loss:  0.001546784769743681\n","Epoch:  9710 , current epoch train loss:  0.0015538226580247283\n","Epoch:  9711 , current epoch train loss:  0.0016670981422066689\n","Epoch:  9712 , current epoch train loss:  0.001541182748042047\n","Epoch:  9713 , current epoch train loss:  0.0015361834084615111\n","Epoch:  9714 , current epoch train loss:  0.0017223702743649483\n","Epoch:  9715 , current epoch train loss:  0.0015129068633541465\n","Epoch:  9716 , current epoch train loss:  0.0015540013555437326\n","Epoch:  9717 , current epoch train loss:  0.001649142592214048\n","Epoch:  9718 , current epoch train loss:  0.0015314954798668623\n","Epoch:  9719 , current epoch train loss:  0.0015491846716031432\n","Epoch:  9720 , current epoch train loss:  0.0015620723133906722\n","Epoch:  9721 , current epoch train loss:  0.0015544912312179804\n","Epoch:  9722 , current epoch train loss:  0.0016993246972560883\n","Epoch:  9723 , current epoch train loss:  0.0016516509931534529\n","Epoch:  9724 , current epoch train loss:  0.001543632009997964\n","Epoch:  9725 , current epoch train loss:  0.0015425786841660738\n","Epoch:  9726 , current epoch train loss:  0.0015429516788572073\n","Epoch:  9727 , current epoch train loss:  0.0015107688959687948\n","Epoch:  9728 , current epoch train loss:  0.0015351813053712249\n","Epoch:  9729 , current epoch train loss:  0.0015436019748449326\n","Epoch:  9730 , current epoch train loss:  0.0015561943873763084\n","Epoch:  9731 , current epoch train loss:  0.0015406121965497732\n","Epoch:  9732 , current epoch train loss:  0.0015304809203371406\n","Epoch:  9733 , current epoch train loss:  0.0015546451322734356\n","Epoch:  9734 , current epoch train loss:  0.0015469184145331383\n","Epoch:  9735 , current epoch train loss:  0.0015607911627739668\n","Epoch:  9736 , current epoch train loss:  0.0016231521731242537\n","Epoch:  9737 , current epoch train loss:  0.001561129349283874\n","Epoch:  9738 , current epoch train loss:  0.0017230152152478695\n","Epoch:  9739 , current epoch train loss:  0.0016340769361704588\n","Epoch:  9740 , current epoch train loss:  0.001555082155391574\n","Epoch:  9741 , current epoch train loss:  0.0015811801422387362\n","Epoch:  9742 , current epoch train loss:  0.0015193566214293242\n","Epoch:  9743 , current epoch train loss:  0.0016018885653465986\n","Epoch:  9744 , current epoch train loss:  0.001575032016262412\n","Epoch:  9745 , current epoch train loss:  0.0015910289948806167\n","Epoch:  9746 , current epoch train loss:  0.0016139401122927666\n","Epoch:  9747 , current epoch train loss:  0.0015732781030237675\n","Epoch:  9748 , current epoch train loss:  0.0016782961320132017\n","Epoch:  9749 , current epoch train loss:  0.001646007876843214\n","Epoch:  9750 , current epoch train loss:  0.0015396424569189548\n","Epoch:  9751 , current epoch train loss:  0.0015202879440039396\n","Epoch:  9752 , current epoch train loss:  0.0015452232910320163\n","Epoch:  9753 , current epoch train loss:  0.0015411153435707092\n","Epoch:  9754 , current epoch train loss:  0.0015493188984692097\n","Epoch:  9755 , current epoch train loss:  0.0015269012656062841\n","Epoch:  9756 , current epoch train loss:  0.001550521468743682\n","Epoch:  9757 , current epoch train loss:  0.0015356455696746707\n","Epoch:  9758 , current epoch train loss:  0.0015192547580227256\n","Epoch:  9759 , current epoch train loss:  0.0015488804783672094\n","Epoch:  9760 , current epoch train loss:  0.0015776511281728745\n","Epoch:  9761 , current epoch train loss:  0.0015628457767888904\n","Epoch:  9762 , current epoch train loss:  0.0015573524869978428\n","Epoch:  9763 , current epoch train loss:  0.001552392146550119\n","Epoch:  9764 , current epoch train loss:  0.0015192953869700432\n","Epoch:  9765 , current epoch train loss:  0.0015355765353888273\n","Epoch:  9766 , current epoch train loss:  0.0016820419114083052\n","Epoch:  9767 , current epoch train loss:  0.0015746339922770858\n","Epoch:  9768 , current epoch train loss:  0.0015273508615791798\n","Epoch:  9769 , current epoch train loss:  0.001556009752675891\n","Epoch:  9770 , current epoch train loss:  0.0016901548951864243\n","Epoch:  9771 , current epoch train loss:  0.001548473141156137\n","Epoch:  9772 , current epoch train loss:  0.0015094842528924346\n","Epoch:  9773 , current epoch train loss:  0.0015408064937219024\n","Epoch:  9774 , current epoch train loss:  0.001540577970445156\n","Epoch:  9775 , current epoch train loss:  0.0015505425399169326\n","Epoch:  9776 , current epoch train loss:  0.0015519818989560008\n","Epoch:  9777 , current epoch train loss:  0.00162214576266706\n","Epoch:  9778 , current epoch train loss:  0.0016591529129073024\n","Epoch:  9779 , current epoch train loss:  0.0015519162407144904\n","Epoch:  9780 , current epoch train loss:  0.0016104299575090408\n","Epoch:  9781 , current epoch train loss:  0.001545633189380169\n","Epoch:  9782 , current epoch train loss:  0.0015430403873324394\n","Epoch:  9783 , current epoch train loss:  0.0015601497143507004\n","Epoch:  9784 , current epoch train loss:  0.0015681351069360971\n","Epoch:  9785 , current epoch train loss:  0.0016408663941547275\n","Epoch:  9786 , current epoch train loss:  0.0015215696766972542\n","Epoch:  9787 , current epoch train loss:  0.0015639004996046424\n","Epoch:  9788 , current epoch train loss:  0.001540454337373376\n","Epoch:  9789 , current epoch train loss:  0.0015723671531304717\n","Epoch:  9790 , current epoch train loss:  0.0015611769631505013\n","Epoch:  9791 , current epoch train loss:  0.001598129514604807\n","Epoch:  9792 , current epoch train loss:  0.0015396944945678115\n","Epoch:  9793 , current epoch train loss:  0.0015198083128780127\n","Epoch:  9794 , current epoch train loss:  0.0015658391639590263\n","Epoch:  9795 , current epoch train loss:  0.0015800572000443935\n","Epoch:  9796 , current epoch train loss:  0.0016910182312130928\n","Epoch:  9797 , current epoch train loss:  0.001522958162240684\n","Epoch:  9798 , current epoch train loss:  0.001535242423415184\n","Epoch:  9799 , current epoch train loss:  0.0015473803505301476\n","Epoch:  9800 , current epoch train loss:  0.0015488516073673964\n","Epoch:  9801 , current epoch train loss:  0.0015400238335132599\n","Epoch:  9802 , current epoch train loss:  0.001545170322060585\n","Epoch:  9803 , current epoch train loss:  0.0015554719138890505\n","Epoch:  9804 , current epoch train loss:  0.0015768862795084715\n","Epoch:  9805 , current epoch train loss:  0.0015628146938979626\n","Epoch:  9806 , current epoch train loss:  0.0017572895158082247\n","Epoch:  9807 , current epoch train loss:  0.0015625430969521403\n","Epoch:  9808 , current epoch train loss:  0.001537266536615789\n","Epoch:  9809 , current epoch train loss:  0.0016077586915344\n","Epoch:  9810 , current epoch train loss:  0.001537420554086566\n","Epoch:  9811 , current epoch train loss:  0.0015506133204326034\n","Epoch:  9812 , current epoch train loss:  0.0015593522693961859\n","Epoch:  9813 , current epoch train loss:  0.0015441132709383965\n","Epoch:  9814 , current epoch train loss:  0.0015618682373315096\n","Epoch:  9815 , current epoch train loss:  0.0015368228778243065\n","Epoch:  9816 , current epoch train loss:  0.0015508346259593964\n","Epoch:  9817 , current epoch train loss:  0.0015635614981874824\n","Epoch:  9818 , current epoch train loss:  0.0015416331589221954\n","Epoch:  9819 , current epoch train loss:  0.0015737900976091623\n","Epoch:  9820 , current epoch train loss:  0.0017407931154593825\n","Epoch:  9821 , current epoch train loss:  0.0015644007362425327\n","Epoch:  9822 , current epoch train loss:  0.0018186713568866253\n","Epoch:  9823 , current epoch train loss:  0.0015328869922086596\n","Epoch:  9824 , current epoch train loss:  0.0016800265293568373\n","Epoch:  9825 , current epoch train loss:  0.0016023879870772362\n","Epoch:  9826 , current epoch train loss:  0.0015506772324442863\n","Epoch:  9827 , current epoch train loss:  0.001576551585458219\n","Epoch:  9828 , current epoch train loss:  0.001567038125358522\n","Epoch:  9829 , current epoch train loss:  0.0015311383176594973\n","Epoch:  9830 , current epoch train loss:  0.0015668071573600173\n","Epoch:  9831 , current epoch train loss:  0.001513627590611577\n","Epoch:  9832 , current epoch train loss:  0.001574510708451271\n","Epoch:  9833 , current epoch train loss:  0.001537330448627472\n","Epoch:  9834 , current epoch train loss:  0.0015563307097181678\n","Epoch:  9835 , current epoch train loss:  0.0015688523417338729\n","Epoch:  9836 , current epoch train loss:  0.0015984385972842574\n","Epoch:  9837 , current epoch train loss:  0.0015850819181650877\n","Epoch:  9838 , current epoch train loss:  0.0015748078003525734\n","Epoch:  9839 , current epoch train loss:  0.0015466203913092613\n","Epoch:  9840 , current epoch train loss:  0.0015472987433895469\n","Epoch:  9841 , current epoch train loss:  0.001622525043785572\n","Epoch:  9842 , current epoch train loss:  0.0015289033763110638\n","Epoch:  9843 , current epoch train loss:  0.0016818440053611994\n","Epoch:  9844 , current epoch train loss:  0.0015695516485720873\n","Epoch:  9845 , current epoch train loss:  0.0015684388345107436\n","Epoch:  9846 , current epoch train loss:  0.0015420851996168494\n","Epoch:  9847 , current epoch train loss:  0.0015425964957103133\n","Epoch:  9848 , current epoch train loss:  0.0015697062481194735\n","Epoch:  9849 , current epoch train loss:  0.0015434185042977333\n","Epoch:  9850 , current epoch train loss:  0.0015381984412670135\n","Epoch:  9851 , current epoch train loss:  0.0015481521841138601\n","Epoch:  9852 , current epoch train loss:  0.001570728374645114\n","Epoch:  9853 , current epoch train loss:  0.0015610646223649383\n","Epoch:  9854 , current epoch train loss:  0.0015814276412129402\n","Epoch:  9855 , current epoch train loss:  0.0016146954149007797\n","Epoch:  9856 , current epoch train loss:  0.0015617189928889275\n","Epoch:  9857 , current epoch train loss:  0.0015275857876986265\n","Epoch:  9858 , current epoch train loss:  0.0015203662915155292\n","Epoch:  9859 , current epoch train loss:  0.0015570586547255516\n","Epoch:  9860 , current epoch train loss:  0.0015749537851661444\n","Epoch:  9861 , current epoch train loss:  0.001583604491315782\n","Epoch:  9862 , current epoch train loss:  0.001564538455568254\n","Epoch:  9863 , current epoch train loss:  0.0015748811420053244\n","Epoch:  9864 , current epoch train loss:  0.001568591920658946\n","Epoch:  9865 , current epoch train loss:  0.001565232640132308\n","Epoch:  9866 , current epoch train loss:  0.0015441093128174543\n","Epoch:  9867 , current epoch train loss:  0.0015251592267304659\n","Epoch:  9868 , current epoch train loss:  0.001586450613103807\n","Epoch:  9869 , current epoch train loss:  0.0016026233788579702\n","Epoch:  9870 , current epoch train loss:  0.00154746079351753\n","Epoch:  9871 , current epoch train loss:  0.0015841902932152152\n","Epoch:  9872 , current epoch train loss:  0.0016402441542595625\n","Epoch:  9873 , current epoch train loss:  0.0015595124568790197\n","Epoch:  9874 , current epoch train loss:  0.0016082597430795431\n","Epoch:  9875 , current epoch train loss:  0.0015495754778385162\n","Epoch:  9876 , current epoch train loss:  0.0015774404164403677\n","Epoch:  9877 , current epoch train loss:  0.0015375630464404821\n","Epoch:  9878 , current epoch train loss:  0.0015642427606508136\n","Epoch:  9879 , current epoch train loss:  0.00155577901750803\n","Epoch:  9880 , current epoch train loss:  0.001527632586658001\n","Epoch:  9881 , current epoch train loss:  0.0015382582787424326\n","Epoch:  9882 , current epoch train loss:  0.0015912419185042381\n","Epoch:  9883 , current epoch train loss:  0.0016009167302399874\n","Epoch:  9884 , current epoch train loss:  0.0015685189282521605\n","Epoch:  9885 , current epoch train loss:  0.0015943830367177725\n","Epoch:  9886 , current epoch train loss:  0.0015343243721872568\n","Epoch:  9887 , current epoch train loss:  0.0015232979785650969\n","Epoch:  9888 , current epoch train loss:  0.0015572775155305862\n","Epoch:  9889 , current epoch train loss:  0.0015655289171263576\n","Epoch:  9890 , current epoch train loss:  0.0016430660616606474\n","Epoch:  9891 , current epoch train loss:  0.0015485783806070685\n","Epoch:  9892 , current epoch train loss:  0.0015899548307061195\n","Epoch:  9893 , current epoch train loss:  0.0015304688131436706\n","Epoch:  9894 , current epoch train loss:  0.0015639289049431682\n","Epoch:  9895 , current epoch train loss:  0.0015287273563444614\n","Epoch:  9896 , current epoch train loss:  0.0015497015556320548\n","Epoch:  9897 , current epoch train loss:  0.0015612542629241943\n","Epoch:  9898 , current epoch train loss:  0.0015546409413218498\n","Epoch:  9899 , current epoch train loss:  0.0015358332311734557\n","Epoch:  9900 , current epoch train loss:  0.0015269125578925014\n","Epoch:  9901 , current epoch train loss:  0.0015505616320297122\n","Epoch:  9902 , current epoch train loss:  0.0015583591302856803\n","Epoch:  9903 , current epoch train loss:  0.0015638562617823482\n","Epoch:  9904 , current epoch train loss:  0.0015704641118645668\n","Epoch:  9905 , current epoch train loss:  0.001554984482936561\n","Epoch:  9906 , current epoch train loss:  0.0015398995019495487\n","Epoch:  9907 , current epoch train loss:  0.001556234317831695\n","Epoch:  9908 , current epoch train loss:  0.001608563936315477\n","Epoch:  9909 , current epoch train loss:  0.0017802560469135642\n","Epoch:  9910 , current epoch train loss:  0.0015125961508601904\n","Epoch:  9911 , current epoch train loss:  0.0015486623160541058\n","Epoch:  9912 , current epoch train loss:  0.0015908281784504652\n","Epoch:  9913 , current epoch train loss:  0.001617818488739431\n","Epoch:  9914 , current epoch train loss:  0.0015923915198072791\n","Epoch:  9915 , current epoch train loss:  0.001557673211209476\n","Epoch:  9916 , current epoch train loss:  0.001558066112920642\n","Epoch:  9917 , current epoch train loss:  0.0015362338162958622\n","Epoch:  9918 , current epoch train loss:  0.0015354629140347242\n","Epoch:  9919 , current epoch train loss:  0.001563341706059873\n","Epoch:  9920 , current epoch train loss:  0.0015703549142926931\n","Epoch:  9921 , current epoch train loss:  0.0015971967950463295\n","Epoch:  9922 , current epoch train loss:  0.0015562239568680525\n","Epoch:  9923 , current epoch train loss:  0.001556606381200254\n","Epoch:  9924 , current epoch train loss:  0.001569370273500681\n","Epoch:  9925 , current epoch train loss:  0.001549749867990613\n","Epoch:  9926 , current epoch train loss:  0.0016531486762687564\n","Epoch:  9927 , current epoch train loss:  0.001565534039400518\n","Epoch:  9928 , current epoch train loss:  0.0015540607273578644\n","Epoch:  9929 , current epoch train loss:  0.0015367807354778051\n","Epoch:  9930 , current epoch train loss:  0.0015183136565610766\n","Epoch:  9931 , current epoch train loss:  0.0015365548897534609\n","Epoch:  9932 , current epoch train loss:  0.0015662338119000196\n","Epoch:  9933 , current epoch train loss:  0.0015614008298143744\n","Epoch:  9934 , current epoch train loss:  0.0016496110474690795\n","Epoch:  9935 , current epoch train loss:  0.0016177594661712646\n","Epoch:  9936 , current epoch train loss:  0.0015391726046800613\n","Epoch:  9937 , current epoch train loss:  0.0015497652348130941\n","Epoch:  9938 , current epoch train loss:  0.0015434359665960073\n","Epoch:  9939 , current epoch train loss:  0.0017288308590650558\n","Epoch:  9940 , current epoch train loss:  0.0015181777998805046\n","Epoch:  9941 , current epoch train loss:  0.0015723556280136108\n","Epoch:  9942 , current epoch train loss:  0.0016013942658901215\n","Epoch:  9943 , current epoch train loss:  0.0015557322185486555\n","Epoch:  9944 , current epoch train loss:  0.0015165505465120077\n","Epoch:  9945 , current epoch train loss:  0.0016020478215068579\n","Epoch:  9946 , current epoch train loss:  0.0015836147358641028\n","Epoch:  9947 , current epoch train loss:  0.001542086130939424\n","Epoch:  9948 , current epoch train loss:  0.001565216458402574\n","Epoch:  9949 , current epoch train loss:  0.0015175755834206939\n","Epoch:  9950 , current epoch train loss:  0.0016568973660469055\n","Epoch:  9951 , current epoch train loss:  0.0015352300833910704\n","Epoch:  9952 , current epoch train loss:  0.0015476731350645423\n","Epoch:  9953 , current epoch train loss:  0.001569070853292942\n","Epoch:  9954 , current epoch train loss:  0.0015869972994551063\n","Epoch:  9955 , current epoch train loss:  0.0015112794935703278\n","Epoch:  9956 , current epoch train loss:  0.0015515906270593405\n","Epoch:  9957 , current epoch train loss:  0.0015807155286893249\n","Epoch:  9958 , current epoch train loss:  0.0015432432992383838\n","Epoch:  9959 , current epoch train loss:  0.0015948287909850478\n","Epoch:  9960 , current epoch train loss:  0.0015554779674857855\n","Epoch:  9961 , current epoch train loss:  0.0015793991042301059\n","Epoch:  9962 , current epoch train loss:  0.0015502891037613153\n","Epoch:  9963 , current epoch train loss:  0.001561079523526132\n","Epoch:  9964 , current epoch train loss:  0.0015430934727191925\n","Epoch:  9965 , current epoch train loss:  0.0015597919700667262\n","Epoch:  9966 , current epoch train loss:  0.0015623451909050345\n","Epoch:  9967 , current epoch train loss:  0.0015364873688668013\n","Epoch:  9968 , current epoch train loss:  0.0015221945941448212\n","Epoch:  9969 , current epoch train loss:  0.0015433377120643854\n","Epoch:  9970 , current epoch train loss:  0.0015590231632813811\n","Epoch:  9971 , current epoch train loss:  0.0015810535987839103\n","Epoch:  9972 , current epoch train loss:  0.001587900216691196\n","Epoch:  9973 , current epoch train loss:  0.0015260863583534956\n","Epoch:  9974 , current epoch train loss:  0.0015942256432026625\n","Epoch:  9975 , current epoch train loss:  0.0016028372338041663\n","Epoch:  9976 , current epoch train loss:  0.0015396340750157833\n","Epoch:  9977 , current epoch train loss:  0.0015866707544773817\n","Epoch:  9978 , current epoch train loss:  0.0015351362526416779\n","Epoch:  9979 , current epoch train loss:  0.0015716477064415812\n","Epoch:  9980 , current epoch train loss:  0.0015355086652562022\n","Epoch:  9981 , current epoch train loss:  0.0015698897186666727\n","Epoch:  9982 , current epoch train loss:  0.001530165784060955\n","Epoch:  9983 , current epoch train loss:  0.0015350751345977187\n","Epoch:  9984 , current epoch train loss:  0.001539013348519802\n","Epoch:  9985 , current epoch train loss:  0.001626198529265821\n","Epoch:  9986 , current epoch train loss:  0.0016589671140536666\n","Epoch:  9987 , current epoch train loss:  0.001554232556372881\n","Epoch:  9988 , current epoch train loss:  0.0016071153804659843\n","Epoch:  9989 , current epoch train loss:  0.0015503827016800642\n","Epoch:  9990 , current epoch train loss:  0.0016329082427546382\n","Epoch:  9991 , current epoch train loss:  0.0015511879464611411\n","Epoch:  9992 , current epoch train loss:  0.001543994527310133\n","Epoch:  9993 , current epoch train loss:  0.0015273839235305786\n","Epoch:  9994 , current epoch train loss:  0.0015332195907831192\n","Epoch:  9995 , current epoch train loss:  0.0015820765402168036\n","Epoch:  9996 , current epoch train loss:  0.0015879690181463957\n","Epoch:  9997 , current epoch train loss:  0.0015626167878508568\n","Epoch:  9998 , current epoch train loss:  0.0016054747393354774\n","Epoch:  9999 , current epoch train loss:  0.0015702515374869108\n","Total training loss:  1.570251537486911e-07\n"]}],"source":["# code from: https://github.com/AnTao97/dgcnn.pytorch#point-cloud-part-segmentation\n","!python3 main_partseg.py --batch_size=2 --exp_name=yellow_toy_1_2_L1_trainOnTop_6dim_10000 --epochs=10000 --dgcnn_pretrained_model_path=\"/content/drive/MyDrive/dgcnn.pytorch/pretrained/model.partseg.t7\""]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"mg4NWE_M9_k8"}},{"cell_type":"code","source":["!python3 main_partseg.py --test_batch_size=2 --dgcnn_pretrained_model_path=\"/content/drive/MyDrive/dgcnn.pytorch/pretrained/model.partseg.t7\" --exp_name=yellow_toy_1_2_L1_trainOnTop_6dim_10000 --eval=True  --model_path=\"/content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_L1_trainOnTop_6dim_10000/models/model_final.t7\" --predicted_pc=\"/content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_L1_trainOnTop_6dim_10000/predicted_pointclouds\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6sYc-iI72Gi","executionInfo":{"status":"ok","timestamp":1687916902602,"user_tz":-120,"elapsed":10592,"user":{"displayName":"GÃ¶zde Ãœnver","userId":"09682688360508506676"}},"outputId":"be77bdd2-e336-4147-a3ca-d3eb41a53ade"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(exp_name='yellow_toy_1_2_L1_trainOnTop_6dim_10000', model='dgcnn', dataset='shapenetpart', class_choice=None, batch_size=32, test_batch_size=2, epochs=200, use_sgd=True, lr=0.001, momentum=0.9, scheduler='cos', no_cuda=False, seed=1, eval=True, num_points=2048, dropout=0.5, emb_dims=1024, k=40, model_path='/content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_L1_trainOnTop_6dim_10000/models/model_final.t7', visu='', visu_format='ply', predicted_pc='/content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_L1_trainOnTop_6dim_10000/predicted_pointclouds', dgcnn_pretrained_model_path='/content/drive/MyDrive/dgcnn.pytorch/pretrained/model.partseg.t7')\n","Using GPU : 0 from 1 devices\n","/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","device cuda\n","/content/drive/MyDrive/dgcnn.pytorch/outputs/yellow_toy_1_2_L1_trainOnTop_6dim_10000/models/model_final.t7\n","Test loss for the batch:  0.00052025041077286\n"]}]},{"cell_type":"markdown","metadata":{"id":"2_5okEBGiImL"},"source":["### Create pointclouds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PO0BiHKoiImM"},"outputs":[],"source":["inp=\"/content/drive/MyDrive/data/dataset/YellowToy01/yellow_push_toy_1_70000.obj\"\n","target=\"/content/drive/MyDrive/data/dataset/YellowToy01/yellow_push_toy_2_70000.obj\"\n","#import torch\n","#import pytorch3d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6s2lYwskiImN"},"outputs":[],"source":["from scipy.spatial import distance\n","import numpy as np\n","import trimesh"]},{"cell_type":"code","source":["pc_inp=trimesh.points.PointCloud(trimesh.load_mesh(inp).vertices[:35000])\n","pc_inp.export(file_obj=\"yellow_push_toy_1_35000_points.ply\")\n","\n","pc_target=trimesh.points.PointCloud(trimesh.load_mesh(target).vertices[:35000])\n","pc_target.export(file_obj=\"yellow_push_toy_2_35000_points.ply\")\n","euc_dist=distance.cdist(pc_inp.vertices,pc_target.vertices)\n","min_idx=np.argmin(euc_dist, axis=1)\n","min_vals=np.min(euc_dist,axis=1)\n","deformation_dist=np.max(min_vals)\n","deformation_idx=np.argmax(min_vals)\n","f=open(\"yellow_deformation_1_2_35000.txt\",\"w\")\n","f.write(\"1 2\\n\")\n","f.write(str(pc_inp.vertices[deformation_idx])+\" \"+str(pc_target.vertices[min_idx[deformation_idx]])+\"\\n\")\n","f.write(str(deformation_idx)+\" \"+str(min_idx[deformation_idx])+\"\\n\")\n","f.write(str(pc_target.vertices[min_idx[deformation_idx]]-pc_inp.vertices[deformation_idx])+\"\\n\")\n","f.write(str(deformation_dist))\n","f.close()"],"metadata":{"id":"Dh54odRK4i-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f=open(\"yellow_deformation_1_2_35000.txt\",\"w\")\n","f.write(\"1 2\\n\")\n","f.write(str(pc_inp.vertices[deformation_idx])+\" \"+str(pc_target.vertices[min_idx[deformation_idx]])+\"\\n\")\n","f.write(str(deformation_idx)+\" \"+str(min_idx[deformation_idx])+\"\\n\")\n","f.write(str(pc_target.vertices[min_idx[deformation_idx]]-pc_inp.vertices[deformation_idx])+\"\\n\")\n","f.write(str(deformation_dist))\n","f.close()"],"metadata":{"id":"Ao0VXo6K6Sa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qdH_SiF7iImN"},"outputs":[],"source":["import trimesh\n","f=open(inp)\n","samples_inp,face_index_inp,colors_inp=trimesh.sample.sample_surface(trimesh.load_mesh(f,file_type=\"obj\"),35000,sample_color=True)\n","pc_inp=trimesh.points.PointCloud(samples_inp, colors=colors_inp)\n","pc_inp.export(file_obj=\"yellow_push_toy_1_35000_points.ply\")\n","\n","f=open(target)\n","samples_target,face_index_target,colors_target=trimesh.sample.sample_surface(trimesh.load_mesh(f,file_type=\"obj\"),35000,sample_color=True)\n","pc_target=trimesh.points.PointCloud(samples_target, colors=colors_inp)\n","pc_target.export(file_obj=\"yellow_push_toy_2_35000_points.ply\")\n","euc_dist=distance.cdist(pc_inp.vertices,pc_target.vertices)\n","min_idx=np.argmin(euc_dist, axis=1)\n","min_vals=np.min(euc_dist,axis=1)\n","deformation_dist=np.max(min_vals)\n","deformation_idx=np.argmax(min_vals)\n","f=open(\"yellow_deformation_1_2_35000.txt\",\"w\")\n","f.write(\"1 2\\n\")\n","f.write(str(pc_inp.vertices[deformation_idx])+\" \"+str(pc_target.vertices[min_idx[deformation_idx]])+\"\\n\")\n","f.write(str(deformation_idx)+\" \"+str(min_idx[deformation_idx])+\"\\n\")\n","f.write(str(pc_target.vertices[min_idx[deformation_idx]]-pc_inp.vertices[deformation_idx])+\"\\n\")\n","f.write(str(deformation_dist))\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNdq9jCRlNNl","outputId":"88fa6214-54ef-4069-bf81-4ab219b15b49"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[-0.0145008  -0.5472554   0.04435405]\n","  [ 0.01836372  0.27826715 -0.02386446]\n","  [ 0.01482164 -0.41865438  0.01042023]\n","  ...\n","  [ 0.01482164 -0.48505172  0.0385731 ]\n","  [-0.0145008  -0.24926934  0.06584046]\n","  [ 0.00656791  0.3903607  -0.05943566]]\n","\n"," [[-0.05103381  0.11112406 -0.12842704]\n","  [-0.27297312  0.49021795 -0.3271018 ]\n","  [-0.18034212  0.49021795  0.46365649]\n","  ...\n","  [-0.07147341  0.46562397  0.6983438 ]\n","  [ 0.1906804   0.49021795  0.23400183]\n","  [ 0.23412345  0.49598667 -0.07335173]]\n","\n"," [[-0.49196333 -0.7824724  -0.05577928]\n","  [-0.61777407 -0.5736105   0.00995999]\n","  [ 0.32272163  0.38928664 -0.08381004]\n","  ...\n","  [ 0.5639905   0.2451422  -0.02105892]\n","  [ 0.513404    0.38928664 -0.0162586 ]\n","  [ 0.40939718  0.44893393  0.0110203 ]]\n","\n"," ...\n","\n"," [[-0.14831223 -0.3157267   0.10305884]\n","  [ 0.08609892 -0.04441956 -0.05121949]\n","  [ 0.24735874  0.23165976  0.47029558]\n","  ...\n","  [ 0.12701592  0.03257599  0.09101792]\n","  [ 0.49982017  0.25839278  0.51284754]\n","  [ 0.1834425   0.25839278 -0.54290843]]\n","\n"," [[-0.27953616  0.34972104  0.45977625]\n","  [-0.29883745 -0.129907   -0.23372854]\n","  [-0.16931655 -0.03959531 -0.28749648]\n","  ...\n","  [-0.2908412   0.33549327  0.4091885 ]\n","  [ 0.2681244  -0.05259152  0.33496124]\n","  [ 0.4408802   0.14771917  0.40286502]]\n","\n"," [[-0.49688652 -0.07040447 -0.03515033]\n","  [-0.72669214  0.04352979  0.05954932]\n","  [ 0.05136745 -0.03245366  0.4687413 ]\n","  ...\n","  [ 0.22638857 -0.15053493  0.3377824 ]\n","  [-0.8478837   0.28997806 -0.00811278]\n","  [ 0.95048213 -0.0143104  -0.05094725]]] [[ 7]\n"," [15]\n"," [ 8]\n"," ...\n"," [15]\n"," [ 4]\n"," [ 0]] [[23 23 23 ... 23 23 22]\n"," [48 47 47 ... 47 47 47]\n"," [24 27 25 ... 25 25 25]\n"," ...\n"," [48 48 47 ... 48 47 47]\n"," [12 13 13 ... 12 13 15]\n"," [ 0  2  1 ...  3  2  0]]\n"]}],"source":["import h5py\n","\n","filename = \"data/shapenet_part_seg_hdf5_data/ply_data_train1.h5\"\n","\n","f = h5py.File(filename,'r')\n","data = f['data'][:].astype('float32')\n","label = f['label'][:].astype('int64')\n","seg = f['pid'][:].astype('int64')\n","f.close()\n","print(data.shape,label.shape,seg.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7KIZ1KNiImP","outputId":"b05b086e-8f10-446f-ce51-7f1047b4219a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2048, 2048, 3) (2048, 1) (2048, 2048)\n"]}],"source":["\n","print(data.shape,label.shape,seg.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4ecjr8RiImQ"},"outputs":[],"source":["import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeJ17CUHiImQ","outputId":"6f7b1fef-f8cc-421d-d079-159cba7a28df"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n"," 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n"," 48 49] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"]}],"source":["print(seg[0].shape,np.unique(label[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJ7HqJrliImQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}